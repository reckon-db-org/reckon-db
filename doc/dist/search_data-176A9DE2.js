searchData={"items":[{"type":"module","title":"esdb_aggregate_nif","doc":"Optimized event aggregation operations for reckon-db. This module provides high-performance event aggregation implementations: aggregate_events : Bulk fold with tagged value semantics sum_field : Vectorized sum accumulation for numeric fields count_where : Count events matching a condition merge_tagged_batch : Batch map merge with tagged values The mode is automatically detected at startup based on whether the NIF library is available. Community edition users (hex.pm) will always use the Erlang fallbacks, which provide identical functionality. Tagged Value Semantics Tagged values control how fields are aggregated: {sum, N} : Add N to the current value (numeric accumulation) {overwrite, V} : Replace current value with V Plain value: Replace current value (default behavior) Usage <span class=\"w\">  </span><span class=\"c1\">%% Aggregate events with tagged value semantics</span><span class=\"w\">\n  </span><span class=\"n\">Events</span><span class=\"w\"> </span><span class=\"o\">=</span><span class=\"w\"> </span><span class=\"p\" data-group-id=\"8750210643-1\">[</span><span class=\"p\" data-group-id=\"8750210643-2\">#{</span><span class=\"ss\">amount</span><span class=\"w\"> </span><span class=\"p\">=&gt;</span><span class=\"w\"> </span><span class=\"p\" data-group-id=\"8750210643-3\">{</span><span class=\"ss\">sum</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"mi\">100</span><span class=\"p\" data-group-id=\"8750210643-3\">}</span><span class=\"p\" data-group-id=\"8750210643-2\">}</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"p\" data-group-id=\"8750210643-4\">#{</span><span class=\"ss\">amount</span><span class=\"w\"> </span><span class=\"p\">=&gt;</span><span class=\"w\"> </span><span class=\"p\" data-group-id=\"8750210643-5\">{</span><span class=\"ss\">sum</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"mi\">50</span><span class=\"p\" data-group-id=\"8750210643-5\">}</span><span class=\"p\" data-group-id=\"8750210643-4\">}</span><span class=\"p\" data-group-id=\"8750210643-1\">]</span><span class=\"p\">,</span><span class=\"w\">\n  </span><span class=\"n\">Result</span><span class=\"w\"> </span><span class=\"o\">=</span><span class=\"w\"> </span><span class=\"nc\">esdb_aggregate_nif</span><span class=\"p\">:</span><span class=\"nf\">aggregate_events</span><span class=\"p\" data-group-id=\"8750210643-6\">(</span><span class=\"n\">Events</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"p\" data-group-id=\"8750210643-7\">#{</span><span class=\"p\" data-group-id=\"8750210643-7\">}</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"ss\">true</span><span class=\"p\" data-group-id=\"8750210643-6\">)</span><span class=\"p\">,</span><span class=\"w\">\n  </span><span class=\"p\" data-group-id=\"8750210643-8\">#{</span><span class=\"ss\">amount</span><span class=\"w\"> </span><span class=\"p\">:=</span><span class=\"w\"> </span><span class=\"mi\">150</span><span class=\"p\" data-group-id=\"8750210643-8\">}</span><span class=\"w\"> </span><span class=\"o\">=</span><span class=\"w\"> </span><span class=\"n\">Result</span><span class=\"p\">.</span><span class=\"w\">\n \n  </span><span class=\"c1\">%% Sum a specific field across all events</span><span class=\"w\">\n  </span><span class=\"n\">Total</span><span class=\"w\"> </span><span class=\"o\">=</span><span class=\"w\"> </span><span class=\"nc\">esdb_aggregate_nif</span><span class=\"p\">:</span><span class=\"nf\">sum_field</span><span class=\"p\" data-group-id=\"8750210643-9\">(</span><span class=\"n\">Events</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"ss\">amount</span><span class=\"p\" data-group-id=\"8750210643-9\">)</span><span class=\"p\">.</span><span class=\"w\">\n \n  </span><span class=\"c1\">%% Check which mode is active</span><span class=\"w\">\n  </span><span class=\"ss\">nif</span><span class=\"w\"> </span><span class=\"o\">=</span><span class=\"w\"> </span><span class=\"nc\">esdb_aggregate_nif</span><span class=\"p\">:</span><span class=\"nf\">implementation</span><span class=\"p\" data-group-id=\"8750210643-10\">(</span><span class=\"p\" data-group-id=\"8750210643-10\">)</span><span class=\"p\">.</span><span class=\"w\">  </span><span class=\"c1\">%% Enterprise</span><span class=\"w\">\n  </span><span class=\"ss\">erlang</span><span class=\"w\"> </span><span class=\"o\">=</span><span class=\"w\"> </span><span class=\"nc\">esdb_aggregate_nif</span><span class=\"p\">:</span><span class=\"nf\">implementation</span><span class=\"p\" data-group-id=\"8750210643-11\">(</span><span class=\"p\" data-group-id=\"8750210643-11\">)</span><span class=\"p\">.</span><span class=\"w\"> </span><span class=\"c1\">%% Community</span>","ref":"esdb_aggregate_nif.html"},{"type":"function","title":"esdb_aggregate_nif.aggregate_events/3","doc":"Aggregate a list of events with tagged value semantics. Processes events in order, applying tagged value rules: -  {sum, N}  adds N to the current value -  {overwrite, V}  replaces the current value with V - Plain values replace the current value If Finalize is true, the result will have tagged values unwrapped.","ref":"esdb_aggregate_nif.html#aggregate_events/3"},{"type":"function","title":"esdb_aggregate_nif.aggregation_stats/1","doc":"Get statistics about event aggregation. Returns counts and basic metrics about the event list.","ref":"esdb_aggregate_nif.html#aggregation_stats/1"},{"type":"function","title":"esdb_aggregate_nif.count_where/3","doc":"Count events matching a simple field condition. Returns the number of events where the specified field equals the expected value.","ref":"esdb_aggregate_nif.html#count_where/3"},{"type":"function","title":"esdb_aggregate_nif.finalize/1","doc":"Finalize a tagged map by unwrapping all tagged values. Converts {sum, N} to N and {overwrite, V} to V.","ref":"esdb_aggregate_nif.html#finalize/1"},{"type":"function","title":"esdb_aggregate_nif.implementation/0","doc":"Get the current implementation mode.","ref":"esdb_aggregate_nif.html#implementation/0"},{"type":"function","title":"esdb_aggregate_nif.is_nif_loaded/0","doc":"Check if the NIF is loaded (Enterprise mode).","ref":"esdb_aggregate_nif.html#is_nif_loaded/0"},{"type":"function","title":"esdb_aggregate_nif.merge_tagged_batch/2","doc":"Merge a batch of key-value pairs into a state map. Applies tagged value semantics to each pair.","ref":"esdb_aggregate_nif.html#merge_tagged_batch/2"},{"type":"function","title":"esdb_aggregate_nif.sum_field/2","doc":"Sum a specific field across all events. Efficiently accumulates numeric values from a named field. Handles tagged values ({sum, N}) and plain numeric values.","ref":"esdb_aggregate_nif.html#sum_field/2"},{"type":"module","title":"esdb_archive_nif","doc":"Optimized archive compression for reckon-db. This module provides high-performance compression implementations: LZ4 : Fastest compression/decompression (real-time use) Zstd : Best compression ratio (cold storage) Zlib : Standard compatibility (Erlang term_to_binary) The mode is automatically detected at startup based on whether the NIF library is available. Community edition users (hex.pm) will always use the Erlang fallbacks, which provide identical functionality. Algorithm Selection | Algorithm | Speed | Ratio | Use Case | |-----------|-------|-------|----------| | LZ4 | Fastest | ~2.5:1 | Hot archives, real-time | | Zstd | Fast | ~4:1 | Cold archives, storage | | Zlib | Moderate | ~3:1 | Compatibility | Usage <span class=\"w\">  </span><span class=\"c1\">%% Compress with LZ4 (fastest)</span><span class=\"w\">\n  </span><span class=\"p\" data-group-id=\"0913193199-1\">{</span><span class=\"ss\">ok</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"n\">Compressed</span><span class=\"p\" data-group-id=\"0913193199-1\">}</span><span class=\"w\"> </span><span class=\"o\">=</span><span class=\"w\"> </span><span class=\"nc\">esdb_archive_nif</span><span class=\"p\">:</span><span class=\"nf\">compress</span><span class=\"p\" data-group-id=\"0913193199-2\">(</span><span class=\"n\">Data</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"ss\">lz4</span><span class=\"p\" data-group-id=\"0913193199-2\">)</span><span class=\"p\">.</span><span class=\"w\">\n \n  </span><span class=\"c1\">%% Compress with Zstd level 3 (good balance)</span><span class=\"w\">\n  </span><span class=\"p\" data-group-id=\"0913193199-3\">{</span><span class=\"ss\">ok</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"n\">Compressed</span><span class=\"p\" data-group-id=\"0913193199-3\">}</span><span class=\"w\"> </span><span class=\"o\">=</span><span class=\"w\"> </span><span class=\"nc\">esdb_archive_nif</span><span class=\"p\">:</span><span class=\"nf\">compress</span><span class=\"p\" data-group-id=\"0913193199-4\">(</span><span class=\"n\">Data</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"ss\">zstd</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"mi\">3</span><span class=\"p\" data-group-id=\"0913193199-4\">)</span><span class=\"p\">.</span><span class=\"w\">\n \n  </span><span class=\"c1\">%% Decompress</span><span class=\"w\">\n  </span><span class=\"p\" data-group-id=\"0913193199-5\">{</span><span class=\"ss\">ok</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"n\">Original</span><span class=\"p\" data-group-id=\"0913193199-5\">}</span><span class=\"w\"> </span><span class=\"o\">=</span><span class=\"w\"> </span><span class=\"nc\">esdb_archive_nif</span><span class=\"p\">:</span><span class=\"nf\">decompress</span><span class=\"p\" data-group-id=\"0913193199-6\">(</span><span class=\"n\">Compressed</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"ss\">lz4</span><span class=\"p\" data-group-id=\"0913193199-6\">)</span><span class=\"p\">.</span><span class=\"w\">\n \n  </span><span class=\"c1\">%% Check which mode is active</span><span class=\"w\">\n  </span><span class=\"ss\">nif</span><span class=\"w\"> </span><span class=\"o\">=</span><span class=\"w\"> </span><span class=\"nc\">esdb_archive_nif</span><span class=\"p\">:</span><span class=\"nf\">implementation</span><span class=\"p\" data-group-id=\"0913193199-7\">(</span><span class=\"p\" data-group-id=\"0913193199-7\">)</span><span class=\"p\">.</span><span class=\"w\">  </span><span class=\"c1\">%% Enterprise</span><span class=\"w\">\n  </span><span class=\"ss\">erlang</span><span class=\"w\"> </span><span class=\"o\">=</span><span class=\"w\"> </span><span class=\"nc\">esdb_archive_nif</span><span class=\"p\">:</span><span class=\"nf\">implementation</span><span class=\"p\" data-group-id=\"0913193199-8\">(</span><span class=\"p\" data-group-id=\"0913193199-8\">)</span><span class=\"p\">.</span><span class=\"w\"> </span><span class=\"c1\">%% Community</span>","ref":"esdb_archive_nif.html"},{"type":"function","title":"esdb_archive_nif.compress/2","doc":"Compress data with specified algorithm using default level.","ref":"esdb_archive_nif.html#compress/2"},{"type":"function","title":"esdb_archive_nif.compress/3","doc":"Compress data with specified algorithm and compression level.","ref":"esdb_archive_nif.html#compress/3"},{"type":"function","title":"esdb_archive_nif.compress_lz4/1","doc":"Compress data using LZ4 (fastest).","ref":"esdb_archive_nif.html#compress_lz4/1"},{"type":"function","title":"esdb_archive_nif.compress_zlib/1","doc":"Compress data using Zlib with default level.","ref":"esdb_archive_nif.html#compress_zlib/1"},{"type":"function","title":"esdb_archive_nif.compress_zlib/2","doc":"Compress data using Zlib with specified level (0-9).","ref":"esdb_archive_nif.html#compress_zlib/2"},{"type":"function","title":"esdb_archive_nif.compress_zstd/1","doc":"Compress data using Zstd with default level.","ref":"esdb_archive_nif.html#compress_zstd/1"},{"type":"function","title":"esdb_archive_nif.compress_zstd/2","doc":"Compress data using Zstd with specified level (1-22).","ref":"esdb_archive_nif.html#compress_zstd/2"},{"type":"function","title":"esdb_archive_nif.compression_stats/2","doc":"Get compression statistics for data with algorithm.","ref":"esdb_archive_nif.html#compression_stats/2"},{"type":"function","title":"esdb_archive_nif.compression_stats/3","doc":"Get compression statistics with specified level.","ref":"esdb_archive_nif.html#compression_stats/3"},{"type":"function","title":"esdb_archive_nif.decompress/2","doc":"Decompress data with specified algorithm.","ref":"esdb_archive_nif.html#decompress/2"},{"type":"function","title":"esdb_archive_nif.decompress_lz4/1","doc":"Decompress LZ4-compressed data.","ref":"esdb_archive_nif.html#decompress_lz4/1"},{"type":"function","title":"esdb_archive_nif.decompress_zlib/1","doc":"Decompress Zlib-compressed data.","ref":"esdb_archive_nif.html#decompress_zlib/1"},{"type":"function","title":"esdb_archive_nif.decompress_zstd/1","doc":"Decompress Zstd-compressed data.","ref":"esdb_archive_nif.html#decompress_zstd/1"},{"type":"function","title":"esdb_archive_nif.implementation/0","doc":"Get the current implementation mode.","ref":"esdb_archive_nif.html#implementation/0"},{"type":"function","title":"esdb_archive_nif.is_nif_loaded/0","doc":"Check if the NIF is loaded (Enterprise mode).","ref":"esdb_archive_nif.html#is_nif_loaded/0"},{"type":"module","title":"esdb_capability_verifier","doc":"Server-side capability token verification for reckon-db. Verifies UCAN-inspired capability tokens for authorization decisions. Tokens are created client-side (reckon-gater) and verified server-side here. Verification steps: Decode token (JWT or binary format, auto-detected) Verify Ed25519 signature using issuer's public key from DID Check token is not expired (exp less than now) Check token is active (nbf less than or equal to now, if present) Check token is not revoked (via gossip list) Match resource URI against request Match action against permitted actions See also:  esdb_capability ,  esdb_identity .","ref":"esdb_capability_verifier.html"},{"type":"function","title":"esdb_capability_verifier.authorize/3","doc":"Authorize a request with a capability token Verifies the token AND checks it grants permission for the specified resource and action.","ref":"esdb_capability_verifier.html#authorize/3"},{"type":"function","title":"esdb_capability_verifier.authorize/4","doc":"Authorize a request with options","ref":"esdb_capability_verifier.html#authorize/4"},{"type":"type","title":"esdb_capability_verifier.capability/0","doc":"","ref":"esdb_capability_verifier.html#t:capability/0"},{"type":"type","title":"esdb_capability_verifier.capability_error/0","doc":"","ref":"esdb_capability_verifier.html#t:capability_error/0"},{"type":"type","title":"esdb_capability_verifier.capability_grant/0","doc":"","ref":"esdb_capability_verifier.html#t:capability_grant/0"},{"type":"function","title":"esdb_capability_verifier.check_permission/3","doc":"Check if a verified capability grants permission for resource/action The capability should already be verified (signature, expiration). This function only checks the grants against the requested resource/action.","ref":"esdb_capability_verifier.html#check_permission/3"},{"type":"function","title":"esdb_capability_verifier.extract_token_cid/1","doc":"Extract a content-addressed identifier for a token Uses SHA-256 hash of the token's core fields (excluding signature). This CID can be used for revocation.","ref":"esdb_capability_verifier.html#extract_token_cid/1"},{"type":"function","title":"esdb_capability_verifier.is_revoked/1","doc":"Check if a token CID is revoked Currently returns false (not revoked) as revocation gossip is not yet implemented. This will be integrated with a gossip-based revocation list in Phase 4.","ref":"esdb_capability_verifier.html#is_revoked/1"},{"type":"type","title":"esdb_capability_verifier.verification_result/0","doc":"","ref":"esdb_capability_verifier.html#t:verification_result/0"},{"type":"function","title":"esdb_capability_verifier.verify/1","doc":"Verify a capability token Decodes the token and verifies: - Signature is valid (Ed25519) - Token is not expired - Token is not revoked Does NOT check permissions against a specific resource/action. Use authorize/3 for full authorization.","ref":"esdb_capability_verifier.html#verify/1"},{"type":"function","title":"esdb_capability_verifier.verify/2","doc":"Verify a capability token with options","ref":"esdb_capability_verifier.html#verify/2"},{"type":"type","title":"esdb_capability_verifier.verify_opts/0","doc":"","ref":"esdb_capability_verifier.html#t:verify_opts/0"},{"type":"module","title":"esdb_crypto_nif","doc":"Optimized cryptographic operations for reckon-db. This module provides high-performance implementations of cryptographic operations used throughout reckon-db. It supports two modes: Enterprise mode : Uses Rust NIFs for maximum performance Community mode : Uses pure Erlang fallbacks (fully functional) The mode is automatically detected at startup based on whether the NIF library is available. Community edition users (hex.pm) will always use the Erlang fallbacks, which provide identical functionality. Usage All functions work identically regardless of which implementation is active: <span class=\"w\">  </span><span class=\"c1\">%% Verify Ed25519 signature</span><span class=\"w\">\n  </span><span class=\"ss\">true</span><span class=\"w\"> </span><span class=\"o\">=</span><span class=\"w\"> </span><span class=\"nc\">esdb_crypto_nif</span><span class=\"p\">:</span><span class=\"nf\">verify_ed25519</span><span class=\"p\" data-group-id=\"5915633393-1\">(</span><span class=\"n\">Message</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"n\">Signature</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"n\">PublicKey</span><span class=\"p\" data-group-id=\"5915633393-1\">)</span><span class=\"p\">.</span><span class=\"w\">\n \n  </span><span class=\"c1\">%% Generate token CID (SHA256 + base64)</span><span class=\"w\">\n  </span><span class=\"n\">CID</span><span class=\"w\"> </span><span class=\"o\">=</span><span class=\"w\"> </span><span class=\"nc\">esdb_crypto_nif</span><span class=\"p\">:</span><span class=\"nf\">hash_sha256_base64</span><span class=\"p\" data-group-id=\"5915633393-2\">(</span><span class=\"n\">Data</span><span class=\"p\" data-group-id=\"5915633393-2\">)</span><span class=\"p\">.</span><span class=\"w\">\n \n  </span><span class=\"c1\">%% Check which mode is active</span><span class=\"w\">\n  </span><span class=\"ss\">true</span><span class=\"w\"> </span><span class=\"o\">=</span><span class=\"w\"> </span><span class=\"nc\">esdb_crypto_nif</span><span class=\"p\">:</span><span class=\"nf\">is_nif_loaded</span><span class=\"p\" data-group-id=\"5915633393-3\">(</span><span class=\"p\" data-group-id=\"5915633393-3\">)</span><span class=\"p\">.</span><span class=\"w\">  </span><span class=\"c1\">%% Enterprise</span><span class=\"w\">\n  </span><span class=\"ss\">false</span><span class=\"w\"> </span><span class=\"o\">=</span><span class=\"w\"> </span><span class=\"nc\">esdb_crypto_nif</span><span class=\"p\">:</span><span class=\"nf\">is_nif_loaded</span><span class=\"p\" data-group-id=\"5915633393-4\">(</span><span class=\"p\" data-group-id=\"5915633393-4\">)</span><span class=\"p\">.</span><span class=\"w\"> </span><span class=\"c1\">%% Community</span>","ref":"esdb_crypto_nif.html"},{"type":"function","title":"esdb_crypto_nif.base64_decode_urlsafe/1","doc":"Decode URL-safe base64 string. Returns  {ok, Binary}  on success,  {error, invalid_base64}  on failure.","ref":"esdb_crypto_nif.html#base64_decode_urlsafe/1"},{"type":"function","title":"esdb_crypto_nif.base64_encode_urlsafe/1","doc":"Encode binary as URL-safe base64 (no padding).","ref":"esdb_crypto_nif.html#base64_encode_urlsafe/1"},{"type":"function","title":"esdb_crypto_nif.hash_sha256/1","doc":"Compute SHA-256 hash. Returns a 32-byte binary containing the SHA-256 hash of the input.","ref":"esdb_crypto_nif.html#hash_sha256/1"},{"type":"function","title":"esdb_crypto_nif.hash_sha256_base64/1","doc":"Compute SHA-256 hash and encode as URL-safe base64. This is optimized for token CID generation - combines hash + encode in a single call to avoid intermediate allocations. Returns a URL-safe base64 string (no padding).","ref":"esdb_crypto_nif.html#hash_sha256_base64/1"},{"type":"function","title":"esdb_crypto_nif.implementation/0","doc":"Get the current implementation mode. Returns  nif  for Enterprise mode or  erlang  for Community mode.","ref":"esdb_crypto_nif.html#implementation/0"},{"type":"function","title":"esdb_crypto_nif.is_nif_loaded/0","doc":"Check if the NIF is loaded (Enterprise mode). Returns  true  if running in Enterprise mode with NIF optimizations,  false  if running in Community mode with pure Erlang.","ref":"esdb_crypto_nif.html#is_nif_loaded/0"},{"type":"function","title":"esdb_crypto_nif.secure_compare/2","doc":"Constant-time comparison of two binaries. This is important for security - prevents timing attacks when comparing signatures, hashes, or tokens. Always takes the same amount of time regardless of where the difference is (if any). Returns  true  if equal,  false  otherwise.","ref":"esdb_crypto_nif.html#secure_compare/2"},{"type":"function","title":"esdb_crypto_nif.verify_ed25519/3","doc":"Verify an Ed25519 signature. This function verifies that a signature was created by the private key corresponding to the given public key. Arguments: Message  - The original message that was signed (binary) Signature  - The 64-byte Ed25519 signature (binary) PublicKey  - The 32-byte Ed25519 public key (binary) Returns  true  if valid,  false  otherwise.","ref":"esdb_crypto_nif.html#verify_ed25519/3"},{"type":"module","title":"esdb_filter_nif","doc":"Optimized pattern matching and filtering operations for reckon-db. This module provides high-performance pattern matching implementations: Wildcard matching : Fast * and ? pattern matching Regex matching : Compiled regex pattern matching Prefix/Suffix matching : Optimized start/end checks Batch filtering : Filter lists by patterns efficiently The mode is automatically detected at startup based on whether the NIF library is available. Community edition users (hex.pm) will always use the Erlang fallbacks, which provide identical functionality. Wildcard Patterns Wildcards supported: *  matches any sequence of characters (including empty) ?  matches any single character Usage <span class=\"w\">  </span><span class=\"c1\">%% Check if a stream matches a pattern</span><span class=\"w\">\n  </span><span class=\"ss\">true</span><span class=\"w\"> </span><span class=\"o\">=</span><span class=\"w\"> </span><span class=\"nc\">esdb_filter_nif</span><span class=\"p\">:</span><span class=\"nf\">wildcard_match</span><span class=\"p\" data-group-id=\"7091978478-1\">(</span><span class=\"p\" data-group-id=\"7091978478-2\">&lt;&lt;</span><span class=\"s\">&quot;orders-123&quot;</span><span class=\"p\" data-group-id=\"7091978478-2\">&gt;&gt;</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"p\" data-group-id=\"7091978478-3\">&lt;&lt;</span><span class=\"s\">&quot;orders-*&quot;</span><span class=\"p\" data-group-id=\"7091978478-3\">&gt;&gt;</span><span class=\"p\" data-group-id=\"7091978478-1\">)</span><span class=\"p\">.</span><span class=\"w\">\n \n  </span><span class=\"c1\">%% Filter streams by pattern</span><span class=\"w\">\n  </span><span class=\"n\">Matching</span><span class=\"w\"> </span><span class=\"o\">=</span><span class=\"w\"> </span><span class=\"nc\">esdb_filter_nif</span><span class=\"p\">:</span><span class=\"nf\">filter_by_wildcard</span><span class=\"p\" data-group-id=\"7091978478-4\">(</span><span class=\"n\">Streams</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"p\" data-group-id=\"7091978478-5\">&lt;&lt;</span><span class=\"s\">&quot;user-*&quot;</span><span class=\"p\" data-group-id=\"7091978478-5\">&gt;&gt;</span><span class=\"p\" data-group-id=\"7091978478-4\">)</span><span class=\"p\">.</span><span class=\"w\">\n \n  </span><span class=\"c1\">%% Check which mode is active</span><span class=\"w\">\n  </span><span class=\"ss\">nif</span><span class=\"w\"> </span><span class=\"o\">=</span><span class=\"w\"> </span><span class=\"nc\">esdb_filter_nif</span><span class=\"p\">:</span><span class=\"nf\">implementation</span><span class=\"p\" data-group-id=\"7091978478-6\">(</span><span class=\"p\" data-group-id=\"7091978478-6\">)</span><span class=\"p\">.</span><span class=\"w\">  </span><span class=\"c1\">%% Enterprise</span><span class=\"w\">\n  </span><span class=\"ss\">erlang</span><span class=\"w\"> </span><span class=\"o\">=</span><span class=\"w\"> </span><span class=\"nc\">esdb_filter_nif</span><span class=\"p\">:</span><span class=\"nf\">implementation</span><span class=\"p\" data-group-id=\"7091978478-7\">(</span><span class=\"p\" data-group-id=\"7091978478-7\">)</span><span class=\"p\">.</span><span class=\"w\"> </span><span class=\"c1\">%% Community</span>","ref":"esdb_filter_nif.html"},{"type":"function","title":"esdb_filter_nif.count_matches/2","doc":"Count items matching a wildcard pattern.","ref":"esdb_filter_nif.html#count_matches/2"},{"type":"function","title":"esdb_filter_nif.filter_by_prefix/2","doc":"Filter a list of binaries by prefix.","ref":"esdb_filter_nif.html#filter_by_prefix/2"},{"type":"function","title":"esdb_filter_nif.filter_by_regex/2","doc":"Filter a list of binaries by regex pattern.","ref":"esdb_filter_nif.html#filter_by_regex/2"},{"type":"function","title":"esdb_filter_nif.filter_by_suffix/2","doc":"Filter a list of binaries by suffix.","ref":"esdb_filter_nif.html#filter_by_suffix/2"},{"type":"function","title":"esdb_filter_nif.filter_by_wildcard/2","doc":"Filter a list of binaries by wildcard pattern.","ref":"esdb_filter_nif.html#filter_by_wildcard/2"},{"type":"function","title":"esdb_filter_nif.has_prefix/2","doc":"Check if a string has a specific prefix.","ref":"esdb_filter_nif.html#has_prefix/2"},{"type":"function","title":"esdb_filter_nif.has_suffix/2","doc":"Check if a string has a specific suffix.","ref":"esdb_filter_nif.html#has_suffix/2"},{"type":"function","title":"esdb_filter_nif.implementation/0","doc":"Get the current implementation mode.","ref":"esdb_filter_nif.html#implementation/0"},{"type":"function","title":"esdb_filter_nif.is_nif_loaded/0","doc":"Check if the NIF is loaded (Enterprise mode).","ref":"esdb_filter_nif.html#is_nif_loaded/0"},{"type":"function","title":"esdb_filter_nif.is_valid_regex/1","doc":"Validate that a pattern is a valid regex.","ref":"esdb_filter_nif.html#is_valid_regex/1"},{"type":"function","title":"esdb_filter_nif.match_indices/2","doc":"Return indices of items matching a wildcard pattern.","ref":"esdb_filter_nif.html#match_indices/2"},{"type":"function","title":"esdb_filter_nif.regex_match/2","doc":"Check if a string matches a regex pattern.","ref":"esdb_filter_nif.html#regex_match/2"},{"type":"function","title":"esdb_filter_nif.wildcard_match/2","doc":"Check if a string matches a wildcard pattern.","ref":"esdb_filter_nif.html#wildcard_match/2"},{"type":"function","title":"esdb_filter_nif.wildcard_to_regex/1","doc":"Convert a wildcard pattern to a regex pattern. Wildcards: -  *  matches any sequence of characters -  ?  matches any single character","ref":"esdb_filter_nif.html#wildcard_to_regex/1"},{"type":"module","title":"esdb_graph_nif","doc":"Optimized graph operations for causation analysis in reckon-db. This module provides high-performance graph algorithms: Graph building : Build edges from event causation relationships Topological sort : Order events by causation (causes before effects) Path finding : Check paths, find ancestors/descendants DOT export : Generate Graphviz visualization The mode is automatically detected at startup based on whether the NIF library is available. Community edition users (hex.pm) will always use the Erlang fallbacks, which provide identical functionality. Usage <span class=\"w\">  </span><span class=\"c1\">%% Build edges from events (event_id, causation_id pairs)</span><span class=\"w\">\n  </span><span class=\"n\">Nodes</span><span class=\"w\"> </span><span class=\"o\">=</span><span class=\"w\"> </span><span class=\"p\" data-group-id=\"8675325052-1\">[</span><span class=\"p\" data-group-id=\"8675325052-2\">{</span><span class=\"p\" data-group-id=\"8675325052-3\">&lt;&lt;</span><span class=\"s\">&quot;evt-1&quot;</span><span class=\"p\" data-group-id=\"8675325052-3\">&gt;&gt;</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"ss\">undefined</span><span class=\"p\" data-group-id=\"8675325052-2\">}</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"p\" data-group-id=\"8675325052-4\">{</span><span class=\"p\" data-group-id=\"8675325052-5\">&lt;&lt;</span><span class=\"s\">&quot;evt-2&quot;</span><span class=\"p\" data-group-id=\"8675325052-5\">&gt;&gt;</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"p\" data-group-id=\"8675325052-6\">&lt;&lt;</span><span class=\"s\">&quot;evt-1&quot;</span><span class=\"p\" data-group-id=\"8675325052-6\">&gt;&gt;</span><span class=\"p\" data-group-id=\"8675325052-4\">}</span><span class=\"p\" data-group-id=\"8675325052-1\">]</span><span class=\"p\">,</span><span class=\"w\">\n  </span><span class=\"n\">Edges</span><span class=\"w\"> </span><span class=\"o\">=</span><span class=\"w\"> </span><span class=\"nc\">esdb_graph_nif</span><span class=\"p\">:</span><span class=\"nf\">build_edges</span><span class=\"p\" data-group-id=\"8675325052-7\">(</span><span class=\"n\">Nodes</span><span class=\"p\" data-group-id=\"8675325052-7\">)</span><span class=\"p\">.</span><span class=\"w\">\n \n  </span><span class=\"c1\">%% Topological sort</span><span class=\"w\">\n  </span><span class=\"p\" data-group-id=\"8675325052-8\">{</span><span class=\"ss\">ok</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"n\">Sorted</span><span class=\"p\" data-group-id=\"8675325052-8\">}</span><span class=\"w\"> </span><span class=\"o\">=</span><span class=\"w\"> </span><span class=\"nc\">esdb_graph_nif</span><span class=\"p\">:</span><span class=\"nf\">topo_sort</span><span class=\"p\" data-group-id=\"8675325052-9\">(</span><span class=\"p\" data-group-id=\"8675325052-10\">[</span><span class=\"p\" data-group-id=\"8675325052-11\">&lt;&lt;</span><span class=\"s\">&quot;evt-1&quot;</span><span class=\"p\" data-group-id=\"8675325052-11\">&gt;&gt;</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"p\" data-group-id=\"8675325052-12\">&lt;&lt;</span><span class=\"s\">&quot;evt-2&quot;</span><span class=\"p\" data-group-id=\"8675325052-12\">&gt;&gt;</span><span class=\"p\" data-group-id=\"8675325052-10\">]</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"n\">Edges</span><span class=\"p\" data-group-id=\"8675325052-9\">)</span><span class=\"p\">.</span><span class=\"w\">\n \n  </span><span class=\"c1\">%% Check which mode is active</span><span class=\"w\">\n  </span><span class=\"ss\">nif</span><span class=\"w\"> </span><span class=\"o\">=</span><span class=\"w\"> </span><span class=\"nc\">esdb_graph_nif</span><span class=\"p\">:</span><span class=\"nf\">implementation</span><span class=\"p\" data-group-id=\"8675325052-13\">(</span><span class=\"p\" data-group-id=\"8675325052-13\">)</span><span class=\"p\">.</span><span class=\"w\">  </span><span class=\"c1\">%% Enterprise</span><span class=\"w\">\n  </span><span class=\"ss\">erlang</span><span class=\"w\"> </span><span class=\"o\">=</span><span class=\"w\"> </span><span class=\"nc\">esdb_graph_nif</span><span class=\"p\">:</span><span class=\"nf\">implementation</span><span class=\"p\" data-group-id=\"8675325052-14\">(</span><span class=\"p\" data-group-id=\"8675325052-14\">)</span><span class=\"p\">.</span><span class=\"w\"> </span><span class=\"c1\">%% Community</span>","ref":"esdb_graph_nif.html"},{"type":"function","title":"esdb_graph_nif.build_edges/1","doc":"Build edges from a list of event_id, causation_id tuples. Causation ID can be undefined for root events. Returns list of from_id, to_id edges.","ref":"esdb_graph_nif.html#build_edges/1"},{"type":"function","title":"esdb_graph_nif.find_leaves/2","doc":"Find leaf nodes (nodes with no outgoing edges).","ref":"esdb_graph_nif.html#find_leaves/2"},{"type":"function","title":"esdb_graph_nif.find_roots/2","doc":"Find root nodes (nodes with no incoming edges).","ref":"esdb_graph_nif.html#find_roots/2"},{"type":"function","title":"esdb_graph_nif.get_ancestors/3","doc":"Get all ancestors of a node (nodes that can reach it).","ref":"esdb_graph_nif.html#get_ancestors/3"},{"type":"function","title":"esdb_graph_nif.get_descendants/3","doc":"Get all descendants of a node (nodes reachable from it).","ref":"esdb_graph_nif.html#get_descendants/3"},{"type":"function","title":"esdb_graph_nif.graph_stats/2","doc":"Get graph statistics. Returns a map with node_count, edge_count, root_count, leaf_count, max_depth.","ref":"esdb_graph_nif.html#graph_stats/2"},{"type":"function","title":"esdb_graph_nif.has_cycle/2","doc":"Check if the graph contains cycles.","ref":"esdb_graph_nif.html#has_cycle/2"},{"type":"function","title":"esdb_graph_nif.has_path/4","doc":"Check if there's a path between two nodes.","ref":"esdb_graph_nif.html#has_path/4"},{"type":"function","title":"esdb_graph_nif.implementation/0","doc":"Get the current implementation mode.","ref":"esdb_graph_nif.html#implementation/0"},{"type":"function","title":"esdb_graph_nif.is_nif_loaded/0","doc":"Check if the NIF is loaded (Enterprise mode).","ref":"esdb_graph_nif.html#is_nif_loaded/0"},{"type":"function","title":"esdb_graph_nif.to_dot/2","doc":"Generate DOT format for Graphviz visualization. Nodes are tuples of {event_id, event_type, label}.","ref":"esdb_graph_nif.html#to_dot/2"},{"type":"function","title":"esdb_graph_nif.to_dot_simple/2","doc":"Generate DOT format with simplified labels (just type). Nodes are tuples of {event_id, event_type}.","ref":"esdb_graph_nif.html#to_dot_simple/2"},{"type":"function","title":"esdb_graph_nif.topo_sort/2","doc":"Perform topological sort on the graph. Returns nodes in dependency order (causes before effects).","ref":"esdb_graph_nif.html#topo_sort/2"},{"type":"module","title":"esdb_hash_nif","doc":"Optimized hashing operations for reckon-db. This module provides high-performance hash implementations: xxHash64 : Extremely fast 64-bit hash xxHash3 : Even faster, modern 64-bit hash with SIMD Partition hash : For consistent stream/subscription routing FNV-1a : Fast for small keys The mode is automatically detected at startup based on whether the NIF library is available. Community edition users (hex.pm) will always use the Erlang fallbacks, which provide identical functionality. Usage <span class=\"w\">  </span><span class=\"c1\">%% Fast hash for routing</span><span class=\"w\">\n  </span><span class=\"n\">Partition</span><span class=\"w\"> </span><span class=\"o\">=</span><span class=\"w\"> </span><span class=\"nc\">esdb_hash_nif</span><span class=\"p\">:</span><span class=\"nf\">partition_hash</span><span class=\"p\" data-group-id=\"0634978541-1\">(</span><span class=\"n\">StreamId</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"mi\">16</span><span class=\"p\" data-group-id=\"0634978541-1\">)</span><span class=\"p\">.</span><span class=\"w\">\n \n  </span><span class=\"c1\">%% Stream-specific routing</span><span class=\"w\">\n  </span><span class=\"n\">Partition</span><span class=\"w\"> </span><span class=\"o\">=</span><span class=\"w\"> </span><span class=\"nc\">esdb_hash_nif</span><span class=\"p\">:</span><span class=\"nf\">stream_partition</span><span class=\"p\" data-group-id=\"0634978541-2\">(</span><span class=\"n\">StoreId</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"n\">StreamId</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"mi\">16</span><span class=\"p\" data-group-id=\"0634978541-2\">)</span><span class=\"p\">.</span><span class=\"w\">\n \n  </span><span class=\"c1\">%% Raw xxHash for checksums</span><span class=\"w\">\n  </span><span class=\"n\">Hash</span><span class=\"w\"> </span><span class=\"o\">=</span><span class=\"w\"> </span><span class=\"nc\">esdb_hash_nif</span><span class=\"p\">:</span><span class=\"nf\">xxhash64</span><span class=\"p\" data-group-id=\"0634978541-3\">(</span><span class=\"n\">Data</span><span class=\"p\" data-group-id=\"0634978541-3\">)</span><span class=\"p\">.</span><span class=\"w\">\n \n  </span><span class=\"c1\">%% Check which mode is active</span><span class=\"w\">\n  </span><span class=\"ss\">nif</span><span class=\"w\"> </span><span class=\"o\">=</span><span class=\"w\"> </span><span class=\"nc\">esdb_hash_nif</span><span class=\"p\">:</span><span class=\"nf\">implementation</span><span class=\"p\" data-group-id=\"0634978541-4\">(</span><span class=\"p\" data-group-id=\"0634978541-4\">)</span><span class=\"p\">.</span><span class=\"w\">  </span><span class=\"c1\">%% Enterprise</span><span class=\"w\">\n  </span><span class=\"ss\">erlang</span><span class=\"w\"> </span><span class=\"o\">=</span><span class=\"w\"> </span><span class=\"nc\">esdb_hash_nif</span><span class=\"p\">:</span><span class=\"nf\">implementation</span><span class=\"p\" data-group-id=\"0634978541-5\">(</span><span class=\"p\" data-group-id=\"0634978541-5\">)</span><span class=\"p\">.</span><span class=\"w\"> </span><span class=\"c1\">%% Community</span>","ref":"esdb_hash_nif.html"},{"type":"function","title":"esdb_hash_nif.fast_phash/2","doc":"Fast replacement for erlang:phash2/2.","ref":"esdb_hash_nif.html#fast_phash/2"},{"type":"function","title":"esdb_hash_nif.fnv1a/1","doc":"Compute FNV-1a hash of binary data. Fast for small keys (under 32 bytes).","ref":"esdb_hash_nif.html#fnv1a/1"},{"type":"function","title":"esdb_hash_nif.implementation/0","doc":"Get the current implementation mode.","ref":"esdb_hash_nif.html#implementation/0"},{"type":"function","title":"esdb_hash_nif.is_nif_loaded/0","doc":"Check if the NIF is loaded (Enterprise mode).","ref":"esdb_hash_nif.html#is_nif_loaded/0"},{"type":"function","title":"esdb_hash_nif.partition_hash/2","doc":"Hash data and map to a partition number. Used for consistent routing of streams/subscriptions to workers.","ref":"esdb_hash_nif.html#partition_hash/2"},{"type":"function","title":"esdb_hash_nif.partition_hash_batch/2","doc":"Hash multiple binaries and return their partition assignments.","ref":"esdb_hash_nif.html#partition_hash_batch/2"},{"type":"function","title":"esdb_hash_nif.stream_partition/3","doc":"Hash {StoreId, StreamId} tuple for stream routing.","ref":"esdb_hash_nif.html#stream_partition/3"},{"type":"function","title":"esdb_hash_nif.xxhash3/1","doc":"Compute xxHash3 (64-bit) of binary data. xxHash3 is faster than xxHash64, especially for small inputs.","ref":"esdb_hash_nif.html#xxhash3/1"},{"type":"function","title":"esdb_hash_nif.xxhash64/1","doc":"Compute xxHash64 of binary data.","ref":"esdb_hash_nif.html#xxhash64/1"},{"type":"function","title":"esdb_hash_nif.xxhash64/2","doc":"Compute xxHash64 with a seed.","ref":"esdb_hash_nif.html#xxhash64/2"},{"type":"module","title":"esdb_revocation","doc":"Token revocation management for reckon-db. Manages revocation of capability tokens. Tokens can be revoked before their expiration when: A key is compromised An identity is removed from the system Permissions need to be immediately revoked This module supports multiple revocation strategies: Local ETS (current): Fast local lookups, no distribution Gossip (planned): Eventually consistent, partition tolerant Epoch-based (planned): Revoke all tokens before a timestamp","ref":"esdb_revocation.html"},{"type":"function","title":"esdb_revocation.clear/0","doc":"Clear all revocations (for testing)","ref":"esdb_revocation.html#clear/0"},{"type":"function","title":"esdb_revocation.get_revocations/0","doc":"Get all active revocations (for debugging/monitoring)","ref":"esdb_revocation.html#get_revocations/0"},{"type":"function","title":"esdb_revocation.is_issuer_revoked/1","doc":"Check if an issuer DID is revoked","ref":"esdb_revocation.html#is_issuer_revoked/1"},{"type":"function","title":"esdb_revocation.is_revoked/1","doc":"Check if a token CID is revoked","ref":"esdb_revocation.html#is_revoked/1"},{"type":"function","title":"esdb_revocation.revoke/1","doc":"Revoke a token by its CID","ref":"esdb_revocation.html#revoke/1"},{"type":"function","title":"esdb_revocation.revoke/2","doc":"Revoke a token with a reason","ref":"esdb_revocation.html#revoke/2"},{"type":"function","title":"esdb_revocation.revoke_issuer/1","doc":"Revoke all tokens from an issuer This is useful when an identity is compromised or removed. All tokens with this issuer DID will be considered revoked.","ref":"esdb_revocation.html#revoke_issuer/1"},{"type":"function","title":"esdb_revocation.start_link/0","doc":"Start the revocation server","ref":"esdb_revocation.html#start_link/0"},{"type":"module","title":"reckon_db_aggregator","doc":"Event aggregator for reckon-db Aggregates events from an event stream using tagged rules. Supports special value tags for custom aggregation behavior: Tagged value types: {sum, N} - Add N to current value (starts at 0) {overwrite, V} - Replace current value with V plain value - Replace current value (default behavior)","ref":"reckon_db_aggregator.html"},{"type":"function","title":"reckon_db_aggregator.aggregate/3","doc":"Aggregate events from a stream with optional snapshot This is a convenience function that: 1. Starts from a snapshot's data (if provided) or empty map 2. Applies events in order 3. Returns the finalized aggregate state","ref":"reckon_db_aggregator.html#aggregate/3"},{"type":"type","title":"reckon_db_aggregator.event/0","doc":"","ref":"reckon_db_aggregator.html#t:event/0"},{"type":"function","title":"reckon_db_aggregator.finalize/1","doc":"Finalize a tagged map by unwrapping all tagged values Converts {sum, N} to N and {overwrite, V} to V.","ref":"reckon_db_aggregator.html#finalize/1"},{"type":"function","title":"reckon_db_aggregator.foldl/1","doc":"Fold a list of events from left to right (chronological order) Events should be sorted by version in ascending order. Returns a tagged map that can be finalized with finalize/1.","ref":"reckon_db_aggregator.html#foldl/1"},{"type":"function","title":"reckon_db_aggregator.foldl/2","doc":"Fold events with an initial state","ref":"reckon_db_aggregator.html#foldl/2"},{"type":"function","title":"reckon_db_aggregator.foldr/1","doc":"Fold a list of events from right to left Events should be sorted by version in ascending order. This will process them in reverse (newest first).","ref":"reckon_db_aggregator.html#foldr/1"},{"type":"function","title":"reckon_db_aggregator.foldr/2","doc":"Fold events from right with an initial state","ref":"reckon_db_aggregator.html#foldr/2"},{"type":"type","title":"reckon_db_aggregator.snapshot/0","doc":"","ref":"reckon_db_aggregator.html#t:snapshot/0"},{"type":"type","title":"reckon_db_aggregator.tagged_map/0","doc":"","ref":"reckon_db_aggregator.html#t:tagged_map/0"},{"type":"type","title":"reckon_db_aggregator.tagged_value/0","doc":"","ref":"reckon_db_aggregator.html#t:tagged_value/0"},{"type":"module","title":"reckon_db_app","doc":"Application behaviour for reckon-db","ref":"reckon_db_app.html"},{"type":"function","title":"reckon_db_app.start/0","doc":"Start the reckon_db application","ref":"reckon_db_app.html#start/0"},{"type":"function","title":"reckon_db_app.stop/0","doc":"Stop the reckon_db application","ref":"reckon_db_app.html#stop/0"},{"type":"behaviour","title":"reckon_db_archive_backend","doc":"Archive backend behaviour for reckon-db Defines the interface for archive storage backends. Implementations can store archived events in various formats: - Local files - S3/object storage - Network storage","ref":"reckon_db_archive_backend.html"},{"type":"callback","title":"reckon_db_archive_backend.archive/3","doc":"","ref":"reckon_db_archive_backend.html#c:archive/3"},{"type":"callback","title":"reckon_db_archive_backend.delete/2","doc":"","ref":"reckon_db_archive_backend.html#c:delete/2"},{"type":"callback","title":"reckon_db_archive_backend.exists/2","doc":"","ref":"reckon_db_archive_backend.html#c:exists/2"},{"type":"callback","title":"reckon_db_archive_backend.init/1","doc":"","ref":"reckon_db_archive_backend.html#c:init/1"},{"type":"callback","title":"reckon_db_archive_backend.list/3","doc":"","ref":"reckon_db_archive_backend.html#c:list/3"},{"type":"function","title":"reckon_db_archive_backend.make_key/4","doc":"Generate a standard archive key.","ref":"reckon_db_archive_backend.html#make_key/4"},{"type":"function","title":"reckon_db_archive_backend.parse_key/1","doc":"Parse an archive key to extract metadata.","ref":"reckon_db_archive_backend.html#parse_key/1"},{"type":"callback","title":"reckon_db_archive_backend.read/2","doc":"","ref":"reckon_db_archive_backend.html#c:read/2"},{"type":"module","title":"reckon_db_archive_file","doc":"File-based archive backend for reckon-db Stores archived events as Erlang term files on the local filesystem. Archives are organized in directories by store and stream. Directory structure: <span class=\"w\">  </span><span class=\"p\" data-group-id=\"4135657964-1\">{</span><span class=\"ss\">base_dir</span><span class=\"p\" data-group-id=\"4135657964-1\">}</span><span class=\"o\">/</span><span class=\"w\">\n    </span><span class=\"p\" data-group-id=\"4135657964-2\">{</span><span class=\"ss\">store_id</span><span class=\"p\" data-group-id=\"4135657964-2\">}</span><span class=\"o\">/</span><span class=\"w\">\n      </span><span class=\"p\" data-group-id=\"4135657964-3\">{</span><span class=\"ss\">stream_id</span><span class=\"p\" data-group-id=\"4135657964-3\">}</span><span class=\"o\">/</span><span class=\"w\">\n        </span><span class=\"mi\">0</span><span class=\"o\">-</span><span class=\"mi\">99</span><span class=\"p\">.</span><span class=\"ss\">archive</span><span class=\"w\">\n        </span><span class=\"mi\">100</span><span class=\"o\">-</span><span class=\"mi\">199</span><span class=\"p\">.</span><span class=\"ss\">archive</span>","ref":"reckon_db_archive_file.html"},{"type":"function","title":"reckon_db_archive_file.archive/3","doc":"Archive events to a file.","ref":"reckon_db_archive_file.html#archive/3"},{"type":"function","title":"reckon_db_archive_file.delete/2","doc":"Delete an archive file.","ref":"reckon_db_archive_file.html#delete/2"},{"type":"type","title":"reckon_db_archive_file.event/0","doc":"","ref":"reckon_db_archive_file.html#t:event/0"},{"type":"function","title":"reckon_db_archive_file.exists/2","doc":"Check if an archive exists.","ref":"reckon_db_archive_file.html#exists/2"},{"type":"function","title":"reckon_db_archive_file.init/1","doc":"Initialize the file archive backend. Options: - base_dir: Directory where archives are stored (required)","ref":"reckon_db_archive_file.html#init/1"},{"type":"function","title":"reckon_db_archive_file.list/3","doc":"List all archive keys for a stream.","ref":"reckon_db_archive_file.html#list/3"},{"type":"function","title":"reckon_db_archive_file.read/2","doc":"Read events from an archive file.","ref":"reckon_db_archive_file.html#read/2"},{"type":"module","title":"reckon_db_backpressure","doc":"Backpressure management for reckon-db subscriptions Provides queue-based backpressure handling to prevent memory explosion when subscribers are slower than event producers. Features: - Configurable queue size limits - Multiple overflow strategies (drop_oldest, drop_newest, block, error) - Pull mode for explicit demand - Warning thresholds with telemetry Usage: <span class=\"w\">  </span><span class=\"p\" data-group-id=\"2653513938-1\">{</span><span class=\"ss\">ok</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"n\">Queue</span><span class=\"p\" data-group-id=\"2653513938-1\">}</span><span class=\"w\"> </span><span class=\"o\">=</span><span class=\"w\"> </span><span class=\"nc\">reckon_db_backpressure</span><span class=\"p\">:</span><span class=\"nf\">new</span><span class=\"p\" data-group-id=\"2653513938-2\">(</span><span class=\"p\" data-group-id=\"2653513938-3\">#{</span><span class=\"w\">\n      </span><span class=\"ss\">max_queue</span><span class=\"w\"> </span><span class=\"p\">=&gt;</span><span class=\"w\"> </span><span class=\"mi\">1000</span><span class=\"p\">,</span><span class=\"w\">\n      </span><span class=\"ss\">strategy</span><span class=\"w\"> </span><span class=\"p\">=&gt;</span><span class=\"w\"> </span><span class=\"ss\">drop_oldest</span><span class=\"p\">,</span><span class=\"w\">\n      </span><span class=\"ss\">warning_threshold</span><span class=\"w\"> </span><span class=\"p\">=&gt;</span><span class=\"w\"> </span><span class=\"mi\">800</span><span class=\"w\">\n  </span><span class=\"p\" data-group-id=\"2653513938-3\">}</span><span class=\"p\" data-group-id=\"2653513938-2\">)</span><span class=\"p\">,</span><span class=\"w\">\n  </span><span class=\"p\" data-group-id=\"2653513938-4\">{</span><span class=\"ss\">ok</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"n\">Queue2</span><span class=\"p\" data-group-id=\"2653513938-4\">}</span><span class=\"w\"> </span><span class=\"o\">=</span><span class=\"w\"> </span><span class=\"nc\">reckon_db_backpressure</span><span class=\"p\">:</span><span class=\"nf\">enqueue</span><span class=\"p\" data-group-id=\"2653513938-5\">(</span><span class=\"n\">Queue</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"n\">Event</span><span class=\"p\" data-group-id=\"2653513938-5\">)</span><span class=\"p\">,</span><span class=\"w\">\n  </span><span class=\"p\" data-group-id=\"2653513938-6\">{</span><span class=\"ss\">ok</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"n\">Events</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"n\">Queue3</span><span class=\"p\" data-group-id=\"2653513938-6\">}</span><span class=\"w\"> </span><span class=\"o\">=</span><span class=\"w\"> </span><span class=\"nc\">reckon_db_backpressure</span><span class=\"p\">:</span><span class=\"nf\">dequeue</span><span class=\"p\" data-group-id=\"2653513938-7\">(</span><span class=\"n\">Queue2</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"mi\">10</span><span class=\"p\" data-group-id=\"2653513938-7\">)</span><span class=\"p\">.</span>","ref":"reckon_db_backpressure.html"},{"type":"function","title":"reckon_db_backpressure.add_demand/2","doc":"Add to demand (for pull mode).","ref":"reckon_db_backpressure.html#add_demand/2"},{"type":"type","title":"reckon_db_backpressure.bp_opts/0","doc":"","ref":"reckon_db_backpressure.html#t:bp_opts/0"},{"type":"type","title":"reckon_db_backpressure.bp_queue/0","doc":"","ref":"reckon_db_backpressure.html#t:bp_queue/0"},{"type":"function","title":"reckon_db_backpressure.dequeue/2","doc":"Dequeue up to N events.","ref":"reckon_db_backpressure.html#dequeue/2"},{"type":"function","title":"reckon_db_backpressure.dequeue_all/1","doc":"Dequeue all events.","ref":"reckon_db_backpressure.html#dequeue_all/1"},{"type":"function","title":"reckon_db_backpressure.enqueue/2","doc":"Enqueue an event, applying backpressure strategy if full.","ref":"reckon_db_backpressure.html#enqueue/2"},{"type":"function","title":"reckon_db_backpressure.enqueue_many/2","doc":"Enqueue multiple events.","ref":"reckon_db_backpressure.html#enqueue_many/2"},{"type":"type","title":"reckon_db_backpressure.event/0","doc":"","ref":"reckon_db_backpressure.html#t:event/0"},{"type":"function","title":"reckon_db_backpressure.get_demand/1","doc":"Get current demand.","ref":"reckon_db_backpressure.html#get_demand/1"},{"type":"function","title":"reckon_db_backpressure.info/1","doc":"Get queue statistics.","ref":"reckon_db_backpressure.html#info/1"},{"type":"function","title":"reckon_db_backpressure.is_empty/1","doc":"Check if queue is empty.","ref":"reckon_db_backpressure.html#is_empty/1"},{"type":"function","title":"reckon_db_backpressure.is_full/1","doc":"Check if queue is at capacity.","ref":"reckon_db_backpressure.html#is_full/1"},{"type":"type","title":"reckon_db_backpressure.mode/0","doc":"","ref":"reckon_db_backpressure.html#t:mode/0"},{"type":"function","title":"reckon_db_backpressure.new/1","doc":"Create a new backpressure queue.","ref":"reckon_db_backpressure.html#new/1"},{"type":"function","title":"reckon_db_backpressure.set_demand/2","doc":"Set demand (for pull mode).","ref":"reckon_db_backpressure.html#set_demand/2"},{"type":"function","title":"reckon_db_backpressure.size/1","doc":"Get current queue size.","ref":"reckon_db_backpressure.html#size/1"},{"type":"type","title":"reckon_db_backpressure.strategy/0","doc":"","ref":"reckon_db_backpressure.html#t:strategy/0"},{"type":"module","title":"reckon_db_causation","doc":"Causation and correlation tracking for reckon-db Provides functionality to trace event lineage: - Causation ID: Links an event to its direct cause - Correlation ID: Groups related events in a business process/saga - Actor ID: Identifies who/what triggered the event Standard metadata fields (convention): <span class=\"w\">  </span><span class=\"p\" data-group-id=\"6780319565-1\">#{</span><span class=\"w\">\n      </span><span class=\"ss\">causation_id</span><span class=\"w\"> </span><span class=\"p\">=&gt;</span><span class=\"w\"> </span><span class=\"nf\">binary</span><span class=\"p\" data-group-id=\"6780319565-2\">(</span><span class=\"p\" data-group-id=\"6780319565-2\">)</span><span class=\"p\">,</span><span class=\"w\">     </span><span class=\"c1\">%% ID of event/command that caused this</span><span class=\"w\">\n      </span><span class=\"ss\">correlation_id</span><span class=\"w\"> </span><span class=\"p\">=&gt;</span><span class=\"w\"> </span><span class=\"nf\">binary</span><span class=\"p\" data-group-id=\"6780319565-3\">(</span><span class=\"p\" data-group-id=\"6780319565-3\">)</span><span class=\"p\">,</span><span class=\"w\">   </span><span class=\"c1\">%% Business process/saga ID</span><span class=\"w\">\n      </span><span class=\"ss\">actor_id</span><span class=\"w\"> </span><span class=\"p\">=&gt;</span><span class=\"w\"> </span><span class=\"nf\">binary</span><span class=\"p\" data-group-id=\"6780319565-4\">(</span><span class=\"p\" data-group-id=\"6780319565-4\">)</span><span class=\"w\">          </span><span class=\"c1\">%% Who/what triggered this</span><span class=\"w\">\n  </span><span class=\"p\" data-group-id=\"6780319565-1\">}</span> Use cases: - Debugging distributed event flows - Audit trails - Saga/process manager state reconstruction - Dependency analysis","ref":"reckon_db_causation.html"},{"type":"function","title":"reckon_db_causation.build_graph/2","doc":"Build a causation graph for visualization. Accepts either an event_id (builds graph from that event) or a correlation_id (builds graph from all correlated events). Returns nodes and edges suitable for graph rendering.","ref":"reckon_db_causation.html#build_graph/2"},{"type":"type","title":"reckon_db_causation.causation_graph/0","doc":"","ref":"reckon_db_causation.html#t:causation_graph/0"},{"type":"type","title":"reckon_db_causation.event/0","doc":"","ref":"reckon_db_causation.html#t:event/0"},{"type":"function","title":"reckon_db_causation.get_cause/2","doc":"Get the event that caused the given event. Finds the event whose event_id matches this event's causation_id.","ref":"reckon_db_causation.html#get_cause/2"},{"type":"function","title":"reckon_db_causation.get_chain/2","doc":"Get the full causation chain from root to the given event. Walks backward through causation_id links until reaching an event with no cause. Returns events in order from root to target.","ref":"reckon_db_causation.html#get_chain/2"},{"type":"function","title":"reckon_db_causation.get_correlated/2","doc":"Get all events sharing the same correlation ID. Useful for finding all events in a saga or business process.","ref":"reckon_db_causation.html#get_correlated/2"},{"type":"function","title":"reckon_db_causation.get_effects/2","doc":"Get all events caused by the given event. Returns events whose causation_id matches the given event_id.","ref":"reckon_db_causation.html#get_effects/2"},{"type":"function","title":"reckon_db_causation.to_dot/1","doc":"Export a causation graph as DOT format for Graphviz. Usage: dot -Tpng -o graph.png with the output.","ref":"reckon_db_causation.html#to_dot/1"},{"type":"module","title":"reckon_db_cluster_sup","doc":"Cluster supervisor for reckon-db Manages cluster-related components (cluster mode only): - Discovery (UDP multicast / K8s DNS) - Store coordinator (cluster join coordination) - Node monitor (health probing)","ref":"reckon_db_cluster_sup.html"},{"type":"function","title":"reckon_db_cluster_sup.start_link/1","doc":"Start the cluster supervisor","ref":"reckon_db_cluster_sup.html#start_link/1"},{"type":"type","title":"reckon_db_cluster_sup.store_config/0","doc":"","ref":"reckon_db_cluster_sup.html#t:store_config/0"},{"type":"module","title":"reckon_db_config","doc":"Configuration management for reckon-db Handles reading and validating store configurations from the application environment.","ref":"reckon_db_config.html"},{"type":"function","title":"reckon_db_config.get_all_store_configs/0","doc":"Get all configured store configurations","ref":"reckon_db_config.html#get_all_store_configs/0"},{"type":"function","title":"reckon_db_config.get_env/2","doc":"Get an application environment value","ref":"reckon_db_config.html#get_env/2"},{"type":"function","title":"reckon_db_config.get_env/3","doc":"Get an application environment value with a specific app","ref":"reckon_db_config.html#get_env/3"},{"type":"function","title":"reckon_db_config.get_store_config/1","doc":"Get configuration for a specific store","ref":"reckon_db_config.html#get_store_config/1"},{"type":"type","title":"reckon_db_config.store_config/0","doc":"","ref":"reckon_db_config.html#t:store_config/0"},{"type":"module","title":"reckon_db_consistency_checker","doc":"Cluster consistency checker for reckon-db Provides active split-brain detection and cluster health verification. Implements multi-layer consistency checking: 1. Membership Consensus - All nodes agree on cluster membership 2. Raft Log Consistency - Log terms and indices match across followers 3. Leader Consensus - All nodes agree on who the leader is 4. Quorum Verification - Sufficient nodes available for operations Split-Brain Detection: Split-brain occurs when network partitions cause nodes to form independent clusters. This module detects such scenarios by: - Collecting membership views from all nodes via RPC - Comparing views to find inconsistencies - Detecting when nodes report different leaders - Identifying when quorum is at risk Academic References: - Ongaro, D. and Ousterhout, J. (2014). In Search of an Understandable Consensus Algorithm (Raft). USENIX ATC 2014. - Brewer, E. (2012). CAP Twelve Years Later: How the \"Rules\" Have Changed. IEEE Computer, 45(2), 23-29. See also:  reckon_db_health_prober .","ref":"reckon_db_consistency_checker.html"},{"type":"type","title":"reckon_db_consistency_checker.check_detail/0","doc":"","ref":"reckon_db_consistency_checker.html#t:check_detail/0"},{"type":"function","title":"reckon_db_consistency_checker.check_now/1","doc":"Force an immediate consistency check","ref":"reckon_db_consistency_checker.html#check_now/1"},{"type":"type","title":"reckon_db_consistency_checker.check_result/0","doc":"","ref":"reckon_db_consistency_checker.html#t:check_result/0"},{"type":"type","title":"reckon_db_consistency_checker.consistency_status/0","doc":"","ref":"reckon_db_consistency_checker.html#t:consistency_status/0"},{"type":"function","title":"reckon_db_consistency_checker.get_quorum_status/1","doc":"Get current quorum status Returns quorum availability and margin information.","ref":"reckon_db_consistency_checker.html#get_quorum_status/1"},{"type":"function","title":"reckon_db_consistency_checker.get_status/1","doc":"Get current consistency status","ref":"reckon_db_consistency_checker.html#get_status/1"},{"type":"function","title":"reckon_db_consistency_checker.handle_call/3","doc":"","ref":"reckon_db_consistency_checker.html#handle_call/3"},{"type":"function","title":"reckon_db_consistency_checker.handle_cast/2","doc":"","ref":"reckon_db_consistency_checker.html#handle_cast/2"},{"type":"function","title":"reckon_db_consistency_checker.handle_info/2","doc":"","ref":"reckon_db_consistency_checker.html#handle_info/2"},{"type":"function","title":"reckon_db_consistency_checker.init/1","doc":"","ref":"reckon_db_consistency_checker.html#init/1"},{"type":"function","title":"reckon_db_consistency_checker.on_status_change/2","doc":"Register a callback for status changes","ref":"reckon_db_consistency_checker.html#on_status_change/2"},{"type":"function","title":"reckon_db_consistency_checker.remove_callback/2","doc":"Remove a previously registered callback","ref":"reckon_db_consistency_checker.html#remove_callback/2"},{"type":"function","title":"reckon_db_consistency_checker.start_link/1","doc":"Start the consistency checker","ref":"reckon_db_consistency_checker.html#start_link/1"},{"type":"type","title":"reckon_db_consistency_checker.store_config/0","doc":"","ref":"reckon_db_consistency_checker.html#t:store_config/0"},{"type":"function","title":"reckon_db_consistency_checker.terminate/2","doc":"","ref":"reckon_db_consistency_checker.html#terminate/2"},{"type":"function","title":"reckon_db_consistency_checker.verify_leader_consensus/1","doc":"Verify all nodes agree on the current leader","ref":"reckon_db_consistency_checker.html#verify_leader_consensus/1"},{"type":"function","title":"reckon_db_consistency_checker.verify_membership_consensus/1","doc":"Verify membership consensus across all cluster nodes Collects membership views from each node and compares them. Returns consensus if all nodes agree, split_brain if they disagree.","ref":"reckon_db_consistency_checker.html#verify_membership_consensus/1"},{"type":"function","title":"reckon_db_consistency_checker.verify_raft_consistency/1","doc":"Verify Raft log consistency across cluster Checks that follower nodes have consistent log terms and indices. Significant divergence may indicate replication issues.","ref":"reckon_db_consistency_checker.html#verify_raft_consistency/1"},{"type":"module","title":"reckon_db_core_sup","doc":"Core supervisor for reckon-db Manages tightly-coupled core components using one_for_all strategy: - PersistenceSystem (Khepri store, streams, snapshots, subscriptions) - NotificationSystem (leader, emitters) - StoreMgr (store lifecycle coordination) If any child fails, all children are restarted to ensure consistency.","ref":"reckon_db_core_sup.html"},{"type":"function","title":"reckon_db_core_sup.start_link/1","doc":"Start the core supervisor","ref":"reckon_db_core_sup.html#start_link/1"},{"type":"type","title":"reckon_db_core_sup.store_config/0","doc":"","ref":"reckon_db_core_sup.html#t:store_config/0"},{"type":"module","title":"reckon_db_discovery","doc":"Cluster discovery for reckon-db Handles node discovery via UDP multicast (LAN) or Kubernetes DNS. Ported from LibCluster's gossip strategy. Protocol: 1. Broadcast {gossip, Node, ClusterSecret, Timestamp} every BROADCAST_INTERVAL 2. On receive: verify secret, call net_kernel:connect_node/1 3. On node up: trigger Khepri cluster join via StoreCoordinator","ref":"reckon_db_discovery.html"},{"type":"function","title":"reckon_db_discovery.get_discovered_nodes/1","doc":"Get list of discovered nodes","ref":"reckon_db_discovery.html#get_discovered_nodes/1"},{"type":"function","title":"reckon_db_discovery.handle_call/3","doc":"","ref":"reckon_db_discovery.html#handle_call/3"},{"type":"function","title":"reckon_db_discovery.handle_cast/2","doc":"","ref":"reckon_db_discovery.html#handle_cast/2"},{"type":"function","title":"reckon_db_discovery.handle_info/2","doc":"","ref":"reckon_db_discovery.html#handle_info/2"},{"type":"function","title":"reckon_db_discovery.init/1","doc":"","ref":"reckon_db_discovery.html#init/1"},{"type":"function","title":"reckon_db_discovery.start_link/1","doc":"","ref":"reckon_db_discovery.html#start_link/1"},{"type":"type","title":"reckon_db_discovery.store_config/0","doc":"","ref":"reckon_db_discovery.html#t:store_config/0"},{"type":"function","title":"reckon_db_discovery.terminate/2","doc":"","ref":"reckon_db_discovery.html#terminate/2"},{"type":"function","title":"reckon_db_discovery.trigger_discovery/1","doc":"Trigger immediate discovery broadcast","ref":"reckon_db_discovery.html#trigger_discovery/1"},{"type":"module","title":"reckon_db_emitter","doc":"Emitter worker for reckon-db A gen_server that handles event broadcasting for subscriptions. Each subscription has a pool of emitter workers that receive events from Khepri triggers and forward them to subscribers. Message types handled: - {broadcast, Topic, Event}: Broadcast event to all subscribers on topic - {forward_to_local, Topic, Event}: Forward event locally (optimization) - {events, [Event]}: Direct event delivery to subscriber pid","ref":"reckon_db_emitter.html"},{"type":"function","title":"reckon_db_emitter.child_spec/4","doc":"Create a child spec for the emitter worker","ref":"reckon_db_emitter.html#child_spec/4"},{"type":"function","title":"reckon_db_emitter.code_change/3","doc":"","ref":"reckon_db_emitter.html#code_change/3"},{"type":"type","title":"reckon_db_emitter.event/0","doc":"","ref":"reckon_db_emitter.html#t:event/0"},{"type":"function","title":"reckon_db_emitter.handle_call/3","doc":"","ref":"reckon_db_emitter.html#handle_call/3"},{"type":"function","title":"reckon_db_emitter.handle_cast/2","doc":"","ref":"reckon_db_emitter.html#handle_cast/2"},{"type":"function","title":"reckon_db_emitter.handle_info/2","doc":"","ref":"reckon_db_emitter.html#handle_info/2"},{"type":"function","title":"reckon_db_emitter.init/1","doc":"","ref":"reckon_db_emitter.html#init/1"},{"type":"function","title":"reckon_db_emitter.start_link/4","doc":"Start an emitter worker","ref":"reckon_db_emitter.html#start_link/4"},{"type":"function","title":"reckon_db_emitter.terminate/2","doc":"","ref":"reckon_db_emitter.html#terminate/2"},{"type":"function","title":"reckon_db_emitter.update_subscriber/2","doc":"Update the subscriber pid","ref":"reckon_db_emitter.html#update_subscriber/2"},{"type":"module","title":"reckon_db_emitter_group","doc":"Emitter group management for reckon-db Uses pg (process groups) for managing emitter workers. Emitters are responsible for broadcasting events to subscribers. This module provides: - Process group management for emitter workers - Random emitter selection for load distribution - Topic generation for pub/sub - Emitter name generation for registration","ref":"reckon_db_emitter_group.html"},{"type":"function","title":"reckon_db_emitter_group.broadcast/3","doc":"Broadcast an event to a random emitter in the group If the selected emitter is on the local node, sends a forward_to_local message for optimized local delivery. Otherwise sends a broadcast message.","ref":"reckon_db_emitter_group.html#broadcast/3"},{"type":"function","title":"reckon_db_emitter_group.emitter_name/2","doc":"Generate the base emitter name for a subscription","ref":"reckon_db_emitter_group.html#emitter_name/2"},{"type":"function","title":"reckon_db_emitter_group.emitter_name/3","doc":"Generate a numbered emitter name for a subscription","ref":"reckon_db_emitter_group.html#emitter_name/3"},{"type":"type","title":"reckon_db_emitter_group.event/0","doc":"","ref":"reckon_db_emitter_group.html#t:event/0"},{"type":"function","title":"reckon_db_emitter_group.group_key/2","doc":"Generate the group key for a subscription's emitters","ref":"reckon_db_emitter_group.html#group_key/2"},{"type":"function","title":"reckon_db_emitter_group.join/3","doc":"Join one or more processes to the emitter group","ref":"reckon_db_emitter_group.html#join/3"},{"type":"function","title":"reckon_db_emitter_group.leave/3","doc":"Remove one or more processes from the emitter group","ref":"reckon_db_emitter_group.html#leave/3"},{"type":"function","title":"reckon_db_emitter_group.members/2","doc":"Get all member processes in the emitter group","ref":"reckon_db_emitter_group.html#members/2"},{"type":"function","title":"reckon_db_emitter_group.persist_emitters/3","doc":"Persist emitter names to persistent_term for fast retrieval","ref":"reckon_db_emitter_group.html#persist_emitters/3"},{"type":"function","title":"reckon_db_emitter_group.retrieve_emitters/2","doc":"Retrieve previously persisted emitter names","ref":"reckon_db_emitter_group.html#retrieve_emitters/2"},{"type":"type","title":"reckon_db_emitter_group.store_id/0","doc":"","ref":"reckon_db_emitter_group.html#t:store_id/0"},{"type":"type","title":"reckon_db_emitter_group.subscription_id/0","doc":"","ref":"reckon_db_emitter_group.html#t:subscription_id/0"},{"type":"function","title":"reckon_db_emitter_group.topic/2","doc":"Generate the topic name for a subscription Special case: the binary <<\"$all\">> creates a topic for all events","ref":"reckon_db_emitter_group.html#topic/2"},{"type":"module","title":"reckon_db_emitter_pool","doc":"Emitter pool supervisor for reckon-db Supervises a pool of emitter workers for a single subscription. Each subscription can have multiple emitter workers for load distribution.","ref":"reckon_db_emitter_pool.html"},{"type":"function","title":"reckon_db_emitter_pool.init/1","doc":"","ref":"reckon_db_emitter_pool.html#init/1"},{"type":"function","title":"reckon_db_emitter_pool.name/2","doc":"Generate the name for an emitter pool","ref":"reckon_db_emitter_pool.html#name/2"},{"type":"function","title":"reckon_db_emitter_pool.start_emitter/2","doc":"Start an emitter pool for a subscription","ref":"reckon_db_emitter_pool.html#start_emitter/2"},{"type":"function","title":"reckon_db_emitter_pool.start_link/2","doc":"Start the emitter pool supervisor","ref":"reckon_db_emitter_pool.html#start_link/2"},{"type":"function","title":"reckon_db_emitter_pool.stop/2","doc":"Stop an emitter pool by key (deprecated, use stop_emitter/2)","ref":"reckon_db_emitter_pool.html#stop/2"},{"type":"function","title":"reckon_db_emitter_pool.stop_emitter/2","doc":"Stop an emitter pool for a subscription","ref":"reckon_db_emitter_pool.html#stop_emitter/2"},{"type":"type","title":"reckon_db_emitter_pool.subscription/0","doc":"","ref":"reckon_db_emitter_pool.html#t:subscription/0"},{"type":"type","title":"reckon_db_emitter_pool.subscription_type/0","doc":"","ref":"reckon_db_emitter_pool.html#t:subscription_type/0"},{"type":"function","title":"reckon_db_emitter_pool.update_emitter/2","doc":"Update an emitter pool configuration","ref":"reckon_db_emitter_pool.html#update_emitter/2"},{"type":"module","title":"reckon_db_emitter_sup","doc":"Emitter supervisor for reckon-db Manages emitter pools for subscriptions. Emitter pools are created dynamically when subscriptions are registered.","ref":"reckon_db_emitter_sup.html"},{"type":"function","title":"reckon_db_emitter_sup.start_emitter_pool/2","doc":"Start an emitter pool for a subscription","ref":"reckon_db_emitter_sup.html#start_emitter_pool/2"},{"type":"function","title":"reckon_db_emitter_sup.start_link/1","doc":"Start the emitter supervisor","ref":"reckon_db_emitter_sup.html#start_link/1"},{"type":"function","title":"reckon_db_emitter_sup.stop_emitter_pool/2","doc":"Stop an emitter pool","ref":"reckon_db_emitter_sup.html#stop_emitter_pool/2"},{"type":"type","title":"reckon_db_emitter_sup.store_config/0","doc":"","ref":"reckon_db_emitter_sup.html#t:store_config/0"},{"type":"type","title":"reckon_db_emitter_sup.subscription/0","doc":"","ref":"reckon_db_emitter_sup.html#t:subscription/0"},{"type":"type","title":"reckon_db_emitter_sup.subscription_type/0","doc":"","ref":"reckon_db_emitter_sup.html#t:subscription_type/0"},{"type":"module","title":"reckon_db_filters","doc":"Khepri event filters for reckon-db Provides filter builders for Khepri event subscriptions. These filters are used to watch for new events matching specific criteria.","ref":"reckon_db_filters.html"},{"type":"function","title":"reckon_db_filters.by_event_pattern/1","doc":"Create a filter matching events with a specific pattern in their metadata The pattern is a map that must be a subset of the event record.","ref":"reckon_db_filters.html#by_event_pattern/1"},{"type":"function","title":"reckon_db_filters.by_event_payload/1","doc":"Create a filter matching events with a specific pattern in their payload The pattern is checked against the data field of the event.","ref":"reckon_db_filters.html#by_event_payload/1"},{"type":"function","title":"reckon_db_filters.by_event_type/1","doc":"Create a filter for events of a specific type","ref":"reckon_db_filters.html#by_event_type/1"},{"type":"function","title":"reckon_db_filters.by_stream/1","doc":"Create a filter for all events in a specific stream Special case: the binary <<\"$all\">> matches events in all streams.","ref":"reckon_db_filters.html#by_stream/1"},{"type":"module","title":"reckon_db_gateway_sup","doc":"Gateway supervisor for reckon-db Manages a pool of gateway workers that provide the external interface for the event store. Starts last in the supervision hierarchy to ensure the system is fully operational before accepting external requests. Gateway Worker Pool The supervisor starts gateway_pool_size workers (default 1) for load distribution. Each worker registers with reckon-gater independently, allowing round-robin load balancing across all workers. Gateway Integration Gateway workers register themselves with reckon-gater (when available) to enable load-balanced, distributed access to the event store. When reckon-gater is not available, the gateway workers still run locally but are not registered for external load balancing.","ref":"reckon_db_gateway_sup.html"},{"type":"function","title":"reckon_db_gateway_sup.start_link/1","doc":"Start the gateway supervisor","ref":"reckon_db_gateway_sup.html#start_link/1"},{"type":"type","title":"reckon_db_gateway_sup.store_config/0","doc":"","ref":"reckon_db_gateway_sup.html#t:store_config/0"},{"type":"module","title":"reckon_db_gateway_worker","doc":"Gateway worker for reckon-db This worker process acts as the gateway endpoint for a store. It registers with reckon-gater and handles incoming requests routed through the gateway API. Multiple gateway workers can run per store for load balancing. Each worker registers independently with the gater's Ra-based worker registry. The message format matches the ExESDB.GatewayWorker from the original Elixir implementation.","ref":"reckon_db_gateway_worker.html"},{"type":"function","title":"reckon_db_gateway_worker.handle_call/3","doc":"","ref":"reckon_db_gateway_worker.html#handle_call/3"},{"type":"function","title":"reckon_db_gateway_worker.handle_cast/2","doc":"","ref":"reckon_db_gateway_worker.html#handle_cast/2"},{"type":"function","title":"reckon_db_gateway_worker.handle_info/2","doc":"","ref":"reckon_db_gateway_worker.html#handle_info/2"},{"type":"function","title":"reckon_db_gateway_worker.start_link/1","doc":"Start a gateway worker for a store Workers are not locally registered to allow multiple per store. They register with reckon-gater for discovery and load balancing.","ref":"reckon_db_gateway_worker.html#start_link/1"},{"type":"type","title":"reckon_db_gateway_worker.store_config/0","doc":"","ref":"reckon_db_gateway_worker.html#t:store_config/0"},{"type":"type","title":"reckon_db_gateway_worker.subscription/0","doc":"","ref":"reckon_db_gateway_worker.html#t:subscription/0"},{"type":"type","title":"reckon_db_gateway_worker.subscription_type/0","doc":"","ref":"reckon_db_gateway_worker.html#t:subscription_type/0"},{"type":"function","title":"reckon_db_gateway_worker.terminate/2","doc":"","ref":"reckon_db_gateway_worker.html#terminate/2"},{"type":"module","title":"reckon_db_health_prober","doc":"Active health prober for reckon-db cluster nodes Implements active health probing to detect node failures faster than passive net_kernel:monitor_nodes/1 events. This is critical for timely split-brain detection and quorum management. Design Philosophy: Passive monitoring (nodeup/nodedown) can take 60+ seconds to detect failures depending on net_ticktime configuration. Active probing provides sub-second detection with configurable thresholds. Probe Types: 1. Ping Probe - net_adm:ping/1, fast but shallow 2. RPC Probe - rpc:call with actual work, deeper health check 3. Khepri Probe - khepri_cluster:members/1, verifies store health Failure Threshold: A node is only declared failed after consecutive probe failures (default: 3). This prevents transient network issues from triggering false positives. Recovery Detection: Once a node is marked failed, probing continues. When probes succeed again, the node is marked recovered and callbacks are notified. See also:  reckon_db_consistency_checker .","ref":"reckon_db_health_prober.html"},{"type":"function","title":"reckon_db_health_prober.configure/2","doc":"Update prober configuration","ref":"reckon_db_health_prober.html#configure/2"},{"type":"function","title":"reckon_db_health_prober.get_all_status/1","doc":"Get health status of all monitored nodes","ref":"reckon_db_health_prober.html#get_all_status/1"},{"type":"function","title":"reckon_db_health_prober.get_node_status/2","doc":"Get health status of a specific node","ref":"reckon_db_health_prober.html#get_node_status/2"},{"type":"function","title":"reckon_db_health_prober.handle_call/3","doc":"","ref":"reckon_db_health_prober.html#handle_call/3"},{"type":"function","title":"reckon_db_health_prober.handle_cast/2","doc":"","ref":"reckon_db_health_prober.html#handle_cast/2"},{"type":"function","title":"reckon_db_health_prober.handle_info/2","doc":"","ref":"reckon_db_health_prober.html#handle_info/2"},{"type":"function","title":"reckon_db_health_prober.health_check/1","doc":"Health check function called via RPC Returns basic node health information","ref":"reckon_db_health_prober.html#health_check/1"},{"type":"function","title":"reckon_db_health_prober.init/1","doc":"","ref":"reckon_db_health_prober.html#init/1"},{"type":"type","title":"reckon_db_health_prober.node_status/0","doc":"","ref":"reckon_db_health_prober.html#t:node_status/0"},{"type":"function","title":"reckon_db_health_prober.on_node_failed/2","doc":"Register callback for node failure events","ref":"reckon_db_health_prober.html#on_node_failed/2"},{"type":"function","title":"reckon_db_health_prober.on_node_recovered/2","doc":"Register callback for node recovery events","ref":"reckon_db_health_prober.html#on_node_recovered/2"},{"type":"type","title":"reckon_db_health_prober.probe_config/0","doc":"","ref":"reckon_db_health_prober.html#t:probe_config/0"},{"type":"function","title":"reckon_db_health_prober.probe_now/1","doc":"Force immediate probe cycle","ref":"reckon_db_health_prober.html#probe_now/1"},{"type":"function","title":"reckon_db_health_prober.remove_callback/2","doc":"Remove a previously registered callback","ref":"reckon_db_health_prober.html#remove_callback/2"},{"type":"function","title":"reckon_db_health_prober.start_link/1","doc":"Start the health prober","ref":"reckon_db_health_prober.html#start_link/1"},{"type":"type","title":"reckon_db_health_prober.store_config/0","doc":"","ref":"reckon_db_health_prober.html#t:store_config/0"},{"type":"function","title":"reckon_db_health_prober.terminate/2","doc":"","ref":"reckon_db_health_prober.html#terminate/2"},{"type":"module","title":"reckon_db_leader","doc":"Leader worker for reckon-db Handles leader responsibilities when this node is the Raft leader. Responsibilities: - Save default subscriptions (like $all stream) - Start emitter pools for active subscriptions - Coordinate leader-specific tasks","ref":"reckon_db_leader.html"},{"type":"function","title":"reckon_db_leader.activate/1","doc":"Activate leader responsibilities Called when this node becomes the cluster leader.","ref":"reckon_db_leader.html#activate/1"},{"type":"function","title":"reckon_db_leader.handle_call/3","doc":"","ref":"reckon_db_leader.html#handle_call/3"},{"type":"function","title":"reckon_db_leader.handle_cast/2","doc":"","ref":"reckon_db_leader.html#handle_cast/2"},{"type":"function","title":"reckon_db_leader.handle_info/2","doc":"","ref":"reckon_db_leader.html#handle_info/2"},{"type":"function","title":"reckon_db_leader.init/1","doc":"","ref":"reckon_db_leader.html#init/1"},{"type":"function","title":"reckon_db_leader.is_active/1","doc":"Check if leader is currently active","ref":"reckon_db_leader.html#is_active/1"},{"type":"function","title":"reckon_db_leader.start_link/1","doc":"","ref":"reckon_db_leader.html#start_link/1"},{"type":"type","title":"reckon_db_leader.store_config/0","doc":"","ref":"reckon_db_leader.html#t:store_config/0"},{"type":"type","title":"reckon_db_leader.subscription/0","doc":"","ref":"reckon_db_leader.html#t:subscription/0"},{"type":"type","title":"reckon_db_leader.subscription_type/0","doc":"","ref":"reckon_db_leader.html#t:subscription_type/0"},{"type":"function","title":"reckon_db_leader.terminate/2","doc":"","ref":"reckon_db_leader.html#terminate/2"},{"type":"module","title":"reckon_db_leader_sup","doc":"Leader supervisor for reckon-db Manages leader-related components: - Leader tracker (subscription tracking) - Leader worker (leader responsibilities)","ref":"reckon_db_leader_sup.html"},{"type":"function","title":"reckon_db_leader_sup.start_link/1","doc":"Start the leader supervisor","ref":"reckon_db_leader_sup.html#start_link/1"},{"type":"type","title":"reckon_db_leader_sup.store_config/0","doc":"","ref":"reckon_db_leader_sup.html#t:store_config/0"},{"type":"module","title":"reckon_db_leader_tracker","doc":"Leader tracker for reckon-db Tracks subscriptions and coordinates with pg groups. Responsibilities: - Observe subscription changes via tracker_group - Start emitter pools when subscriptions are created (on leader) - Stop emitter pools when subscriptions are deleted (on leader) - Update emitter pools when subscriptions are modified (on leader) Since Khepri triggers execute on the leader node, this module coordinates emitter lifecycle with subscription changes.","ref":"reckon_db_leader_tracker.html"},{"type":"function","title":"reckon_db_leader_tracker.handle_call/3","doc":"","ref":"reckon_db_leader_tracker.html#handle_call/3"},{"type":"function","title":"reckon_db_leader_tracker.handle_cast/2","doc":"","ref":"reckon_db_leader_tracker.html#handle_cast/2"},{"type":"function","title":"reckon_db_leader_tracker.handle_info/2","doc":"","ref":"reckon_db_leader_tracker.html#handle_info/2"},{"type":"function","title":"reckon_db_leader_tracker.init/1","doc":"","ref":"reckon_db_leader_tracker.html#init/1"},{"type":"function","title":"reckon_db_leader_tracker.start_link/1","doc":"","ref":"reckon_db_leader_tracker.html#start_link/1"},{"type":"type","title":"reckon_db_leader_tracker.store_config/0","doc":"","ref":"reckon_db_leader_tracker.html#t:store_config/0"},{"type":"type","title":"reckon_db_leader_tracker.subscription/0","doc":"","ref":"reckon_db_leader_tracker.html#t:subscription/0"},{"type":"type","title":"reckon_db_leader_tracker.subscription_type/0","doc":"","ref":"reckon_db_leader_tracker.html#t:subscription_type/0"},{"type":"function","title":"reckon_db_leader_tracker.terminate/2","doc":"","ref":"reckon_db_leader_tracker.html#terminate/2"},{"type":"module","title":"reckon_db_links","doc":"Stream linking and simple projections for reckon-db Provides derived streams from source streams: - Filter events based on predicates - Transform event data - Create materialized streams for specialized queries Links are live - new events are automatically propagated. Link streams can be subscribed to like regular streams. Usage: <span class=\"w\">  </span><span class=\"c1\">%% Create a link for high-value orders</span><span class=\"w\">\n  </span><span class=\"nc\">reckon_db_links</span><span class=\"p\">:</span><span class=\"nf\">create</span><span class=\"p\" data-group-id=\"5382309200-1\">(</span><span class=\"ss\">my_store</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"p\" data-group-id=\"5382309200-2\">#{</span><span class=\"w\">\n      </span><span class=\"ss\">name</span><span class=\"w\"> </span><span class=\"p\">=&gt;</span><span class=\"w\"> </span><span class=\"p\" data-group-id=\"5382309200-3\">&lt;&lt;</span><span class=\"s\">&quot;high-value-orders&quot;</span><span class=\"p\" data-group-id=\"5382309200-3\">&gt;&gt;</span><span class=\"p\">,</span><span class=\"w\">\n      </span><span class=\"ss\">source</span><span class=\"w\"> </span><span class=\"p\">=&gt;</span><span class=\"w\"> </span><span class=\"p\" data-group-id=\"5382309200-4\">#{</span><span class=\"ss\">type</span><span class=\"w\"> </span><span class=\"p\">=&gt;</span><span class=\"w\"> </span><span class=\"ss\">stream_pattern</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"ss\">pattern</span><span class=\"w\"> </span><span class=\"p\">=&gt;</span><span class=\"w\"> </span><span class=\"p\" data-group-id=\"5382309200-5\">&lt;&lt;</span><span class=\"s\">&quot;orders-*&quot;</span><span class=\"p\" data-group-id=\"5382309200-5\">&gt;&gt;</span><span class=\"p\" data-group-id=\"5382309200-4\">}</span><span class=\"p\">,</span><span class=\"w\">\n      </span><span class=\"ss\">filter</span><span class=\"w\"> </span><span class=\"p\">=&gt;</span><span class=\"w\"> </span><span class=\"nf\">fun</span><span class=\"p\" data-group-id=\"5382309200-6\">(</span><span class=\"n\">E</span><span class=\"p\" data-group-id=\"5382309200-6\">)</span><span class=\"w\"> </span><span class=\"p\">-&gt;</span><span class=\"w\"> </span><span class=\"nc\">maps</span><span class=\"p\">:</span><span class=\"nf\">get</span><span class=\"p\" data-group-id=\"5382309200-7\">(</span><span class=\"ss\">total</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"n\">E</span><span class=\"o\">#</span><span class=\"ss\">event</span><span class=\"p\">.</span><span class=\"ss\">data</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"mi\">0</span><span class=\"p\" data-group-id=\"5382309200-7\">)</span><span class=\"w\"> </span><span class=\"o\">&gt;</span><span class=\"w\"> </span><span class=\"mi\">1000</span><span class=\"w\"> </span><span class=\"k\">end</span><span class=\"p\">,</span><span class=\"w\">\n      </span><span class=\"ss\">transform</span><span class=\"w\"> </span><span class=\"p\">=&gt;</span><span class=\"w\"> </span><span class=\"nf\">fun</span><span class=\"p\" data-group-id=\"5382309200-8\">(</span><span class=\"n\">E</span><span class=\"p\" data-group-id=\"5382309200-8\">)</span><span class=\"w\"> </span><span class=\"p\">-&gt;</span><span class=\"w\"> </span><span class=\"n\">E</span><span class=\"o\">#</span><span class=\"ss\">event</span><span class=\"p\" data-group-id=\"5382309200-9\">{</span><span class=\"ss\">data</span><span class=\"w\"> </span><span class=\"o\">=</span><span class=\"w\"> </span><span class=\"n\">E</span><span class=\"o\">#</span><span class=\"ss\">event</span><span class=\"p\">.</span><span class=\"ss\">data</span><span class=\"p\" data-group-id=\"5382309200-10\">#{</span><span class=\"ss\">flagged</span><span class=\"w\"> </span><span class=\"p\">=&gt;</span><span class=\"w\"> </span><span class=\"ss\">true</span><span class=\"p\" data-group-id=\"5382309200-10\">}</span><span class=\"p\" data-group-id=\"5382309200-9\">}</span><span class=\"w\"> </span><span class=\"k\">end</span><span class=\"w\">\n  </span><span class=\"p\" data-group-id=\"5382309200-2\">}</span><span class=\"p\" data-group-id=\"5382309200-1\">)</span><span class=\"p\">.</span><span class=\"w\">\n \n  </span><span class=\"c1\">%% Subscribe to linked stream</span><span class=\"w\">\n  </span><span class=\"nc\">reckon_db_subscriptions</span><span class=\"p\">:</span><span class=\"nf\">subscribe</span><span class=\"p\" data-group-id=\"5382309200-11\">(</span><span class=\"ss\">my_store</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"ss\">stream</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"p\" data-group-id=\"5382309200-12\">&lt;&lt;</span><span class=\"s\">&quot;$link:high-value-orders&quot;</span><span class=\"p\" data-group-id=\"5382309200-12\">&gt;&gt;</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"p\">.</span><span class=\"p\">.</span><span class=\"p\">.</span><span class=\"p\" data-group-id=\"5382309200-11\">)</span><span class=\"p\">.</span>","ref":"reckon_db_links.html"},{"type":"function","title":"reckon_db_links.create/2","doc":"Create a new link. Options: - name: Link name (required, will create stream with $link prefix) - source: Source specification (stream, stream_pattern, or all) - filter: Predicate function to filter events - transform: Function to transform events - backfill: Process existing events (default: false)","ref":"reckon_db_links.html#create/2"},{"type":"function","title":"reckon_db_links.delete/2","doc":"Delete a link.","ref":"reckon_db_links.html#delete/2"},{"type":"type","title":"reckon_db_links.event/0","doc":"","ref":"reckon_db_links.html#t:event/0"},{"type":"type","title":"reckon_db_links.filter_fun/0","doc":"","ref":"reckon_db_links.html#t:filter_fun/0"},{"type":"function","title":"reckon_db_links.get/2","doc":"Get a link by name.","ref":"reckon_db_links.html#get/2"},{"type":"function","title":"reckon_db_links.info/2","doc":"Get detailed link info.","ref":"reckon_db_links.html#info/2"},{"type":"type","title":"reckon_db_links.link_info/0","doc":"","ref":"reckon_db_links.html#t:link_info/0"},{"type":"type","title":"reckon_db_links.link_spec/0","doc":"","ref":"reckon_db_links.html#t:link_spec/0"},{"type":"function","title":"reckon_db_links.list/1","doc":"List all links.","ref":"reckon_db_links.html#list/1"},{"type":"type","title":"reckon_db_links.source_spec/0","doc":"","ref":"reckon_db_links.html#t:source_spec/0"},{"type":"function","title":"reckon_db_links.start/2","doc":"Start processing a link. This will: 1. Subscribe to source stream(s) 2. Optionally backfill existing events 3. Apply filter and transform to each event 4. Write matching events to the link stream","ref":"reckon_db_links.html#start/2"},{"type":"function","title":"reckon_db_links.stop/2","doc":"Stop processing a link.","ref":"reckon_db_links.html#stop/2"},{"type":"type","title":"reckon_db_links.transform_fun/0","doc":"","ref":"reckon_db_links.html#t:transform_fun/0"},{"type":"module","title":"reckon_db_memory","doc":"Memory pressure monitoring for reckon-db Monitors system memory usage and triggers adaptive behavior when memory pressure increases. Components can register callbacks to be notified of pressure changes. Pressure levels: -  normal`: Full caching, all features enabled - `elevated`: Reduce cache sizes, flush more often - `critical`: Pause non-essential operations, aggressive cleanup Usage: ``` %% Start monitoring reckon_db_memory:start_link(). %% Check current level normal = reckon_db_memory:level(). %% Register callback reckon_db_memory:on_pressure_change(fun(Level) -> logger:info(\"Memory pressure: ~p\", [Level]) end). ''","ref":"reckon_db_memory.html"},{"type":"type","title":"reckon_db_memory.callback_fun/0","doc":"","ref":"reckon_db_memory.html#t:callback_fun/0"},{"type":"type","title":"reckon_db_memory.callback_ref/0","doc":"","ref":"reckon_db_memory.html#t:callback_ref/0"},{"type":"function","title":"reckon_db_memory.check_now/0","doc":"Force an immediate memory check.","ref":"reckon_db_memory.html#check_now/0"},{"type":"function","title":"reckon_db_memory.code_change/3","doc":"","ref":"reckon_db_memory.html#code_change/3"},{"type":"type","title":"reckon_db_memory.config/0","doc":"","ref":"reckon_db_memory.html#t:config/0"},{"type":"function","title":"reckon_db_memory.configure/1","doc":"Update configuration.","ref":"reckon_db_memory.html#configure/1"},{"type":"function","title":"reckon_db_memory.get_config/0","doc":"Get current configuration.","ref":"reckon_db_memory.html#get_config/0"},{"type":"function","title":"reckon_db_memory.get_stats/0","doc":"Get current memory statistics.","ref":"reckon_db_memory.html#get_stats/0"},{"type":"function","title":"reckon_db_memory.handle_call/3","doc":"","ref":"reckon_db_memory.html#handle_call/3"},{"type":"function","title":"reckon_db_memory.handle_cast/2","doc":"","ref":"reckon_db_memory.html#handle_cast/2"},{"type":"function","title":"reckon_db_memory.handle_info/2","doc":"","ref":"reckon_db_memory.html#handle_info/2"},{"type":"function","title":"reckon_db_memory.init/1","doc":"","ref":"reckon_db_memory.html#init/1"},{"type":"function","title":"reckon_db_memory.level/0","doc":"Get current memory pressure level.","ref":"reckon_db_memory.html#level/0"},{"type":"function","title":"reckon_db_memory.level/1","doc":"Get pressure level for a given memory usage ratio. This is a pure function useful for testing.","ref":"reckon_db_memory.html#level/1"},{"type":"function","title":"reckon_db_memory.on_pressure_change/1","doc":"Register callback for pressure level changes. Returns a reference that can be used to remove the callback.","ref":"reckon_db_memory.html#on_pressure_change/1"},{"type":"type","title":"reckon_db_memory.pressure_level/0","doc":"","ref":"reckon_db_memory.html#t:pressure_level/0"},{"type":"function","title":"reckon_db_memory.remove_callback/1","doc":"Remove a registered callback.","ref":"reckon_db_memory.html#remove_callback/1"},{"type":"function","title":"reckon_db_memory.start_link/0","doc":"Start the memory monitor with default configuration.","ref":"reckon_db_memory.html#start_link/0"},{"type":"function","title":"reckon_db_memory.start_link/1","doc":"Start the memory monitor with custom configuration.","ref":"reckon_db_memory.html#start_link/1"},{"type":"function","title":"reckon_db_memory.terminate/2","doc":"","ref":"reckon_db_memory.html#terminate/2"},{"type":"module","title":"reckon_db_naming","doc":"Process naming utilities for reckon-db Provides consistent naming conventions for all processes and registered names throughout the system.","ref":"reckon_db_naming.html"},{"type":"function","title":"reckon_db_naming.cluster_sup_name/1","doc":"Cluster supervisor name for a store","ref":"reckon_db_naming.html#cluster_sup_name/1"},{"type":"function","title":"reckon_db_naming.coordinator_name/1","doc":"Store coordinator worker name","ref":"reckon_db_naming.html#coordinator_name/1"},{"type":"function","title":"reckon_db_naming.core_sup_name/1","doc":"Core supervisor name for a store","ref":"reckon_db_naming.html#core_sup_name/1"},{"type":"function","title":"reckon_db_naming.discovery_name/1","doc":"Discovery worker name","ref":"reckon_db_naming.html#discovery_name/1"},{"type":"function","title":"reckon_db_naming.emitter_group_key/2","doc":"Emitter group key (for event distribution via pg)","ref":"reckon_db_naming.html#emitter_group_key/2"},{"type":"function","title":"reckon_db_naming.emitter_pool_name/2","doc":"Emitter pool name for a subscription","ref":"reckon_db_naming.html#emitter_pool_name/2"},{"type":"function","title":"reckon_db_naming.emitter_sup_name/1","doc":"Emitter supervisor name for a store","ref":"reckon_db_naming.html#emitter_sup_name/1"},{"type":"function","title":"reckon_db_naming.gateway_sup_name/1","doc":"Gateway supervisor name for a store","ref":"reckon_db_naming.html#gateway_sup_name/1"},{"type":"function","title":"reckon_db_naming.gateway_worker_name/1","doc":"Gateway worker name for a store","ref":"reckon_db_naming.html#gateway_worker_name/1"},{"type":"function","title":"reckon_db_naming.leader_name/1","doc":"Leader worker name","ref":"reckon_db_naming.html#leader_name/1"},{"type":"function","title":"reckon_db_naming.leader_sup_name/1","doc":"Leader supervisor name for a store","ref":"reckon_db_naming.html#leader_sup_name/1"},{"type":"function","title":"reckon_db_naming.leader_tracker_name/1","doc":"Leader tracker worker name","ref":"reckon_db_naming.html#leader_tracker_name/1"},{"type":"function","title":"reckon_db_naming.node_monitor_name/1","doc":"Node monitor worker name","ref":"reckon_db_naming.html#node_monitor_name/1"},{"type":"function","title":"reckon_db_naming.notification_sup_name/1","doc":"Notification supervisor name for a store","ref":"reckon_db_naming.html#notification_sup_name/1"},{"type":"function","title":"reckon_db_naming.persistence_sup_name/1","doc":"Persistence supervisor name for a store","ref":"reckon_db_naming.html#persistence_sup_name/1"},{"type":"function","title":"reckon_db_naming.persistence_worker_name/1","doc":"Persistence worker name","ref":"reckon_db_naming.html#persistence_worker_name/1"},{"type":"function","title":"reckon_db_naming.pg_group_name/2","doc":"Process group name for a store feature","ref":"reckon_db_naming.html#pg_group_name/2"},{"type":"function","title":"reckon_db_naming.reader_pool_name/1","doc":"Reader pool name for a store","ref":"reckon_db_naming.html#reader_pool_name/1"},{"type":"function","title":"reckon_db_naming.snapshots_store_name/1","doc":"Snapshots store worker name","ref":"reckon_db_naming.html#snapshots_store_name/1"},{"type":"function","title":"reckon_db_naming.store_mgr_name/1","doc":"Store manager worker name","ref":"reckon_db_naming.html#store_mgr_name/1"},{"type":"function","title":"reckon_db_naming.store_name/1","doc":"Khepri store name (this is the atom used for khepri operations)","ref":"reckon_db_naming.html#store_name/1"},{"type":"function","title":"reckon_db_naming.store_worker_name/1","doc":"Store worker name (the gen_server that starts Khepri) NOTE: This is different from store_name/1 (Khepri operations) and store_mgr_name/1 (store lifecycle coordination).","ref":"reckon_db_naming.html#store_worker_name/1"},{"type":"function","title":"reckon_db_naming.streams_sup_name/1","doc":"Streams supervisor name for a store","ref":"reckon_db_naming.html#streams_sup_name/1"},{"type":"function","title":"reckon_db_naming.subscriptions_store_name/1","doc":"Subscriptions store worker name","ref":"reckon_db_naming.html#subscriptions_store_name/1"},{"type":"function","title":"reckon_db_naming.system_sup_name/1","doc":"System supervisor name for a store","ref":"reckon_db_naming.html#system_sup_name/1"},{"type":"function","title":"reckon_db_naming.tracker_group_key/2","doc":"Tracker group key (for subscription tracking via pg)","ref":"reckon_db_naming.html#tracker_group_key/2"},{"type":"function","title":"reckon_db_naming.writer_pool_name/1","doc":"Writer pool name for a store","ref":"reckon_db_naming.html#writer_pool_name/1"},{"type":"module","title":"reckon_db_node_monitor","doc":"Node monitor for reckon-db Monitors cluster node health and handles node up/down events. Responsibilities: - Monitor node connectivity via net_kernel:monitor_nodes/1 - Trigger cluster join attempts on nodeup events - Track cluster membership changes - Emit telemetry on node events - Periodic leader checks","ref":"reckon_db_node_monitor.html"},{"type":"function","title":"reckon_db_node_monitor.get_members/1","doc":"Get current cluster members","ref":"reckon_db_node_monitor.html#get_members/1"},{"type":"function","title":"reckon_db_node_monitor.handle_call/3","doc":"","ref":"reckon_db_node_monitor.html#handle_call/3"},{"type":"function","title":"reckon_db_node_monitor.handle_cast/2","doc":"","ref":"reckon_db_node_monitor.html#handle_cast/2"},{"type":"function","title":"reckon_db_node_monitor.handle_info/2","doc":"","ref":"reckon_db_node_monitor.html#handle_info/2"},{"type":"function","title":"reckon_db_node_monitor.init/1","doc":"","ref":"reckon_db_node_monitor.html#init/1"},{"type":"function","title":"reckon_db_node_monitor.start_link/1","doc":"","ref":"reckon_db_node_monitor.html#start_link/1"},{"type":"type","title":"reckon_db_node_monitor.store_config/0","doc":"","ref":"reckon_db_node_monitor.html#t:store_config/0"},{"type":"function","title":"reckon_db_node_monitor.terminate/2","doc":"","ref":"reckon_db_node_monitor.html#terminate/2"},{"type":"module","title":"reckon_db_notification_sup","doc":"Notification supervisor for reckon-db Manages notification-related components: - LeaderSystem (leader responsibilities, tracking) - EmitterSystem (event distribution workers)","ref":"reckon_db_notification_sup.html"},{"type":"function","title":"reckon_db_notification_sup.start_link/1","doc":"Start the notification supervisor","ref":"reckon_db_notification_sup.html#start_link/1"},{"type":"type","title":"reckon_db_notification_sup.store_config/0","doc":"","ref":"reckon_db_notification_sup.html#t:store_config/0"},{"type":"module","title":"reckon_db_persistence_sup","doc":"Persistence supervisor for reckon-db Manages persistence-related components: - Khepri store worker - Streams supervisor (writers/readers) - Persistence worker (batched flush, currently disabled) Note: Snapshots and Subscriptions stores are facade modules that work directly with Khepri without needing gen_servers.","ref":"reckon_db_persistence_sup.html"},{"type":"function","title":"reckon_db_persistence_sup.start_link/1","doc":"Start the persistence supervisor","ref":"reckon_db_persistence_sup.html#start_link/1"},{"type":"type","title":"reckon_db_persistence_sup.store_config/0","doc":"","ref":"reckon_db_persistence_sup.html#t:store_config/0"},{"type":"module","title":"reckon_db_persistence_worker","doc":"Persistence worker for reckon-db A GenServer that handles periodic disk persistence operations. This worker batches and schedules flush operations to ensure data is persisted to disk without blocking event append operations. Features: - Configurable persistence interval (default: 5 seconds) - Batching of flush operations to reduce disk I/O - Graceful shutdown with final persistence - Per-store persistence workers NOTE: The actual flush operation is currently DISABLED as Khepri/Ra handles persistence internally via Raft consensus. This module exists for future optimization and to match the architecture of ex-esdb.","ref":"reckon_db_persistence_worker.html"},{"type":"function","title":"reckon_db_persistence_worker.force_persistence/1","doc":"Force immediate persistence of all pending stores. This is a synchronous call that blocks until persistence is complete.","ref":"reckon_db_persistence_worker.html#force_persistence/1"},{"type":"function","title":"reckon_db_persistence_worker.request_persistence/1","doc":"Request that a store's data be persisted to disk. This is a non-blocking call that queues the store for persistence. Returns ok immediately; actual persistence happens asynchronously.","ref":"reckon_db_persistence_worker.html#request_persistence/1"},{"type":"function","title":"reckon_db_persistence_worker.start_link/1","doc":"Start a persistence worker for a specific store.","ref":"reckon_db_persistence_worker.html#start_link/1"},{"type":"type","title":"reckon_db_persistence_worker.store_config/0","doc":"","ref":"reckon_db_persistence_worker.html#t:store_config/0"},{"type":"module","title":"reckon_db_scavenge","doc":"Scavenging and archival for reckon-db Provides functionality to: - Remove old events beyond a retention period - Optionally archive events before deletion - Maintain stream integrity by requiring snapshots Use cases: - Reduce storage costs by removing old events - Comply with data retention policies - Archive events to cold storage Safety guarantees: - By default, requires a snapshot before scavenging - Supports dry-run mode for previewing changes - Telemetry events for monitoring","ref":"reckon_db_scavenge.html"},{"type":"function","title":"reckon_db_scavenge.archive_and_scavenge/4","doc":"Archive events to a backend, then scavenge. First archives events to the specified backend, then deletes them. This ensures events are preserved before removal.","ref":"reckon_db_scavenge.html#archive_and_scavenge/4"},{"type":"function","title":"reckon_db_scavenge.dry_run/3","doc":"Preview what would be scavenged without making changes.","ref":"reckon_db_scavenge.html#dry_run/3"},{"type":"type","title":"reckon_db_scavenge.event/0","doc":"","ref":"reckon_db_scavenge.html#t:event/0"},{"type":"function","title":"reckon_db_scavenge.scavenge/3","doc":"Scavenge a single stream. Deletes old events based on the provided options. By default, requires a snapshot to exist (safety measure). Options: -  before`: Delete events with epoch_us before this timestamp - `before_version`: Delete events before this version (alternative to `before`) - `keep_versions`: Always keep at least N latest versions (default: 0) - `require_snapshot`: Require snapshot exists (default: true) - `dry_run`: Preview only, don t delete (default: false) Example: <span class=\"w\">  </span><span class=\"c1\">%% Delete events older than 1 year, keep last 10 versions</span><span class=\"w\">\n  </span><span class=\"n\">OneYearAgo</span><span class=\"w\"> </span><span class=\"o\">=</span><span class=\"w\"> </span><span class=\"nc\">erlang</span><span class=\"p\">:</span><span class=\"nf\">system_time</span><span class=\"p\" data-group-id=\"7198303523-1\">(</span><span class=\"ss\">microsecond</span><span class=\"p\" data-group-id=\"7198303523-1\">)</span><span class=\"w\"> </span><span class=\"o\">-</span><span class=\"w\"> </span><span class=\"p\" data-group-id=\"7198303523-2\">(</span><span class=\"mi\">365</span><span class=\"w\"> </span><span class=\"o\">*</span><span class=\"w\"> </span><span class=\"mi\">24</span><span class=\"w\"> </span><span class=\"o\">*</span><span class=\"w\"> </span><span class=\"mi\">60</span><span class=\"w\"> </span><span class=\"o\">*</span><span class=\"w\"> </span><span class=\"mi\">60</span><span class=\"w\"> </span><span class=\"o\">*</span><span class=\"w\"> </span><span class=\"mi\">1000000</span><span class=\"p\" data-group-id=\"7198303523-2\">)</span><span class=\"p\">,</span><span class=\"w\">\n  </span><span class=\"p\" data-group-id=\"7198303523-3\">{</span><span class=\"ss\">ok</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"n\">Result</span><span class=\"p\" data-group-id=\"7198303523-3\">}</span><span class=\"w\"> </span><span class=\"o\">=</span><span class=\"w\"> </span><span class=\"nc\">reckon_db_scavenge</span><span class=\"p\">:</span><span class=\"nf\">scavenge</span><span class=\"p\" data-group-id=\"7198303523-4\">(</span><span class=\"ss\">my_store</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"p\" data-group-id=\"7198303523-5\">&lt;&lt;</span><span class=\"s\">&quot;orders-123&quot;</span><span class=\"p\" data-group-id=\"7198303523-5\">&gt;&gt;</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"p\" data-group-id=\"7198303523-6\">#{</span><span class=\"w\">\n      </span><span class=\"ss\">before</span><span class=\"w\"> </span><span class=\"p\">=&gt;</span><span class=\"w\"> </span><span class=\"n\">OneYearAgo</span><span class=\"p\">,</span><span class=\"w\">\n      </span><span class=\"ss\">keep_versions</span><span class=\"w\"> </span><span class=\"p\">=&gt;</span><span class=\"w\"> </span><span class=\"mi\">10</span><span class=\"w\">\n  </span><span class=\"p\" data-group-id=\"7198303523-6\">}</span><span class=\"p\" data-group-id=\"7198303523-4\">)</span><span class=\"p\">.</span>","ref":"reckon_db_scavenge.html#scavenge/3"},{"type":"function","title":"reckon_db_scavenge.scavenge_matching/3","doc":"Scavenge all streams matching a pattern. Pattern uses shell-like wildcards (e.g., \"orders-*\"). Applies the same options to all matching streams.","ref":"reckon_db_scavenge.html#scavenge_matching/3"},{"type":"type","title":"reckon_db_scavenge.scavenge_opts/0","doc":"","ref":"reckon_db_scavenge.html#t:scavenge_opts/0"},{"type":"type","title":"reckon_db_scavenge.scavenge_result/0","doc":"","ref":"reckon_db_scavenge.html#t:scavenge_result/0"},{"type":"module","title":"reckon_db_schema","doc":"Schema registry and upcasting for reckon-db Provides schema versioning and automatic event transformation: - Register schemas for event types with version numbers - Define upcast functions to transform old versions to new - Auto-upcast events when reading from streams Event schema evolution strategies: - Weak schema: No validation, just track versions - Strong schema: Validate against JSON Schema or custom validator - Tolerant reader: Accept old versions, upcast on demand Usage: <span class=\"w\">  </span><span class=\"c1\">%% Register schema</span><span class=\"w\">\n  </span><span class=\"nc\">reckon_db_schema</span><span class=\"p\">:</span><span class=\"nf\">register</span><span class=\"p\" data-group-id=\"9839790714-1\">(</span><span class=\"ss\">my_store</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"p\" data-group-id=\"9839790714-2\">&lt;&lt;</span><span class=\"s\">&quot;OrderPlaced&quot;</span><span class=\"p\" data-group-id=\"9839790714-2\">&gt;&gt;</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"p\" data-group-id=\"9839790714-3\">#{</span><span class=\"w\">\n      </span><span class=\"ss\">version</span><span class=\"w\"> </span><span class=\"p\">=&gt;</span><span class=\"w\"> </span><span class=\"mi\">2</span><span class=\"p\">,</span><span class=\"w\">\n      </span><span class=\"ss\">upcast_from</span><span class=\"w\"> </span><span class=\"p\">=&gt;</span><span class=\"w\"> </span><span class=\"p\" data-group-id=\"9839790714-4\">#{</span><span class=\"w\">\n          </span><span class=\"mi\">1</span><span class=\"w\"> </span><span class=\"p\">=&gt;</span><span class=\"w\"> </span><span class=\"nf\">fun</span><span class=\"p\" data-group-id=\"9839790714-5\">(</span><span class=\"n\">V1Data</span><span class=\"p\" data-group-id=\"9839790714-5\">)</span><span class=\"w\"> </span><span class=\"p\">-&gt;</span><span class=\"w\"> </span><span class=\"n\">V1Data</span><span class=\"p\" data-group-id=\"9839790714-6\">#{</span><span class=\"ss\">new_field</span><span class=\"w\"> </span><span class=\"p\">=&gt;</span><span class=\"w\"> </span><span class=\"ss\">default</span><span class=\"p\" data-group-id=\"9839790714-6\">}</span><span class=\"w\"> </span><span class=\"k\">end</span><span class=\"w\">\n      </span><span class=\"p\" data-group-id=\"9839790714-4\">}</span><span class=\"w\">\n  </span><span class=\"p\" data-group-id=\"9839790714-3\">}</span><span class=\"p\" data-group-id=\"9839790714-1\">)</span><span class=\"p\">.</span><span class=\"w\">\n \n  </span><span class=\"c1\">%% Read with upcasting</span><span class=\"w\">\n  </span><span class=\"p\" data-group-id=\"9839790714-7\">{</span><span class=\"ss\">ok</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"n\">Events</span><span class=\"p\" data-group-id=\"9839790714-7\">}</span><span class=\"w\"> </span><span class=\"o\">=</span><span class=\"w\"> </span><span class=\"nc\">reckon_db_streams</span><span class=\"p\">:</span><span class=\"nf\">read</span><span class=\"p\" data-group-id=\"9839790714-8\">(</span><span class=\"ss\">my_store</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"ss\">stream</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"mi\">0</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"mi\">100</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"ss\">forward</span><span class=\"p\" data-group-id=\"9839790714-8\">)</span><span class=\"p\">,</span><span class=\"w\">\n  </span><span class=\"n\">UpcastedEvents</span><span class=\"w\"> </span><span class=\"o\">=</span><span class=\"w\"> </span><span class=\"nc\">reckon_db_schema</span><span class=\"p\">:</span><span class=\"nf\">upcast</span><span class=\"p\" data-group-id=\"9839790714-9\">(</span><span class=\"ss\">my_store</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"n\">Events</span><span class=\"p\" data-group-id=\"9839790714-9\">)</span><span class=\"p\">.</span>","ref":"reckon_db_schema.html"},{"type":"type","title":"reckon_db_schema.event/0","doc":"","ref":"reckon_db_schema.html#t:event/0"},{"type":"function","title":"reckon_db_schema.get/2","doc":"Get a schema by event type.","ref":"reckon_db_schema.html#get/2"},{"type":"function","title":"reckon_db_schema.get_version/2","doc":"Get current version for an event type.","ref":"reckon_db_schema.html#get_version/2"},{"type":"function","title":"reckon_db_schema.list/1","doc":"List all registered schemas.","ref":"reckon_db_schema.html#list/1"},{"type":"function","title":"reckon_db_schema.register/3","doc":"Register a schema for an event type. Options: - version: Schema version (required, positive integer) - upcast_from: Map of OldVersion to UpcastFun for transformations - validator: Fun to validate event data (returns ok or error tuple) - description: Human-readable description","ref":"reckon_db_schema.html#register/3"},{"type":"type","title":"reckon_db_schema.schema/0","doc":"","ref":"reckon_db_schema.html#t:schema/0"},{"type":"type","title":"reckon_db_schema.schema_info/0","doc":"","ref":"reckon_db_schema.html#t:schema_info/0"},{"type":"function","title":"reckon_db_schema.unregister/2","doc":"Unregister a schema.","ref":"reckon_db_schema.html#unregister/2"},{"type":"function","title":"reckon_db_schema.upcast/2","doc":"Upcast a list of events to their current schema versions. Events without registered schemas are returned unchanged. Events already at current version are returned unchanged.","ref":"reckon_db_schema.html#upcast/2"},{"type":"function","title":"reckon_db_schema.upcast_event/2","doc":"Upcast a single event to current schema version.","ref":"reckon_db_schema.html#upcast_event/2"},{"type":"type","title":"reckon_db_schema.upcast_fun/0","doc":"","ref":"reckon_db_schema.html#t:upcast_fun/0"},{"type":"function","title":"reckon_db_schema.validate/2","doc":"Validate an event against its registered schema.","ref":"reckon_db_schema.html#validate/2"},{"type":"type","title":"reckon_db_schema.validator_fun/0","doc":"","ref":"reckon_db_schema.html#t:validator_fun/0"},{"type":"type","title":"reckon_db_schema.version/0","doc":"","ref":"reckon_db_schema.html#t:version/0"},{"type":"module","title":"reckon_db_snapshots","doc":"Snapshots API facade for reckon-db Provides the public API for snapshot operations: - save: Save aggregate state as a snapshot - load: Load the latest snapshot for a stream - load_at: Load a specific snapshot version - list: List all snapshots for a stream - delete: Delete snapshots for a stream - exists: Check if a snapshot exists Snapshots are used to optimize event replay by storing aggregate state at specific versions.","ref":"reckon_db_snapshots.html"},{"type":"function","title":"reckon_db_snapshots.delete/2","doc":"Delete all snapshots for a stream","ref":"reckon_db_snapshots.html#delete/2"},{"type":"function","title":"reckon_db_snapshots.delete_at/3","doc":"Delete a specific snapshot version","ref":"reckon_db_snapshots.html#delete_at/3"},{"type":"function","title":"reckon_db_snapshots.exists/2","doc":"Check if any snapshot exists for a stream","ref":"reckon_db_snapshots.html#exists/2"},{"type":"function","title":"reckon_db_snapshots.exists_at/3","doc":"Check if a specific snapshot version exists","ref":"reckon_db_snapshots.html#exists_at/3"},{"type":"function","title":"reckon_db_snapshots.list/2","doc":"List all snapshots for a stream","ref":"reckon_db_snapshots.html#list/2"},{"type":"function","title":"reckon_db_snapshots.load/2","doc":"Load the latest snapshot for a stream Returns {ok, Snapshot} if found, {error, not_found} otherwise.","ref":"reckon_db_snapshots.html#load/2"},{"type":"function","title":"reckon_db_snapshots.load_at/3","doc":"Load a specific snapshot version","ref":"reckon_db_snapshots.html#load_at/3"},{"type":"function","title":"reckon_db_snapshots.save/4","doc":"Save a snapshot with default empty metadata Parameters: StoreId - The store identifier StreamId - The stream this snapshot belongs to Version - The event version this snapshot represents Data - The aggregate state to snapshot Returns ok on success or {error, Reason} on failure.","ref":"reckon_db_snapshots.html#save/4"},{"type":"function","title":"reckon_db_snapshots.save/5","doc":"Save a snapshot with metadata","ref":"reckon_db_snapshots.html#save/5"},{"type":"type","title":"reckon_db_snapshots.snapshot/0","doc":"","ref":"reckon_db_snapshots.html#t:snapshot/0"},{"type":"type","title":"reckon_db_snapshots.snapshot_data/0","doc":"","ref":"reckon_db_snapshots.html#t:snapshot_data/0"},{"type":"type","title":"reckon_db_snapshots.snapshot_metadata/0","doc":"","ref":"reckon_db_snapshots.html#t:snapshot_metadata/0"},{"type":"module","title":"reckon_db_snapshots_store","doc":"Snapshots store for reckon-db Manages snapshot persistence and retrieval directly via Khepri. Snapshots are stored at path [snapshots, StreamId, PaddedVersion].","ref":"reckon_db_snapshots_store.html"},{"type":"function","title":"reckon_db_snapshots_store.delete/2","doc":"Delete all snapshots for a stream","ref":"reckon_db_snapshots_store.html#delete/2"},{"type":"function","title":"reckon_db_snapshots_store.delete/3","doc":"Delete a specific snapshot version","ref":"reckon_db_snapshots_store.html#delete/3"},{"type":"function","title":"reckon_db_snapshots_store.exists/2","doc":"Check if any snapshot exists for a stream","ref":"reckon_db_snapshots_store.html#exists/2"},{"type":"function","title":"reckon_db_snapshots_store.exists/3","doc":"Check if a specific snapshot version exists","ref":"reckon_db_snapshots_store.html#exists/3"},{"type":"function","title":"reckon_db_snapshots_store.get/2","doc":"Get the latest snapshot for a stream","ref":"reckon_db_snapshots_store.html#get/2"},{"type":"function","title":"reckon_db_snapshots_store.get/3","doc":"Get a specific snapshot version for a stream","ref":"reckon_db_snapshots_store.html#get/3"},{"type":"function","title":"reckon_db_snapshots_store.get_latest/2","doc":"Get the latest snapshot for a stream","ref":"reckon_db_snapshots_store.html#get_latest/2"},{"type":"function","title":"reckon_db_snapshots_store.list/2","doc":"List all snapshots for a stream","ref":"reckon_db_snapshots_store.html#list/2"},{"type":"function","title":"reckon_db_snapshots_store.put/2","doc":"Store a snapshot","ref":"reckon_db_snapshots_store.html#put/2"},{"type":"type","title":"reckon_db_snapshots_store.snapshot/0","doc":"","ref":"reckon_db_snapshots_store.html#t:snapshot/0"},{"type":"type","title":"reckon_db_snapshots_store.store_id/0","doc":"","ref":"reckon_db_snapshots_store.html#t:store_id/0"},{"type":"module","title":"reckon_db_store","doc":"Khepri store lifecycle management for reckon-db Manages the Khepri store instance, including: - Starting and stopping the store - Cluster formation (in cluster mode) - Health checks","ref":"reckon_db_store.html"},{"type":"function","title":"reckon_db_store.get_leader/1","doc":"Get the current leader node for the store","ref":"reckon_db_store.html#get_leader/1"},{"type":"function","title":"reckon_db_store.get_store/1","doc":"Get the store name (for use with khepri operations)","ref":"reckon_db_store.html#get_store/1"},{"type":"function","title":"reckon_db_store.is_ready/1","doc":"Check if the store is ready","ref":"reckon_db_store.html#is_ready/1"},{"type":"function","title":"reckon_db_store.start_link/1","doc":"Start the store worker IMPORTANT: We use store_worker_name/1 for gen_server registration to avoid conflicting with Khepri's internal naming. Khepri uses the StoreId for its Ra cluster and process registration.","ref":"reckon_db_store.html#start_link/1"},{"type":"type","title":"reckon_db_store.store_config/0","doc":"","ref":"reckon_db_store.html#t:store_config/0"},{"type":"module","title":"reckon_db_store_coordinator","doc":"Store coordinator for reckon-db Coordinates cluster join operations and prevents split-brain scenarios. Responsibilities: - Detecting existing clusters via RPC - Coordinator election (lowest node name) - Coordinated cluster joining - Split-brain prevention","ref":"reckon_db_store_coordinator.html"},{"type":"function","title":"reckon_db_store_coordinator.handle_call/3","doc":"","ref":"reckon_db_store_coordinator.html#handle_call/3"},{"type":"function","title":"reckon_db_store_coordinator.handle_cast/2","doc":"","ref":"reckon_db_store_coordinator.html#handle_cast/2"},{"type":"function","title":"reckon_db_store_coordinator.handle_info/2","doc":"","ref":"reckon_db_store_coordinator.html#handle_info/2"},{"type":"function","title":"reckon_db_store_coordinator.init/1","doc":"","ref":"reckon_db_store_coordinator.html#init/1"},{"type":"function","title":"reckon_db_store_coordinator.is_leader/1","doc":"Check if this node is the leader","ref":"reckon_db_store_coordinator.html#is_leader/1"},{"type":"function","title":"reckon_db_store_coordinator.join_cluster/1","doc":"Join the Khepri cluster using coordinated approach","ref":"reckon_db_store_coordinator.html#join_cluster/1"},{"type":"function","title":"reckon_db_store_coordinator.join_cluster/2","doc":"Join a specific node's cluster","ref":"reckon_db_store_coordinator.html#join_cluster/2"},{"type":"function","title":"reckon_db_store_coordinator.leader/1","doc":"Get current leader node","ref":"reckon_db_store_coordinator.html#leader/1"},{"type":"function","title":"reckon_db_store_coordinator.members/1","doc":"Get cluster members","ref":"reckon_db_store_coordinator.html#members/1"},{"type":"function","title":"reckon_db_store_coordinator.should_handle_nodeup/1","doc":"Check if this node should handle nodeup events","ref":"reckon_db_store_coordinator.html#should_handle_nodeup/1"},{"type":"function","title":"reckon_db_store_coordinator.start_link/1","doc":"","ref":"reckon_db_store_coordinator.html#start_link/1"},{"type":"type","title":"reckon_db_store_coordinator.store_config/0","doc":"","ref":"reckon_db_store_coordinator.html#t:store_config/0"},{"type":"function","title":"reckon_db_store_coordinator.terminate/2","doc":"","ref":"reckon_db_store_coordinator.html#terminate/2"},{"type":"module","title":"reckon_db_store_mgr","doc":"Store manager for reckon-db Coordinates store lifecycle and provides store-level operations.","ref":"reckon_db_store_mgr.html"},{"type":"function","title":"reckon_db_store_mgr.get_info/1","doc":"Get store information","ref":"reckon_db_store_mgr.html#get_info/1"},{"type":"function","title":"reckon_db_store_mgr.start_link/1","doc":"Start the store manager","ref":"reckon_db_store_mgr.html#start_link/1"},{"type":"type","title":"reckon_db_store_mgr.store_config/0","doc":"","ref":"reckon_db_store_mgr.html#t:store_config/0"},{"type":"module","title":"reckon_db_streams","doc":"Streams API facade for reckon-db Provides the public API for stream operations: - append: Write events to a stream with optimistic concurrency - read: Read events from a stream - get_version: Get current stream version - exists: Check if stream exists - list_streams: List all streams in the store","ref":"reckon_db_streams.html"},{"type":"function","title":"reckon_db_streams.append/4","doc":"Append events to a stream with expected version check Expected version semantics: -1 (NO_STREAM) - Stream must not exist (first write) -2 (ANY_VERSION) - No version check, always append N >= 0 - Stream version must equal N Returns {ok, NewVersion} on success or {error, Reason} on failure.","ref":"reckon_db_streams.html#append/4"},{"type":"function","title":"reckon_db_streams.append/5","doc":"","ref":"reckon_db_streams.html#append/5"},{"type":"function","title":"reckon_db_streams.delete/2","doc":"Delete a stream and all its events","ref":"reckon_db_streams.html#delete/2"},{"type":"type","title":"reckon_db_streams.direction/0","doc":"","ref":"reckon_db_streams.html#t:direction/0"},{"type":"type","title":"reckon_db_streams.event/0","doc":"","ref":"reckon_db_streams.html#t:event/0"},{"type":"function","title":"reckon_db_streams.exists/2","doc":"Check if a stream exists","ref":"reckon_db_streams.html#exists/2"},{"type":"function","title":"reckon_db_streams.get_version/2","doc":"Get current version of a stream Returns: -1 - if stream doesn't exist or is empty N >= 0 - representing the version of the latest event","ref":"reckon_db_streams.html#get_version/2"},{"type":"function","title":"reckon_db_streams.list_streams/1","doc":"List all streams in the store","ref":"reckon_db_streams.html#list_streams/1"},{"type":"type","title":"reckon_db_streams.new_event/0","doc":"","ref":"reckon_db_streams.html#t:new_event/0"},{"type":"function","title":"reckon_db_streams.read/5","doc":"Read events from a stream Parameters: StoreId - The store identifier StreamId - The stream identifier StartVersion - Starting version (0-based) Count - Maximum number of events to read Direction - forward or backward Returns {ok, [Event]} or {error, Reason}","ref":"reckon_db_streams.html#read/5"},{"type":"function","title":"reckon_db_streams.read_all/4","doc":"Read all events from a stream","ref":"reckon_db_streams.html#read_all/4"},{"type":"function","title":"reckon_db_streams.read_by_event_types/3","doc":"Read all events of specific types from all streams using Khepri native filtering. This function uses Khepri's built-in #if_data_matches condition to filter events by type at the database level, avoiding loading all events into memory. Parameters: StoreId - The store identifier EventTypes - List of event type binaries to match BatchSize - Maximum number of events to return (for pagination) Returns events sorted by epoch_us (global ordering).","ref":"reckon_db_streams.html#read_by_event_types/3"},{"type":"function","title":"reckon_db_streams.read_by_tags/4","doc":"Read all events matching tags from all streams. Tags provide a mechanism for cross-stream querying without affecting stream-based concurrency control. This is useful for the process-centric model where you want to find all events related to specific participants. Match Modes any  (default): Returns events containing ANY of the specified tags (union). Example:  read_by_tags(Store, [<<\"student:456\">>, <<\"student:789\">>], any, 100)  Returns events for either student. all : Returns events containing ALL of the specified tags (intersection). Example:  read_by_tags(Store, [<<\"student:456\">>, <<\"course:CS101\">>], all, 100)  Returns only events tagged with both student 456 AND course CS101. Parameters StoreId - The store identifier Tags - List of tag binaries to match Match -  any  |  all  (matching strategy) BatchSize - Maximum number of events to return Returns Events sorted by epoch_us (global ordering).","ref":"reckon_db_streams.html#read_by_tags/4"},{"type":"module","title":"reckon_db_streams_reader","doc":"Streams reader worker for reckon-db A gen_server that handles read operations for streams. Readers are temporary processes that terminate after a period of inactivity. Features: - Partitioned by stream_id for concurrent reads from different streams - Idle timeout to free up resources - Registration via pg groups","ref":"reckon_db_streams_reader.html"},{"type":"type","title":"reckon_db_streams_reader.event/0","doc":"","ref":"reckon_db_streams_reader.html#t:event/0"},{"type":"function","title":"reckon_db_streams_reader.get_reader/2","doc":"Get or create a reader for a stream","ref":"reckon_db_streams_reader.html#get_reader/2"},{"type":"function","title":"reckon_db_streams_reader.get_streams/1","doc":"Get all streams in the store","ref":"reckon_db_streams_reader.html#get_streams/1"},{"type":"function","title":"reckon_db_streams_reader.read/5","doc":"Read events from a stream via a reader worker","ref":"reckon_db_streams_reader.html#read/5"},{"type":"function","title":"reckon_db_streams_reader.start_link/1","doc":"Start a reader worker","ref":"reckon_db_streams_reader.html#start_link/1"},{"type":"module","title":"reckon_db_streams_sup","doc":"Streams supervisor for reckon-db Manages stream reader and writer pools for concurrent operations. Uses partitioned workers for high-throughput stream access.","ref":"reckon_db_streams_sup.html"},{"type":"function","title":"reckon_db_streams_sup.start_link/1","doc":"Start the streams supervisor","ref":"reckon_db_streams_sup.html#start_link/1"},{"type":"type","title":"reckon_db_streams_sup.store_config/0","doc":"","ref":"reckon_db_streams_sup.html#t:store_config/0"},{"type":"module","title":"reckon_db_streams_writer","doc":"Streams writer worker for reckon-db A gen_server that handles write operations for streams. Writers are temporary processes that terminate after a period of inactivity. Features: - Partitioned by stream_id for concurrent writes to different streams - Idle timeout to free up resources - Swarm-like registration via pg groups","ref":"reckon_db_streams_writer.html"},{"type":"function","title":"reckon_db_streams_writer.append/4","doc":"Append events to a stream via a writer worker","ref":"reckon_db_streams_writer.html#append/4"},{"type":"function","title":"reckon_db_streams_writer.get_writer/2","doc":"Get or create a writer for a stream","ref":"reckon_db_streams_writer.html#get_writer/2"},{"type":"function","title":"reckon_db_streams_writer.start_link/1","doc":"Start a writer worker","ref":"reckon_db_streams_writer.html#start_link/1"},{"type":"module","title":"reckon_db_subscriptions","doc":"Subscriptions API facade for reckon-db Provides the public API for subscription operations: - subscribe: Create a new subscription - unsubscribe: Remove a subscription - get: Get a subscription by key - list: List all subscriptions - exists: Check if a subscription exists Subscription types: - stream: Subscribe to all events in a specific stream - event_type: Subscribe to events of a specific type - event_pattern: Subscribe to events matching a pattern - event_payload: Subscribe to events with specific payload patterns","ref":"reckon_db_subscriptions.html"},{"type":"function","title":"reckon_db_subscriptions.ack/4","doc":"Acknowledge event delivery for a subscription Updates the checkpoint for the subscription to track progress. This is typically called after successfully processing an event. The checkpoint allows subscriptions to resume from where they left off after a restart. Parameters: StoreId - The store identifier SubscriptionName - Name of the subscription StreamId - ID of the stream the event came from (may be undefined for cross-stream) EventNumber - Version/position of the acknowledged event Returns ok on success, or {error, Reason} if the subscription is not found.","ref":"reckon_db_subscriptions.html#ack/4"},{"type":"type","title":"reckon_db_subscriptions.event/0","doc":"","ref":"reckon_db_subscriptions.html#t:event/0"},{"type":"function","title":"reckon_db_subscriptions.exists/2","doc":"Check if a subscription exists","ref":"reckon_db_subscriptions.html#exists/2"},{"type":"function","title":"reckon_db_subscriptions.get/2","doc":"Get a subscription by key","ref":"reckon_db_subscriptions.html#get/2"},{"type":"function","title":"reckon_db_subscriptions.list/1","doc":"List all subscriptions in the store","ref":"reckon_db_subscriptions.html#list/1"},{"type":"function","title":"reckon_db_subscriptions.setup_tracking/2","doc":"Setup subscription tracking for a process Joins the tracker group for subscriptions, allowing the process to receive notifications about subscription lifecycle events (created, updated, deleted). The process will receive messages in the format: - {feature_created, subscriptions, Data} - {feature_updated, subscriptions, Data} - {feature_deleted, subscriptions, Data}","ref":"reckon_db_subscriptions.html#setup_tracking/2"},{"type":"function","title":"reckon_db_subscriptions.subscribe/4","doc":"Create a subscription with default options Parameters: StoreId - The store identifier Type - Subscription type (stream, event_type, event_pattern, event_payload) Selector - The selector for matching events SubscriptionName - Human-readable name for the subscription Returns {ok, SubscriptionKey} on success or {error, Reason} on failure.","ref":"reckon_db_subscriptions.html#subscribe/4"},{"type":"function","title":"reckon_db_subscriptions.subscribe/5","doc":"Create a subscription with options Options: - pool_size: Number of emitter workers (default: 1) - start_from: Starting position for replay (default: 0) - subscriber: PID to receive events directly (default: undefined)","ref":"reckon_db_subscriptions.html#subscribe/5"},{"type":"type","title":"reckon_db_subscriptions.subscribe_opts/0","doc":"","ref":"reckon_db_subscriptions.html#t:subscribe_opts/0"},{"type":"type","title":"reckon_db_subscriptions.subscription/0","doc":"","ref":"reckon_db_subscriptions.html#t:subscription/0"},{"type":"type","title":"reckon_db_subscriptions.subscription_type/0","doc":"","ref":"reckon_db_subscriptions.html#t:subscription_type/0"},{"type":"function","title":"reckon_db_subscriptions.unsubscribe/2","doc":"Remove a subscription by key","ref":"reckon_db_subscriptions.html#unsubscribe/2"},{"type":"function","title":"reckon_db_subscriptions.unsubscribe/3","doc":"Remove a subscription by type, selector, and name","ref":"reckon_db_subscriptions.html#unsubscribe/3"},{"type":"module","title":"reckon_db_subscriptions_store","doc":"Subscriptions store for reckon-db Manages subscription persistence and retrieval directly via Khepri. This is a facade module that provides direct access to the subscription storage without going through a gen_server, since Khepri/Ra handles concurrency internally.","ref":"reckon_db_subscriptions_store.html"},{"type":"function","title":"reckon_db_subscriptions_store.delete/2","doc":"Delete a subscription by key","ref":"reckon_db_subscriptions_store.html#delete/2"},{"type":"function","title":"reckon_db_subscriptions_store.exists/2","doc":"Check if a subscription exists by key","ref":"reckon_db_subscriptions_store.html#exists/2"},{"type":"function","title":"reckon_db_subscriptions_store.exists/3","doc":"Check if a subscription exists by record","ref":"reckon_db_subscriptions_store.html#exists/3"},{"type":"function","title":"reckon_db_subscriptions_store.find_by_name/2","doc":"Find a subscription by name Searches all subscriptions for one matching the given name. Returns the subscription key and record if found.","ref":"reckon_db_subscriptions_store.html#find_by_name/2"},{"type":"function","title":"reckon_db_subscriptions_store.get/2","doc":"Get a subscription by key","ref":"reckon_db_subscriptions_store.html#get/2"},{"type":"function","title":"reckon_db_subscriptions_store.key/1","doc":"Generate a unique key for a subscription The key is a phash2 hash of {type, selector, subscription_name}","ref":"reckon_db_subscriptions_store.html#key/1"},{"type":"function","title":"reckon_db_subscriptions_store.key/3","doc":"Generate a unique key for a subscription from components","ref":"reckon_db_subscriptions_store.html#key/3"},{"type":"function","title":"reckon_db_subscriptions_store.list/1","doc":"List all subscriptions in the store","ref":"reckon_db_subscriptions_store.html#list/1"},{"type":"function","title":"reckon_db_subscriptions_store.put/2","doc":"Store a subscription","ref":"reckon_db_subscriptions_store.html#put/2"},{"type":"type","title":"reckon_db_subscriptions_store.store_id/0","doc":"","ref":"reckon_db_subscriptions_store.html#t:store_id/0"},{"type":"type","title":"reckon_db_subscriptions_store.subscription/0","doc":"","ref":"reckon_db_subscriptions_store.html#t:subscription/0"},{"type":"type","title":"reckon_db_subscriptions_store.subscription_type/0","doc":"","ref":"reckon_db_subscriptions_store.html#t:subscription_type/0"},{"type":"function","title":"reckon_db_subscriptions_store.update_checkpoint/3","doc":"Update the checkpoint for a subscription Parameters: StoreId - The store identifier Key - The subscription key Position - The new checkpoint position","ref":"reckon_db_subscriptions_store.html#update_checkpoint/3"},{"type":"module","title":"reckon_db_sup","doc":"Top-level supervisor for reckon-db This supervisor manages all store instances configured in the application environment. Each store gets its own system supervisor subtree.","ref":"reckon_db_sup.html"},{"type":"function","title":"reckon_db_sup.start_link/0","doc":"Start the top-level supervisor","ref":"reckon_db_sup.html#start_link/0"},{"type":"function","title":"reckon_db_sup.start_store/1","doc":"Start a store dynamically","ref":"reckon_db_sup.html#start_store/1"},{"type":"function","title":"reckon_db_sup.stop_store/1","doc":"Stop a store dynamically","ref":"reckon_db_sup.html#stop_store/1"},{"type":"type","title":"reckon_db_sup.store_config/0","doc":"","ref":"reckon_db_sup.html#t:store_config/0"},{"type":"function","title":"reckon_db_sup.which_stores/0","doc":"Get list of running stores","ref":"reckon_db_sup.html#which_stores/0"},{"type":"module","title":"reckon_db_system_sup","doc":"Per-store system supervisor for reckon-db This supervisor manages all subsystems for a single store instance. Uses rest_for_one strategy to ensure proper startup order: 1. CoreSystem (one_for_all) - persistence, notification, store management 2. ClusterSystem (cluster mode only) - discovery, coordination, monitoring 3. GatewaySystem - external interface workers","ref":"reckon_db_system_sup.html"},{"type":"function","title":"reckon_db_system_sup.start_link/1","doc":"Start the system supervisor for a store","ref":"reckon_db_system_sup.html#start_link/1"},{"type":"type","title":"reckon_db_system_sup.store_config/0","doc":"","ref":"reckon_db_system_sup.html#t:store_config/0"},{"type":"module","title":"reckon_db_telemetry","doc":"Telemetry handler for reckon-db Provides logging handler for telemetry events and utilities for attaching/detaching handlers. Additional handlers (e.g., OpenTelemetry) can be added for datacenter deployments. Usage Attach the default logger handler: ok = reckon_db_telemetry:attach_default_handler(). Attach a custom handler: ok = reckon_db_telemetry:attach(my_handler, fun my_module:handle/4, #{}). Emit an event: reckon_db_telemetry:emit(?STREAM_WRITE_STOP, #{duration => 1000}, #{store_id => my_store}).","ref":"reckon_db_telemetry.html"},{"type":"function","title":"reckon_db_telemetry.attach/3","doc":"Attach a custom handler for all reckon_db events","ref":"reckon_db_telemetry.html#attach/3"},{"type":"function","title":"reckon_db_telemetry.attach_default_handler/0","doc":"Attach the default logger handler for all reckon_db events","ref":"reckon_db_telemetry.html#attach_default_handler/0"},{"type":"function","title":"reckon_db_telemetry.detach/1","doc":"Detach a handler by ID","ref":"reckon_db_telemetry.html#detach/1"},{"type":"function","title":"reckon_db_telemetry.detach_default_handler/0","doc":"Detach the default logger handler","ref":"reckon_db_telemetry.html#detach_default_handler/0"},{"type":"function","title":"reckon_db_telemetry.emit/3","doc":"Emit a telemetry event","ref":"reckon_db_telemetry.html#emit/3"},{"type":"function","title":"reckon_db_telemetry.handle_event/4","doc":"Handle telemetry events (logger handler)","ref":"reckon_db_telemetry.html#handle_event/4"},{"type":"function","title":"reckon_db_telemetry.span/3","doc":"Execute a function with start/stop telemetry Emits start event before, stop event after (with duration)","ref":"reckon_db_telemetry.html#span/3"},{"type":"module","title":"reckon_db_temporal","doc":"Temporal queries for reckon-db. Provides point-in-time and time-range queries for event streams. These queries filter events by their epoch_us timestamp field. Use cases: - Reconstruct aggregate state at a historical point in time - Audit queries (\"what was the state on date X?\") - Time-range analytics","ref":"reckon_db_temporal.html"},{"type":"type","title":"reckon_db_temporal.event/0","doc":"","ref":"reckon_db_temporal.html#t:event/0"},{"type":"type","title":"reckon_db_temporal.opts/0","doc":"","ref":"reckon_db_temporal.html#t:opts/0"},{"type":"function","title":"reckon_db_temporal.read_range/4","doc":"Read events within a time range [FromTimestamp, ToTimestamp]. Returns events where FromTimestamp is less than or equal to epoch_us, and epoch_us is less than or equal to ToTimestamp. Example: <span class=\"w\">  </span><span class=\"c1\">%% Get all events from the first week of 2025</span><span class=\"w\">\n  </span><span class=\"n\">From</span><span class=\"w\"> </span><span class=\"o\">=</span><span class=\"w\"> </span><span class=\"mi\">1735689600000000</span><span class=\"p\">,</span><span class=\"w\">  </span><span class=\"c1\">%% Jan 1, 2025</span><span class=\"w\">\n  </span><span class=\"n\">To</span><span class=\"w\">   </span><span class=\"o\">=</span><span class=\"w\"> </span><span class=\"mi\">1736294400000000</span><span class=\"p\">,</span><span class=\"w\">  </span><span class=\"c1\">%% Jan 8, 2025</span><span class=\"w\">\n  </span><span class=\"p\" data-group-id=\"6420400096-1\">{</span><span class=\"ss\">ok</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"n\">Events</span><span class=\"p\" data-group-id=\"6420400096-1\">}</span><span class=\"w\"> </span><span class=\"o\">=</span><span class=\"w\"> </span><span class=\"nc\">reckon_db_temporal</span><span class=\"p\">:</span><span class=\"nf\">read_range</span><span class=\"p\" data-group-id=\"6420400096-2\">(</span><span class=\"ss\">my_store</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"p\" data-group-id=\"6420400096-3\">&lt;&lt;</span><span class=\"s\">&quot;orders-123&quot;</span><span class=\"p\" data-group-id=\"6420400096-3\">&gt;&gt;</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"n\">From</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"n\">To</span><span class=\"p\" data-group-id=\"6420400096-2\">)</span><span class=\"p\">.</span>","ref":"reckon_db_temporal.html#read_range/4"},{"type":"function","title":"reckon_db_temporal.read_range/5","doc":"Read events within a time range with options.","ref":"reckon_db_temporal.html#read_range/5"},{"type":"function","title":"reckon_db_temporal.read_until/3","doc":"Read all events from a stream up to (and including) a timestamp. Returns events where epoch_us is less than or equal to Timestamp, sorted by version (ascending). This is useful for reconstructing aggregate state at a point in time. Example: <span class=\"w\">  </span><span class=\"c1\">%% Get all events up to January 1, 2025 00:00:00 UTC</span><span class=\"w\">\n  </span><span class=\"n\">Timestamp</span><span class=\"w\"> </span><span class=\"o\">=</span><span class=\"w\"> </span><span class=\"mi\">1735689600000000</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"c1\">%% microseconds</span><span class=\"w\">\n  </span><span class=\"p\" data-group-id=\"5422321175-1\">{</span><span class=\"ss\">ok</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"n\">Events</span><span class=\"p\" data-group-id=\"5422321175-1\">}</span><span class=\"w\"> </span><span class=\"o\">=</span><span class=\"w\"> </span><span class=\"nc\">reckon_db_temporal</span><span class=\"p\">:</span><span class=\"nf\">read_until</span><span class=\"p\" data-group-id=\"5422321175-2\">(</span><span class=\"ss\">my_store</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"p\" data-group-id=\"5422321175-3\">&lt;&lt;</span><span class=\"s\">&quot;orders-123&quot;</span><span class=\"p\" data-group-id=\"5422321175-3\">&gt;&gt;</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"n\">Timestamp</span><span class=\"p\" data-group-id=\"5422321175-2\">)</span><span class=\"p\">.</span>","ref":"reckon_db_temporal.html#read_until/3"},{"type":"function","title":"reckon_db_temporal.read_until/4","doc":"Read events up to a timestamp with options. Options: - direction: forward (default) or backward - limit: Maximum number of events to return","ref":"reckon_db_temporal.html#read_until/4"},{"type":"type","title":"reckon_db_temporal.timestamp/0","doc":"Microseconds since epoch (epoch_us format)","ref":"reckon_db_temporal.html#t:timestamp/0"},{"type":"function","title":"reckon_db_temporal.version_at/3","doc":"Get the stream version at a specific timestamp. Returns the version of the last event with epoch_us less than or equal to Timestamp. This is useful for determining what version to replay up to. Returns: - {ok, Version} if events exist before the timestamp - {ok, -1} if no events exist before the timestamp - {error, Reason} on failure","ref":"reckon_db_temporal.html#version_at/3"},{"type":"module","title":"reckon_db_tracker_group","doc":"Tracker group management for reckon-db Uses pg (process groups) for managing tracker processes. Trackers receive notifications about subscription lifecycle events. This module provides: - Process group management for tracker processes - Notification broadcasting for created/deleted/updated events Features that can be tracked: - subscriptions: Subscription lifecycle - streams: Stream lifecycle - snapshots: Snapshot lifecycle","ref":"reckon_db_tracker_group.html"},{"type":"type","title":"reckon_db_tracker_group.feature/0","doc":"","ref":"reckon_db_tracker_group.html#t:feature/0"},{"type":"function","title":"reckon_db_tracker_group.group_key/2","doc":"Generate the group key for a feature's trackers","ref":"reckon_db_tracker_group.html#group_key/2"},{"type":"function","title":"reckon_db_tracker_group.join/3","doc":"Join one or more processes to the tracker group for a feature","ref":"reckon_db_tracker_group.html#join/3"},{"type":"function","title":"reckon_db_tracker_group.leave/3","doc":"Remove one or more processes from the tracker group","ref":"reckon_db_tracker_group.html#leave/3"},{"type":"function","title":"reckon_db_tracker_group.members/2","doc":"Get all member processes tracking a feature","ref":"reckon_db_tracker_group.html#members/2"},{"type":"function","title":"reckon_db_tracker_group.notify_created/3","doc":"Notify all trackers that a feature instance was created","ref":"reckon_db_tracker_group.html#notify_created/3"},{"type":"function","title":"reckon_db_tracker_group.notify_deleted/3","doc":"Notify all trackers that a feature instance was deleted","ref":"reckon_db_tracker_group.html#notify_deleted/3"},{"type":"function","title":"reckon_db_tracker_group.notify_updated/3","doc":"Notify all trackers that a feature instance was updated","ref":"reckon_db_tracker_group.html#notify_updated/3"},{"type":"type","title":"reckon_db_tracker_group.store_id/0","doc":"","ref":"reckon_db_tracker_group.html#t:store_id/0"},{"type":"extras","title":"Readme","doc":"# reckon-db\n[![Buy Me A Coffee](https://img.shields.io/badge/Buy%20Me%20A%20Coffee-support-yellow.svg)](https://buymeacoffee.com/beamologist)\n\nBEAM-native Event Store built on Khepri/Ra with Raft consensus.\n\n![Architecture](assets/architecture.svg)","ref":"readme.html"},{"type":"extras","title":"Overview - Readme","doc":"reckon-db is an Erlang implementation of a distributed event store designed for:\n- **Event Sourcing**: Store and replay events with optimistic concurrency\n- **Clustering**: Automatic node discovery and Raft-based replication\n- **High Throughput**: Partitioned writers for concurrent stream writes\n- **Edge & Datacenter**: Works on Nerves devices and Kubernetes clusters","ref":"readme.html#overview"},{"type":"extras","title":"Features - Readme","doc":"- Event stream operations (append, read, subscribe) with versioning and optimistic concurrency\n- Persistent subscriptions (stream, event type, pattern, payload matching)\n- Snapshot management for aggregate state\n- Emitter pools for high-throughput event delivery\n- UDP multicast and Kubernetes DNS discovery\n- BEAM telemetry with optional OpenTelemetry exporters","ref":"readme.html#features"},{"type":"extras","title":"Installation - Readme","doc":"Add to your `rebar.config`:\n\n```erlang\n{deps, [\n    {reckon_db, \"1.0.0\"}\n]}.\n```\n\nPure Erlang implementation - works everywhere, no native dependencies.","ref":"readme.html#installation"},{"type":"extras","title":"Quick Start - Readme","doc":"```erlang\n%% Start the application\napplication:ensure_all_started(reckon_db).\n\n%% Append events to a stream\nEvents = [\n    #{\n        event_type => <<\"user_created\">>,\n        data => #{name => <<\"Alice\">>, email => <<\"alice@example.com\">>},\n        metadata => #{correlation_id => <<\"req-123\">>}\n    }\n],\n{ok, Version} = reckon_db_streams:append(my_store, <<\"user-123\">>, -1, Events).\n\n%% Read events from a stream\n{ok, ReadEvents} = reckon_db_streams:read(my_store, <<\"user-123\">>, 0, 100, forward).\n\n%% Subscribe to events\n{ok, SubKey} = reckon_db_subscriptions:subscribe(\n    my_store,\n    stream,                    %% Type: stream | event_type | event_pattern | event_payload\n    <<\"user-123\">>,            %% Selector\n    <<\"user_projection\">>      %% Subscription name\n).\n\n%% Receive events\nreceive\n    {event, Event} -> io:format(\"Received: ~p~n\", [Event])\nend.\n```","ref":"readme.html#quick-start"},{"type":"extras","title":"API Reference - Readme","doc":"","ref":"readme.html#api-reference"},{"type":"extras","title":"Streams - Readme","doc":"```erlang\n%% Append events (returns new version)\nreckon_db_streams:append(StoreId, StreamId, ExpectedVersion, Events) ->\n    {ok, NewVersion} | {error, version_mismatch | term()}.\n\n%% Read events from a stream\nreckon_db_streams:read(StoreId, StreamId, FromVersion, Count, Direction) ->\n    {ok, [Event]} | {error, stream_not_found | term()}.\n\n%% Read across all streams\nreckon_db_streams:read_all(StoreId, FromVersion, Count, Direction) ->\n    {ok, [Event]} | {error, term()}.\n\n%% Read events by type\nreckon_db_streams:read_by_event_types(StoreId, EventTypes, Opts) ->\n    {ok, [Event]} | {error, term()}.\n\n%% Get stream version\nreckon_db_streams:get_version(StoreId, StreamId) -> {ok, Version} | {error, term()}.\n\n%% Check if stream exists\nreckon_db_streams:exists(StoreId, StreamId) -> boolean().\n\n%% List all streams\nreckon_db_streams:list_streams(StoreId) -> {ok, [StreamId]} | {error, term()}.\n\n%% Delete stream (soft delete)\nreckon_db_streams:delete(StoreId, StreamId) -> ok | {error, term()}.\n```","ref":"readme.html#streams"},{"type":"extras","title":"Subscriptions - Readme","doc":"```erlang\n%% Create subscription\nreckon_db_subscriptions:subscribe(StoreId, Type, Selector, Name) ->\n    {ok, SubscriptionKey} | {error, term()}.\nreckon_db_subscriptions:subscribe(StoreId, Type, Selector, Name, Opts) ->\n    {ok, SubscriptionKey} | {error, term()}.\n\n%% Remove subscription (by key or by type+name)\nreckon_db_subscriptions:unsubscribe(StoreId, SubscriptionKey) -> ok | {error, term()}.\nreckon_db_subscriptions:unsubscribe(StoreId, Type, SubscriptionName) -> ok | {error, term()}.\n\n%% Get subscription by key\nreckon_db_subscriptions:get(StoreId, SubscriptionKey) ->\n    {ok, Subscription} | {error, not_found}.\n\n%% Acknowledge event processing\nreckon_db_subscriptions:ack(StoreId, StreamId, SubscriptionName, EventNumber) -> ok.\n\n%% List subscriptions\nreckon_db_subscriptions:list(StoreId) -> {ok, [Subscription]}.\n\n%% Check if subscription exists\nreckon_db_subscriptions:exists(StoreId, SubscriptionKey) -> boolean().\n\n%% Subscription types:\n%%   stream - Events from a specific stream\n%%   event_type - Events matching event type\n%%   event_pattern - Events matching stream pattern (wildcards)\n%%   event_payload - Events matching payload criteria\n```","ref":"readme.html#subscriptions"},{"type":"extras","title":"Snapshots - Readme","doc":"```erlang\n%% Save snapshot\nreckon_db_snapshots:save(StoreId, StreamId, Version, Data) -> ok.\nreckon_db_snapshots:save(StoreId, StreamId, Version, Data, Metadata) -> ok.\n\n%% Load latest snapshot\nreckon_db_snapshots:load(StoreId, StreamId) -> {ok, Snapshot} | {error, not_found}.\n\n%% Load snapshot at specific version\nreckon_db_snapshots:load_at(StoreId, StreamId, Version) -> {ok, Snapshot} | {error, not_found}.\n\n%% List all snapshots for stream\nreckon_db_snapshots:list(StoreId, StreamId) -> {ok, [Snapshot]}.\n\n%% Delete all snapshots for stream\nreckon_db_snapshots:delete(StoreId, StreamId) -> ok.\n\n%% Delete snapshot at specific version\nreckon_db_snapshots:delete_at(StoreId, StreamId, Version) -> ok.\n\n%% Check if snapshot exists\nreckon_db_snapshots:exists(StoreId, StreamId) -> boolean().\nreckon_db_snapshots:exists_at(StoreId, StreamId, Version) -> boolean().\n```","ref":"readme.html#snapshots"},{"type":"extras","title":"Aggregation - Readme","doc":"```erlang\n%% Fold events left to right (chronological order)\n%% Returns a tagged_map with {sum, N} and {overwrite, V} tags preserved\nreckon_db_aggregator:foldl(Events) -> tagged_map().\nreckon_db_aggregator:foldl(Events, InitialState) -> tagged_map().\n\n%% Fold events right to left (reverse order)\nreckon_db_aggregator:foldr(Events) -> tagged_map().\nreckon_db_aggregator:foldr(Events, InitialState) -> tagged_map().\n\n%% Finalize a tagged map (unwrap {sum, N} -> N, {overwrite, V} -> V)\nreckon_db_aggregator:finalize(TaggedMap) -> map().\n\n%% Aggregate events with optional snapshot (convenience function)\nreckon_db_aggregator:aggregate(Events, Snapshot | undefined, Opts) -> map().\n%% Opts: #{initial_state => map(), finalize => boolean()}\n```\n\nExample usage:\n\n```erlang\n%% Load events and aggregate\n{ok, Events} = reckon_db_streams:read(my_store, <<\"account-123\">>, 0, 10000, forward),\nTaggedState = reckon_db_aggregator:foldl(Events, #{balance => {sum, 0}}),\nFinalState = reckon_db_aggregator:finalize(TaggedState).\n\n%% Or use aggregate/3 with snapshot support\n{ok, Snapshot} = reckon_db_snapshots:load(my_store, <<\"account-123\">>),\n{ok, NewEvents} = reckon_db_streams:read(my_store, <<\"account-123\">>, Snapshot#snapshot.version + 1, 10000, forward),\nState = reckon_db_aggregator:aggregate(NewEvents, Snapshot, #{}).\n```","ref":"readme.html#aggregation"},{"type":"extras","title":"Telemetry - Readme","doc":"```erlang\n%% Attach default logger handler\nreckon_db_telemetry:attach_default_handler() -> ok.\n\n%% Attach custom handler\nreckon_db_telemetry:attach(HandlerId, HandlerFun, Config) -> ok.\n\n%% Detach handler\nreckon_db_telemetry:detach(HandlerId) -> ok.\n```","ref":"readme.html#telemetry"},{"type":"extras","title":"Configuration - Readme","doc":"```erlang\n%% sys.config\n[{reckon_db, [\n    {stores, [\n        {my_store, [\n            {data_dir, \"/var/lib/reckon_db/my_store\"},\n            {mode, cluster},  %% single | cluster\n            {timeout, 5000}\n        ]}\n    ]},\n    {telemetry_handlers, [logger]},\n    {writer_pool_size, 10},\n    {reader_pool_size, 10},\n\n    %% Cluster discovery (cluster mode only)\n    {discovery, [\n        {method, multicast},  %% multicast | k8s_dns\n        {port, 45892},\n        {multicast_addr, {239, 255, 0, 1}},\n        {secret, <<\"cluster_secret\">>}\n    ]}\n]}].\n```","ref":"readme.html#configuration"},{"type":"extras","title":"Architecture - Readme","doc":"","ref":"readme.html#architecture"},{"type":"extras","title":"Supervision Tree - Readme","doc":"![Supervision Tree](assets/supervision_tree.svg)","ref":"readme.html#supervision-tree"},{"type":"extras","title":"Event Flow - Readme","doc":"![Event Flow](assets/event_flow.svg)","ref":"readme.html#event-flow"},{"type":"extras","title":"Telemetry Events - Readme","doc":"| Event | Measurements | Metadata |\n|-------|--------------|----------|\n| `[reckon_db, stream, write, start]` | system_time | store_id, stream_id, event_count |\n| `[reckon_db, stream, write, stop]` | duration, event_count | store_id, stream_id, new_version |\n| `[reckon_db, stream, write, error]` | duration | store_id, stream_id, reason |\n| `[reckon_db, stream, read, start]` | system_time | store_id, stream_id |\n| `[reckon_db, stream, read, stop]` | duration, event_count | store_id, stream_id |\n| `[reckon_db, subscription, created]` | system_time | store_id, subscription_id, type |\n| `[reckon_db, subscription, deleted]` | system_time | store_id, subscription_id |\n| `[reckon_db, snapshot, created]` | duration, size_bytes | store_id, stream_id, version |\n| `[reckon_db, cluster, node, up]` | system_time | store_id, node, member_count |\n| `[reckon_db, cluster, node, down]` | system_time | store_id, node, reason |\n| `[reckon_db, cluster, leader, elected]` | system_time | store_id, leader |","ref":"readme.html#telemetry-events"},{"type":"extras","title":"Building - Readme","doc":"```bash\nrebar3 compile         # Compile\nrebar3 eunit           # Unit tests\nrebar3 ct              # Integration tests\nrebar3 dialyzer        # Type checking\nrebar3 cover           # Coverage report\n```","ref":"readme.html#building"},{"type":"extras","title":"Testing - Readme","doc":"Test counts:\n- **Unit tests**: 446 tests (including NIF modules with enterprise/community equivalence tests)\n- **Integration tests**: 53 tests (streams, subscriptions, snapshots, cluster)\n- **End-to-end tests**: 24 tests (full gater integration)\n\n```bash\nrebar3 eunit --dir=test/unit              # All unit tests\nrebar3 ct --dir=test/integration          # Integration tests\nrebar3 ct --dir=test/e2e                  # E2E tests with gater\nrebar3 ct --suite=reckon_db_streams_SUITE  # Streams tests\nrebar3 ct --suite=reckon_db_cluster_SUITE  # Cluster tests\n```","ref":"readme.html#testing"},{"type":"extras","title":"Gateway API - Readme","doc":"reckon-db is accessed through [reckon-gater](https://github.com/reckon-db-org/reckon-gater), which provides the unified API for load-balanced, distributed access to event stores.","ref":"readme.html#gateway-api"},{"type":"extras","title":"How It Works - Readme","doc":"1. **reckon-db** starts and creates a gateway worker for each store\n2. Gateway workers register with the **reckon-gater** pg-based registry\n3. Clients use the gater API for all event store operations\n4. The gater routes requests to registered workers using round-robin with failover","ref":"readme.html#how-it-works"},{"type":"extras","title":"Architecture - Readme","doc":"![Gateway Architecture](assets/gateway_architecture.svg)","ref":"readme.html#architecture-1"},{"type":"extras","title":"Using the Gateway API - Readme","doc":"All event store operations go through the gater API:\n\n```erlang\n%% Stream operations\n{ok, Version} = esdb_gater_api:append_events(my_store, StreamId, Events).\n{ok, Events} = esdb_gater_api:stream_forward(my_store, StreamId, 0, 100).\n{ok, Version} = esdb_gater_api:get_version(my_store, StreamId).\n\n%% Subscription operations\nok = esdb_gater_api:save_subscription(my_store, stream, StreamId, Name, 0, self()).\n\n%% Snapshot operations\nok = esdb_gater_api:record_snapshot(my_store, SourceUuid, StreamUuid, Version, Record).\n{ok, Snap} = esdb_gater_api:read_snapshot(my_store, SourceUuid, StreamUuid, Version).\n```\n\nSee [reckon-gater](https://hex.pm/packages/reckon_gater) for complete API documentation.","ref":"readme.html#using-the-gateway-api"},{"type":"extras","title":"Related Projects - Readme","doc":"- [reckon-gater](https://github.com/reckon-db-org/reckon-gater) - Gateway for distributed access\n- [ex-esdb](https://github.com/beam-campus/ex-esdb) - Original Elixir implementation","ref":"readme.html#related-projects"},{"type":"extras","title":"License - Readme","doc":"Apache-2.0","ref":"readme.html#license"},{"type":"extras","title":"License","doc":"Apache License\n                           Version 2.0, January 2004\n                        http://www.apache.org/licenses/\n\n   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION\n\n   1. Definitions.\n\n      \"License\" shall mean the terms and conditions for use, reproduction,\n      and distribution as defined by Sections 1 through 9 of this document.\n\n      \"Licensor\" shall mean the copyright owner or entity authorized by\n      the copyright owner that is granting the License.\n\n      \"Legal Entity\" shall mean the union of the acting entity and all\n      other entities that control, are controlled by, or are under common\n      control with that entity. For the purposes of this definition,\n      \"control\" means (i) the power, direct or indirect, to cause the\n      direction or management of such entity, whether by contract or\n      otherwise, or (ii) ownership of fifty percent (50%) or more of the\n      outstanding shares, or (iii) beneficial ownership of such entity.\n\n      \"You\" (or \"Your\") shall mean an individual or Legal Entity\n      exercising permissions granted by this License.\n\n      \"Source\" form shall mean the preferred form for making modifications,\n      including but not limited to software source code, documentation\n      source, and configuration files.\n\n      \"Object\" form shall mean any form resulting from mechanical\n      transformation or translation of a Source form, including but\n      not limited to compiled object code, generated documentation,\n      and conversions to other media types.\n\n      \"Work\" shall mean the work of authorship, whether in Source or\n      Object form, made available under the License, as indicated by a\n      copyright notice that is included in or attached to the work\n      (an example is provided in the Appendix below).\n\n      \"Derivative Works\" shall mean any work, whether in Source or Object\n      form, that is based on (or derived from) the Work and for which the\n      editorial revisions, annotations, elaborations, or other modifications\n      represent, as a whole, an original work of authorship. For the purposes\n      of this License, Derivative Works shall not include works that remain\n      separable from, or merely link (or bind by name) to the interfaces of,\n      the Work and Derivative Works thereof.\n\n      \"Contribution\" shall mean any work of authorship, including\n      the original version of the Work and any modifications or additions\n      to that Work or Derivative Works thereof, that is intentionally\n      submitted to Licensor for inclusion in the Work by the copyright owner\n      or by an individual or Legal Entity authorized to submit on behalf of\n      the copyright owner. For the purposes of this definition, \"submitted\"\n      means any form of electronic, verbal, or written communication sent\n      to the Licensor or its representatives, including but not limited to\n      communication on electronic mailing lists, source code control systems,\n      and issue tracking systems that are managed by, or on behalf of, the\n      Licensor for the purpose of discussing and improving the Work, but\n      excluding communication that is conspicuously marked or otherwise\n      designated in writing by the copyright owner as \"Not a Contribution.\"\n\n      \"Contributor\" shall mean Licensor and any individual or Legal Entity\n      on behalf of whom a Contribution has been received by Licensor and\n      subsequently incorporated within the Work.\n\n   2. Grant of Copyright License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      copyright license to reproduce, prepare Derivative Works of,\n      publicly display, publicly perform, sublicense, and distribute the\n      Work and such Derivative Works in Source or Object form.\n\n   3. Grant of Patent License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      (except as stated in this section) patent license to make, have made,\n      use, offer to sell, sell, import, and otherwise transfer the Work,\n      where such license applies only to those patent claims licensable\n      by such Contributor that are necessarily infringed by their\n      Contribution(s) alone or by combination of their Contribution(s)\n      with the Work to which such Contribution(s) was submitted. If You\n      institute patent litigation against any entity (including a\n      cross-claim or counterclaim in a lawsuit) alleging that the Work\n      or a Contribution incorporated within the Work constitutes direct\n      or contributory patent infringement, then any patent licenses\n      granted to You under this License for that Work shall terminate\n      as of the date such litigation is filed.\n\n   4. Redistribution. You may reproduce and distribute copies of the\n      Work or Derivative Works thereof in any medium, with or without\n      modifications, and in Source or Object form, provided that You\n      meet the following conditions:\n\n      (a) You must give any other recipients of the Work or\n          Derivative Works a copy of this License; and\n\n      (b) You must cause any modified files to carry prominent notices\n          stating that You changed the files; and\n\n      (c) You must retain, in the Source form of any Derivative Works\n          that You distribute, all copyright, patent, trademark, and\n          attribution notices from the Source form of the Work,\n          excluding those notices that do not pertain to any part of\n          the Derivative Works; and\n\n      (d) If the Work includes a \"NOTICE\" text file as part of its\n          distribution, then any Derivative Works that You distribute must\n          include a readable copy of the attribution notices contained\n          within such NOTICE file, excluding those notices that do not\n          pertain to any part of the Derivative Works, in at least one\n          of the following places: within a NOTICE text file distributed\n          as part of the Derivative Works; within the Source form or\n          documentation, if provided along with the Derivative Works; or,\n          within a display generated by the Derivative Works, if and\n          wherever such third-party notices normally appear. The contents\n          of the NOTICE file are for informational purposes only and\n          do not modify the License. You may add Your own attribution\n          notices within Derivative Works that You distribute, alongside\n          or as an addendum to the NOTICE text from the Work, provided\n          that such additional attribution notices cannot be construed\n          as modifying the License.\n\n      You may add Your own copyright statement to Your modifications and\n      may provide additional or different license terms and conditions\n      for use, reproduction, or distribution of Your modifications, or\n      for any such Derivative Works as a whole, provided Your use,\n      reproduction, and distribution of the Work otherwise complies with\n      the conditions stated in this License.\n\n   5. Submission of Contributions. Unless You explicitly state otherwise,\n      any Contribution intentionally submitted for inclusion in the Work\n      by You to the Licensor shall be under the terms and conditions of\n      this License, without any additional terms or conditions.\n      Notwithstanding the above, nothing herein shall supersede or modify\n      the terms of any separate license agreement you may have executed\n      with Licensor regarding such Contributions.\n\n   6. Trademarks. This License does not grant permission to use the trade\n      names, trademarks, service marks, or product names of the Licensor,\n      except as required for reasonable and customary use in describing the\n      origin of the Work and reproducing the content of the NOTICE file.\n\n   7. Disclaimer of Warranty. Unless required by applicable law or\n      agreed to in writing, Licensor provides the Work (and each\n      Contributor provides its Contributions) on an \"AS IS\" BASIS,\n      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or\n      implied, including, without limitation, any warranties or conditions\n      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A\n      PARTICULAR PURPOSE. You are solely responsible for determining the\n      appropriateness of using or redistributing the Work and assume any\n      risks associated with Your exercise of permissions under this License.\n\n   8. Limitation of Liability. In no event and under no legal theory,\n      whether in tort (including negligence), contract, or otherwise,\n      unless required by applicable law (such as deliberate and grossly\n      negligent acts) or agreed to in writing, shall any Contributor be\n      liable to You for damages, including any direct, indirect, special,\n      incidental, or consequential damages of any character arising as a\n      result of this License or out of the use or inability to use the\n      Work (including but not limited to damages for loss of goodwill,\n      work stoppage, computer failure or malfunction, or any and all\n      other commercial damages or losses), even if such Contributor\n      has been advised of the possibility of such damages.\n\n   9. Accepting Warranty or Additional Liability. While redistributing\n      the Work or Derivative Works thereof, You may choose to offer,\n      and charge a fee for, acceptance of support, warranty, indemnity,\n      or other liability obligations and/or rights consistent with this\n      License. However, in accepting such obligations, You may act only\n      on Your own behalf and on Your sole responsibility, not on behalf\n      of any other Contributor, and only if You agree to indemnify,\n      defend, and hold each Contributor harmless for any liability\n      incurred by, or claims asserted against, such Contributor by reason\n      of your accepting any such warranty or additional liability.\n\n   END OF TERMS AND CONDITIONS\n\n   APPENDIX: How to apply the Apache License to your work.\n\n      To apply the Apache License to your work, attach the following\n      boilerplate notice, with the fields enclosed by brackets \"[]\"\n      replaced with your own identifying information. (Don't include\n      the brackets!)  The text should be enclosed in the appropriate\n      comment syntax for the file format. We also recommend that a\n      file or class name and description of purpose be included on the\n      same \"printed page\" as the copyright notice for easier\n      identification within third-party archives.\n\n   Copyright [yyyy] [name of copyright owner]\n\n   Licensed under the Apache License, Version 2.0 (the \"License\");\n   you may not use this file except in compliance with the License.\n   You may obtain a copy of the License at\n\n       http://www.apache.org/licenses/LICENSE-2.0\n\n   Unless required by applicable law or agreed to in writing, software\n   distributed under the License is distributed on an \"AS IS\" BASIS,\n   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n   See the License for the specific language governing permissions and\n   limitations under the License.","ref":"license.html"},{"type":"extras","title":"Changelog","doc":"# Changelog\n\nAll notable changes to reckon-db will be documented in this file.\n\nThe format is based on [Keep a Changelog](https://keepachangelog.com/en/1.0.0/),\nand this project adheres to [Semantic Versioning](https://semver.org/spec/v2.0.0.html).\n\n## [Unreleased]\n\n## [1.1.0] - 2026-01-21","ref":"changelog.html"},{"type":"extras","title":"Added - Changelog","doc":"- **Tag-Based Querying**: Cross-stream event queries using tags\n  - `read_by_tags/4` - Query events by tags across all streams\n  - Support for `any` (union) and `all` (intersection) matching modes\n  - Tags field added to event records and storage\n  - 15 new unit tests for tag filtering\n  - Tags are for QUERY purposes only, NOT for concurrency control","ref":"changelog.html#added"},{"type":"extras","title":"Changed - Changelog","doc":"- **Dependencies**: Updated reckon_gater from `~> 1.0.3` to `~> 1.1.0` for tags support\n\n## [1.0.3] - 2026-01-19","ref":"changelog.html#changed"},{"type":"extras","title":"Changed - Changelog","doc":"- **Dependencies**: Updated reckon_gater from exact `1.0.0` to `~> 1.0.3` to include\n  critical double-wrapping bugfix\n\n## [1.0.2] - 2026-01-09","ref":"changelog.html#changed-1"},{"type":"extras","title":"Fixed - Changelog","doc":"- **Documentation**: Minor documentation improvements\n\n## [1.0.0] - 2026-01-03","ref":"changelog.html#fixed"},{"type":"extras","title":"Changed - Changelog","doc":"- **Stable Release**: First stable release of reckon-db under reckon-db-org\n- All APIs considered stable and ready for production use\n- Updated Dockerfile with correct package names (reckon_db)\n- Fixed guide asset paths for hexdocs compatibility\n\n## [0.4.6] - 2025-12-26","ref":"changelog.html#changed-2"},{"type":"extras","title":"Fixed - Changelog","doc":"- **Dependency conflict**: Removed direct `ra` dependency (khepri provides it).\n  Updated to `reckon_db_gater ~> 0.6.5` which removed stale ra from its lock file.\n\n## [0.4.5] - 2025-12-26","ref":"changelog.html#fixed-1"},{"type":"extras","title":"Fixed - Changelog","doc":"- **Dependency conflict**: Updated `ra` dependency from exact `2.16.12` to `~> 2.17.1`\n  to resolve conflict with `reckon_db_gater ~> 0.6.4` which requires `ra ~> 2.17.1`\n\n## [0.4.4] - 2025-12-22","ref":"changelog.html#fixed-2"},{"type":"extras","title":"Added - Changelog","doc":"- **Configuration Guide**: Comprehensive configuration documentation\n  - Store configuration options (data_dir, mode, pool sizes)\n  - Health probing configuration\n  - Consistency checking and persistence intervals\n  - Erlang (sys.config) and Elixir (config.exs) examples\n  - Complete development/staging/production examples\n  - Performance tuning recommendations\n  - Telemetry events reference\n\n## [0.4.3] - 2025-12-22","ref":"changelog.html#added-1"},{"type":"extras","title":"Added - Changelog","doc":"- **Gateway Worker Handlers**:\n  - `delete_stream` - Delete streams via gateway\n  - `read_by_event_types` - Native Khepri type filtering via gateway\n  - `get_subscription` - Get subscription details including checkpoint\n\nThese handlers support the erl-evoq-esdb adapter improvements.\n\n## [0.4.2] - 2025-12-22","ref":"changelog.html#added-2"},{"type":"extras","title":"Added - Changelog","doc":"- **Cluster Consistency Checker** (`reckon_db_consistency_checker.erl`):\n  - Split-brain detection via membership consensus verification\n  - Leader consensus verification across all cluster nodes\n  - Raft log consistency checks (term and commit index)\n  - Quorum status monitoring with margin calculation\n  - Four status levels: `healthy`, `degraded`, `split_brain`, `no_quorum`\n  - Configurable check intervals (default: 5000ms)\n  - Status change callbacks for alerting\n  - Telemetry events: `[reckon_db, consistency, ...]`\n\n- **Active Health Prober** (`reckon_db_health_prober.erl`):\n  - Fast failure detection via active probing (default: 2000ms intervals)\n  - Three probe types: `ping`, `rpc`, `khepri`\n  - Configurable failure threshold (default: 3 consecutive failures)\n  - Node status tracking: `healthy`, `suspect`, `failed`, `unknown`\n  - Recovery detection with callbacks\n  - Telemetry events: `[reckon_db, health, ...]`\n\n- **Cluster Consistency Guide** (`guides/cluster_consistency.md`):\n  - Split-brain problem explanation and prevention strategies\n  - Consistency checker usage and configuration\n  - Health prober integration patterns\n  - Quorum management and recovery procedures\n  - Circuit breaker and load balancer integration examples\n\n- **Architecture Diagrams** (SVG):\n  - `assets/consistency_checker.svg` - Consistency checker architecture\n  - `assets/split_brain_detection.svg` - Split-brain detection flow\n  - `assets/health_probing.svg` - Health probing timeline","ref":"changelog.html#added-3"},{"type":"extras","title":"Tests - Changelog","doc":"- 35 unit tests for consistency checker\n- 37 unit tests for health prober\n- All 72 new tests passing\n\n## [0.4.1] - 2025-12-22","ref":"changelog.html#tests"},{"type":"extras","title":"Added - Changelog","doc":"- **Server-Side Documentation Guides**:\n  - `guides/temporal_queries.md` - Point-in-time queries, timestamp filtering, cluster behavior\n  - `guides/scavenging.md` - Event lifecycle, archival backends, safety guarantees\n  - `guides/causation.md` - Causation/correlation tracking, graph building, DOT export\n  - `guides/stream_links.md` - Derived streams, filter/transform patterns\n  - `guides/schema_evolution.md` - Schema registry, version-based upcasting, validation\n  - `guides/memory_pressure.md` - Pressure levels, callbacks, integration patterns\n  - `guides/storage_internals.md` - Khepri paths, version padding, cluster replication\n\n- **Architecture Diagrams** (SVG):\n  - `assets/temporal_query_flow.svg` - Temporal query processing flow\n  - `assets/scavenge_lifecycle.svg` - Event lifecycle state machine\n  - `assets/causation_graph.svg` - Causation chain visualization\n  - `assets/stream_links.svg` - Stream linking architecture\n  - `assets/schema_upcasting.svg` - Schema version upcasting flow\n  - `assets/memory_levels.svg` - Memory pressure level thresholds\n  - `assets/khepri_paths.svg` - Khepri storage path structure","ref":"changelog.html#added-4"},{"type":"extras","title":"Changed - Changelog","doc":"- **Documentation Improvements**:\n  - Replaced ASCII diagrams with professional SVG graphics\n  - `snapshot_recovery.svg` - Performance comparison visualization\n  - `event_fanout.svg` - Multi-subscriber event delivery diagram\n  - Updated `rebar.config` ex_doc with new guides organized into Core Concepts, Advanced Features, and Operations sections\n\n## [0.4.0] - 2025-12-22","ref":"changelog.html#changed-3"},{"type":"extras","title":"Added - Changelog","doc":"- **Enterprise Edition NIFs**: High-performance Rust NIFs with pure Erlang fallbacks\n  - Community Edition (hex.pm) uses pure Erlang implementations\n  - Enterprise Edition (git + Rust) gets 5-100x speedups for specific operations\n  - Automatic fallback detection via `persistent_term`\n\n- **esdb_crypto_nif** (Phase 1):\n  - `nif_base58_encode/1` - Fast Base58 encoding for DIDs\n  - `nif_base58_decode/1` - Fast Base58 decoding\n  - Uses Bitcoin alphabet, ~5x faster than pure Erlang\n\n- **esdb_archive_nif** (Phase 2):\n  - `nif_compress/1,2` - Zstd compression with configurable level\n  - `nif_decompress/1` - Zstd decompression\n  - `nif_compress_batch/1,2` - Batch compression for multiple items\n  - `nif_decompress_batch/1` - Batch decompression\n  - ~10x faster than zlib, better compression ratios\n\n- **esdb_hash_nif** (Phase 3):\n  - `nif_xxhash64/1,2` - 64-bit xxHash with optional seed\n  - `nif_xxhash3/1` - Modern xxHash3 (SIMD optimized)\n  - `nif_partition_hash/2` - Hash to partition number\n  - `nif_stream_partition/3` - Combined store+stream routing\n  - `nif_partition_hash_batch/2` - Batch hashing for bulk ops\n  - `nif_fnv1a/1` - FNV-1a for small keys\n  - `nif_fast_phash/2` - Drop-in phash2 replacement\n\n- **esdb_aggregate_nif** (Phase 3):\n  - `nif_aggregate_events/2` - Bulk fold with tagged value semantics\n  - `nif_sum_field/2` - Vectorized sum accumulation for numeric fields\n  - `nif_count_where/3` - Count events matching field condition\n  - `nif_merge_tagged_batch/1` - Batch map merge with tagged values\n  - `nif_finalize/1` - Unwrap tagged values ({sum, N}, {overwrite, V})\n  - `nif_aggregation_stats/1` - Event statistics (counts, unique fields)\n\n- **esdb_filter_nif** (Phase 3):\n  - `nif_filter_events/2` - Filter events by compiled predicate\n  - `nif_filter_count/2` - Count matching events without collecting\n  - `nif_compile_predicate/1` - Pre-compile filter predicates\n  - `nif_partition_events/2` - Partition events by predicate (matching/non-matching)\n  - `nif_first_match/2` - Find first matching event\n  - `nif_find_all/2` - Find all matching events with indexes\n  - `nif_any_match/2`, `nif_all_match/2` - Boolean aggregate predicates\n\n- **esdb_graph_nif** (Phase 4):\n  - `nif_build_edges/1` - Build edge list from event causation relationships\n  - `nif_find_roots/1`, `nif_find_leaves/1` - Find root/leaf nodes\n  - `nif_topo_sort/1` - Topological sort (Kahn's algorithm via petgraph)\n  - `nif_has_cycle/1` - Detect cycles in causation graph\n  - `nif_graph_stats/1` - Calculate node/edge/depth statistics\n  - `nif_to_dot/1,2` - Generate Graphviz DOT format\n  - `nif_has_path/2` - Check if path exists between nodes\n  - `nif_get_ancestors/2`, `nif_get_descendants/2` - BFS path finding","ref":"changelog.html#added-5"},{"type":"extras","title":"Changed - Changelog","doc":"- **Build profiles**:\n  - Added `enterprise` profile with Rust NIF compilation hooks\n  - Added `enterprise_test` profile for testing with NIFs\n  - Build with `rebar3 as enterprise compile` to enable NIFs","ref":"changelog.html#changed-4"},{"type":"extras","title":"Documentation - Changelog","doc":"- Updated README with Enterprise/Community edition information\n- Added NIF function documentation with academic references\n\n## [0.3.1] - 2025-12-20","ref":"changelog.html#documentation"},{"type":"extras","title":"Changed - Changelog","doc":"- **Version padding**: Increased from 6 to 12 characters (`?VERSION_PADDING` macro)\n  - Previous: 999,999 events per stream max (~2.7 hours at 100 events/sec)\n  - Now: 999,999,999,999 events per stream max (~317 years at 100 events/sec)\n  - Supports long-running neuroevolution, IoT, and continuous event streams","ref":"changelog.html#changed-5"},{"type":"extras","title":"Fixed - Changelog","doc":"- **EDoc errors**: Removed backticks and markdown from EDoc comments (breaks hex.pm docs)\n\n## [0.3.0] - 2025-12-20","ref":"changelog.html#fixed-3"},{"type":"extras","title":"Added - Changelog","doc":"- **Capability-Based Security** (`esdb_capability_verifier.erl`, `esdb_revocation.erl`):\n  - Server-side verification of UCAN-inspired capability tokens\n  - Ed25519 signature verification using issuer's public key from DID\n  - Token expiration and not-before time validation\n  - Resource URI pattern matching (exact, wildcard suffix, prefix)\n  - Action permission checking with wildcard support\n  - Token revocation management (ETS-based, gossip integration planned)\n  - Issuer revocation for compromised identities\n  - Content-addressed token IDs (CIDs) for revocation tracking\n  - Comprehensive unit tests (13 verifier tests + 6 revocation tests)\n\nThis completes Phase 3 of the decentralized security implementation.\nClient-side token creation is in reckon-gater, server-side verification is here.","ref":"changelog.html#added-6"},{"type":"extras","title":"Changed - Changelog","doc":"- **Documentation**: Replaced ASCII diagrams with SVG in README and guides","ref":"changelog.html#changed-6"},{"type":"extras","title":"Fixed - Changelog","doc":"- **README API documentation**: Fixed incorrect function signatures\n  - Subscriptions: Added missing `unsubscribe/3`, `get/2` functions\n  - Snapshots: Fixed `load/3`  `load_at/3`, `delete/3`  `delete_at/3`, added `exists/2`, `exists_at/3`\n  - Aggregator: Completely rewrote section - was showing non-existent API (`foldl/4`, `foldl_from_snapshot/4`)\n- **guides/snapshots.md**: Fixed `load/3`  `load_at/3`, `delete/3`  `delete_at/3`, rewrote aggregator example\n- **guides/cqrs.md**: Fixed subscription key usage in emitter group join\n- **guides/subscriptions.md**: Fixed invalid map access syntax\n- **guides/event_sourcing.md**: Fixed aggregator foldl signature (takes events list, not store/stream)\n\n## [0.2.0] - 2024-12-19","ref":"changelog.html#fixed-4"},{"type":"extras","title":"Added - Changelog","doc":"- **End-to-end tests**: 24 comprehensive e2e tests for gater integration:\n  - Worker registration (4 tests)\n  - Stream operations via gater (9 tests)\n  - Subscription operations (4 tests)\n  - Snapshot operations (4 tests)\n  - Load balancing (3 tests)\n- **Subscriptions**: Added `ack/4` function for acknowledging event delivery","ref":"changelog.html#added-7"},{"type":"extras","title":"Fixed - Changelog","doc":"- **Gateway worker API compatibility**:\n  - `get_version` now handles integer return correctly\n  - Snapshot operations use correct function names (`save`, `load_at`, `delete_at`)\n  - Subscription unsubscribe uses correct 3-arg version\n- **Header conflicts**: Added `ifndef` guards for `DEFAULT_TIMEOUT` macro","ref":"changelog.html#fixed-5"},{"type":"extras","title":"Changed - Changelog","doc":"- **reckon-gater integration**: Updated to work with gater's pg-based registry (replacing Ra)\n- **Test counts**: Now 72 unit + 53 integration + 24 e2e = 149 total tests\n\n## [0.1.0] - 2024-12-18","ref":"changelog.html#changed-7"},{"type":"extras","title":"Added - Changelog","doc":"- Initial release of reckon-db, a BEAM-native Event Store built on Khepri/Ra\n- Event stream operations:\n  - `append/4,5` - Write events with optimistic concurrency control\n  - `read/5` - Read events from streams (forward/backward)\n  - `get_version/2` - Get current stream version\n  - `exists/2` - Check if stream exists\n  - `list_streams/1` - List all streams in store\n  - `delete/2` - Soft delete streams\n- Subscription system:\n  - Stream subscriptions - events from specific streams\n  - Event type subscriptions - events by type across streams\n  - Pattern subscriptions - wildcard stream matching\n  - Payload subscriptions - content-based filtering\n- Snapshot management:\n  - `save/5` - Save aggregate state snapshots\n  - `load/2,3` - Load latest or specific version snapshots\n  - `list/2` - List all snapshots for a stream\n  - `delete/3` - Delete old snapshots\n- Aggregation utilities:\n  - `foldl/4` - Fold over events with accumulator\n  - `foldl_from_snapshot/4` - Fold starting from latest snapshot\n- Cluster support:\n  - UDP multicast discovery (LibCluster gossip compatible)\n  - Automatic Khepri/Ra cluster formation\n  - Node monitoring and failover\n  - Leader election and tracking\n- Emitter pools for high-throughput event delivery\n- Partitioned writers for concurrent stream writes\n- BEAM telemetry integration with configurable handlers\n- Comprehensive test suite (72 unit + 53 integration tests)\n- Educational guides:\n  - Event Sourcing fundamentals\n  - CQRS patterns\n  - Subscriptions usage\n  - Snapshots optimization","ref":"changelog.html#added-8"},{"type":"extras","title":"Dependencies - Changelog","doc":"- Khepri 0.17.2 - Raft-based distributed storage\n- Ra 2.16.12 - Raft consensus implementation\n- Telemetry 1.3.0 - BEAM telemetry for observability","ref":"changelog.html#dependencies"},{"type":"extras","title":"Event Sourcing Paradigms","doc":"# Event Sourcing Paradigms: Entity, Relationship, and Process-Centric\n\nThis guide explores three distinct approaches to event sourcing, each with different mental models, trade-offs, and use cases. Understanding these paradigms helps you choose the right approach for your domain.\n\n![Three Paradigms Overview](../assets/event_sourcing_paradigms.svg)","ref":"event_sourcing_paradigms.html"},{"type":"extras","title":"The Three Paradigms - Event Sourcing Paradigms","doc":"| Aspect | Entity-Centric | Relationship-Centric (DCB) | Process-Centric |\n|--------|----------------|---------------------------|-----------------|\n| **Stream identity** | Entity ID | Context name | Process instance ID |\n| **What stream represents** | Entity's complete history | All facts in a bounded context | Business process execution log |\n| **Aggregate purpose** | Entity state reconstruction | Ad-hoc decision model (query-time) | Process context (\"dossier\") |\n| **Concurrency unit** | Single entity's stream | Query-based tag intersection | Single process instance |\n| **Cross-cutting concerns** | Sagas, process managers | Tags + query-based locking | Process flow owns all participants |\n| **Mental model** | \"Entity has history\" | \"Facts are tagged for slicing\" | \"Dossier passed desk to desk\" |\n\n---","ref":"event_sourcing_paradigms.html#the-three-paradigms"},{"type":"extras","title":"Entity-Centric Paradigm (Traditional DDD) - Event Sourcing Paradigms","doc":"The most common approach, derived from Domain-Driven Design. Each entity (aggregate) owns a dedicated event stream.","ref":"event_sourcing_paradigms.html#entity-centric-paradigm-traditional-ddd"},{"type":"extras","title":"Mental Model - Event Sourcing Paradigms","doc":"> \"Every entity has a history. The entity IS its history.\"\n\n```\nStream: order-123\n OrderPlaced\n ItemAdded\n PaymentReceived\n OrderShipped\n OrderDelivered\n\nStream: user-456\n UserRegistered\n ProfileUpdated\n PasswordChanged\n AccountVerified\n```","ref":"event_sourcing_paradigms.html#mental-model"},{"type":"extras","title":"Characteristics - Event Sourcing Paradigms","doc":"**Strengths:**\n- Intuitive mapping to domain objects\n- Clear ownership: one stream per aggregate\n- Well-understood patterns (DDD literature)\n- Simple optimistic concurrency (stream version)\n\n**Weaknesses:**\n- Cross-entity invariants require sagas\n- Fact duplication when entities interact\n- Saga complexity grows with domain complexity\n- Compensating transactions for failure handling","ref":"event_sourcing_paradigms.html#characteristics"},{"type":"extras","title":"Example: Course Enrollment (Entity-Centric) - Event Sourcing Paradigms","doc":"```erlang\n%% Two streams, fact duplicated\n%% Stream: course-CS101\n%%    CourseCreated\n%%    StudentEnrolled {student_id: 456}   FACT 1\n%%\n%% Stream: student-456\n%%    StudentCreated\n%%    CourseJoined {course_id: CS101}     SAME FACT, DUPLICATED\n\n%% Saga required to coordinate:\n%% 1. Check student course count < 10\n%% 2. Check course student count < 30\n%% 3. Append to course stream\n%% 4. Append to student stream\n%% 5. If step 4 fails, compensate step 3\n```","ref":"event_sourcing_paradigms.html#example-course-enrollment-entity-centric"},{"type":"extras","title":"When to Use - Event Sourcing Paradigms","doc":"- Domains where entities are truly independent\n- Simple CRUD-like operations\n- Well-bounded aggregates with few cross-entity invariants\n\n---","ref":"event_sourcing_paradigms.html#when-to-use"},{"type":"extras","title":"Relationship-Centric Paradigm (Dynamic Consistency Boundaries) - Event Sourcing Paradigms","doc":"Introduced by Sara Pellegrini in her \"Killing the Aggregate\" thesis. Events are tagged with multiple entities, enabling multi-dimensional querying.","ref":"event_sourcing_paradigms.html#relationship-centric-paradigm-dynamic-consistency-boundaries"},{"type":"extras","title":"Mental Model - Event Sourcing Paradigms","doc":"> \"Events are facts about relationships, not entities. Facts can be sliced by any participating entity.\"\n\n```\nStream: enrollment-context (single stream for all enrollments)\n Enrolled {student: 456, course: CS101, tags: [student:456, course:CS101]}\n Enrolled {student: 456, course: MATH201, tags: [student:456, course:MATH201]}\n Enrolled {student: 789, course: CS101, tags: [student:789, course:CS101]}\n ...\n\nQuery by tags:\n- \"course:CS101\"  2 enrollments\n- \"student:456\"  2 enrollments\n```","ref":"event_sourcing_paradigms.html#mental-model-1"},{"type":"extras","title":"Characteristics - Event Sourcing Paradigms","doc":"**Strengths:**\n- No fact duplication\n- No sagas for cross-entity invariants\n- Query-time flexibility\n- Single source of truth for relationships\n\n**Weaknesses:**\n- Requires tag indexing infrastructure\n- Double-query cost (decision + precondition check)\n- Query-time aggregation loses temporal context\n- Mental model shift from traditional DDD","ref":"event_sourcing_paradigms.html#characteristics-1"},{"type":"extras","title":"Example: Course Enrollment (DCB) - Event Sourcing Paradigms","doc":"```erlang\n%% Single stream with tags\nEvent = #{\n    event_type => enrollment_requested,\n    data => #{student_id => 456, course_id => <<\"CS101\">>},\n    tags => [<<\"student:456\">>, <<\"course:CS101\">>]\n},\n\n%% Query-based concurrency check:\n%% 1. Query events matching \"course:CS101\"  get count + last position\n%% 2. Query events matching \"student:456\"  get count + last position\n%% 3. Append with precondition: no new events matching either query since position\nQuery = {and, [{tags, [<<\"course:CS101\">>]}, {tags, [<<\"student:456\">>]}]},\nreckon_db:append(<<\"enrollments\">>, [Event], {query, Query, LastPosition}).\n```","ref":"event_sourcing_paradigms.html#example-course-enrollment-dcb"},{"type":"extras","title":"When to Use - Event Sourcing Paradigms","doc":"- Domains dominated by relationships between entities\n- Cross-entity invariants are common\n- Analytical/reporting needs require multi-dimensional slicing\n- Willing to invest in tag indexing infrastructure","ref":"event_sourcing_paradigms.html#when-to-use-1"},{"type":"extras","title":"References - Event Sourcing Paradigms","doc":"- [DCB.events](https://dcb.events/) - Official documentation\n- [Sara Pellegrini's Event Thinking](https://sara.event-thinking.io/)\n- [Axon Framework 5 DCB Support](https://www.axoniq.io/blog/dcb-in-af-5)\n- [EventSourcingDB DCB Best Practices](https://docs.eventsourcingdb.io/best-practices/dynamic-consistency-boundaries/)\n\n---","ref":"event_sourcing_paradigms.html#references"},{"type":"extras","title":"Process-Centric Paradigm (The Dossier Model) - Event Sourcing Paradigms","doc":"**ReckonDB's recommended approach.** An event stream represents a business process instance, not an entity. The aggregate is the accumulated context - like a dossier passed from desk to desk.","ref":"event_sourcing_paradigms.html#process-centric-paradigm-the-dossier-model"},{"type":"extras","title":"Mental Model - Event Sourcing Paradigms","doc":"> \"An event stream is an ordered log of facts that represent a business process. The aggregate is the 'dossier' - accumulated context passed from desk to desk, gradually filling with facts.\"\n\n![The Dossier Metaphor](../assets/dossier_metaphor.svg)","ref":"event_sourcing_paradigms.html#mental-model-2"},{"type":"extras","title":"The Dossier Metaphor - Event Sourcing Paradigms","doc":"Imagine a loan application process in a traditional office:\n\n```\n\n THE DOSSIER - Physical folder passed between desks              \n\n                                                                 \n DESK 1 (Intake):                                                \n    Application form received                                  \n    Dossier starts: applicant name, requested amount           \n                                                                 \n DESK 2 (Credit Check):                                          \n    Credit report pulled                                       \n    Dossier gains: credit score, debt-to-income ratio          \n                                                                 \n DESK 3 (Underwriting):                                          \n    Risk assessment completed                                  \n    Dossier gains: approval decision, conditions, notes        \n                                                                 \n DESK 4 (Documentation):                                         \n    Documents verified                                         \n    Dossier gains: verification checklist, issues found        \n                                                                 \n DESK 5 (Closing):                                               \n    Loan disbursed                                             \n    Dossier complete: final terms, disbursement details        \n                                                                 \n\n\nThe PROCESS owns the stream.\nThe applicant, the loan, the credit bureau - all are PARTICIPANTS.\n```","ref":"event_sourcing_paradigms.html#the-dossier-metaphor"},{"type":"extras","title":"Characteristics - Event Sourcing Paradigms","doc":"**Strengths:**\n- Complete auditability: dossier records WHAT WE KNEW at each decision\n- Single stream per process instance (no coordination)\n- Invariants checked as process guards using read models\n- Natural fit for workflow-heavy domains\n- \"Why did we approve this?\"  replay the dossier\n\n**Weaknesses:**\n- Requires read models for cross-process queries\n- Less intuitive for pure entity-focused domains\n- May need to denormalize entity state across processes","ref":"event_sourcing_paradigms.html#characteristics-2"},{"type":"extras","title":"Example: Course Enrollment (Process-Centric) - Event Sourcing Paradigms","doc":"```erlang\n%% Process: EnrollmentRequest\n%% Stream: enrollment-request-{uuid}\n\n-record(enrollment_dossier, {\n    request_id,\n    student_id,\n    course_id,\n    %% Context captured at decision time\n    student_courses_at_request = [],\n    course_students_at_request = [],\n    %% Process outcome\n    validation_status,\n    outcome,\n    outcome_reason\n}).\n\n%% Command handler: check invariants as process GUARDS\nhandle_command(#request_enrollment{student_id = S, course_id = C}, _State) ->\n    %% Fetch current state from READ MODELS (projections)\n    StudentCourses = enrollments_projection:courses_for_student(S),\n    CourseStudents = enrollments_projection:students_for_course(C),\n\n    %% Guard: check both invariants BEFORE emitting events\n    case {length(StudentCourses) < 10, length(CourseStudents) < 30} of\n        {true, true} ->\n            %% Dossier records what we knew at decision time\n            [#enrollment_requested{\n                student_id = S,\n                course_id = C,\n                student_courses_at_request = StudentCourses,\n                course_students_at_request = CourseStudents\n            }];\n        {false, _} ->\n            [#enrollment_rejected{\n                student_id = S,\n                course_id = C,\n                reason = student_course_limit_reached,\n                student_course_count = length(StudentCourses)\n            }];\n        {_, false} ->\n            [#enrollment_rejected{\n                student_id = S,\n                course_id = C,\n                reason = course_capacity_reached,\n                course_student_count = length(CourseStudents)\n            }]\n    end.\n\n%% Apply events to build dossier state\napply_event(#enrollment_requested{} = E, Dossier) ->\n    Dossier#enrollment_dossier{\n        student_id = E#enrollment_requested.student_id,\n        course_id = E#enrollment_requested.course_id,\n        student_courses_at_request = E#enrollment_requested.student_courses_at_request,\n        course_students_at_request = E#enrollment_requested.course_students_at_request,\n        validation_status = valid\n    };\n\napply_event(#enrollment_completed{}, Dossier) ->\n    Dossier#enrollment_dossier{outcome = completed};\n\napply_event(#enrollment_rejected{reason = R}, Dossier) ->\n    Dossier#enrollment_dossier{\n        outcome = rejected,\n        outcome_reason = R\n    }.\n```","ref":"event_sourcing_paradigms.html#example-course-enrollment-process-centric"},{"type":"extras","title":"The Key Insight: Temporal Context - Event Sourcing Paradigms","doc":"The process-centric model captures **temporal context** that query-time aggregation cannot:\n\n```\nAUDITOR: \"Why was this loan approved despite the applicant's low credit score?\"\n\nENTITY-CENTRIC: \"The aggregate says approved. Check the saga logs... somewhere.\"\n\nDCB: \"Query the events... but we only know the final state, not what\n      the underwriter saw at decision time.\"\n\nPROCESS-CENTRIC: \"Replay the dossier. At decision time (event 3),\n                  the underwriter saw: credit score 620, but debt-to-income\n                  was 28% (good), and applicant had 5 years at current job.\n                  The dossier records all context that was available.\"\n```","ref":"event_sourcing_paradigms.html#the-key-insight-temporal-context"},{"type":"extras","title":"When to Use - Event Sourcing Paradigms","doc":"- Workflow-heavy domains (loan processing, insurance claims, order fulfillment)\n- Regulatory/compliance requirements for decision auditability\n- Processes with multiple participants that must be coordinated\n- Long-running processes with multiple decision points\n\n---","ref":"event_sourcing_paradigms.html#when-to-use-2"},{"type":"extras","title":"Paradigm Comparison: The Enrollment Problem - Event Sourcing Paradigms","doc":"All three paradigms can solve the same problem. Here's how they compare:\n\n| Paradigm | Solution | Code Complexity | Runtime Complexity | Auditability |\n|----------|----------|-----------------|-------------------|--------------|\n| **Entity-Centric** | Two streams + saga | High (saga logic, compensation) | Medium (saga orchestration) | Medium |\n| **DCB** | Tags + query-based locking | Medium (tag indexing) | Higher (double queries) | Low (query-time) |\n| **Process-Centric** | Process stream + read model guards | Low (simple guards) | Low (single stream writes) | High (temporal) |","ref":"event_sourcing_paradigms.html#paradigm-comparison-the-enrollment-problem"},{"type":"extras","title":"Complexity Analysis - Event Sourcing Paradigms","doc":"```\nEntity-Centric:\n  \n   Components needed:                                          \n   - Course aggregate                                          \n   - Student aggregate                                         \n   - EnrollmentSaga (coordinates both)                         \n   - CompensationHandler (rollback on failure)                 \n   - 2 streams, 4 event types, saga state machine              \n  \n\nDCB:\n  \n   Components needed:                                          \n   - Tag index (new infrastructure)                            \n   - Query-based concurrency (new append mode)                 \n   - Decision model builder                                    \n   - 1 stream, 1 event type, 2 queries per write               \n  \n\nProcess-Centric:\n  \n   Components needed:                                          \n   - EnrollmentRequest process (aggregate)                     \n   - EnrollmentsProjection (read model)                        \n   - 1 stream per request, 2-3 event types, simple guards      \n  \n```\n\n---","ref":"event_sourcing_paradigms.html#complexity-analysis"},{"type":"extras","title":"Choosing the Right Paradigm - Event Sourcing Paradigms","doc":"","ref":"event_sourcing_paradigms.html#choosing-the-right-paradigm"},{"type":"extras","title":"Decision Framework - Event Sourcing Paradigms","doc":"```\nSTART\n  \n  \n\n Is your domain workflow-heavy?          \n (loan processing, claims, fulfillment)  \n\n                                \n  YES                            NO\n                                \n                                \n    \n PROCESS-CENTRIC      Do you have many cross-entity   \n (Dossier Model)      invariants?                     \n    \n                                                 \n                         YES                      NO\n                                                 \n                                                 \n                  \n                Can you invest in       ENTITY-CENTRIC  \n                tag infrastructure?     (Traditional)   \n                  \n                                \n                 YES             NO\n                                \n                                \n            \n             DCB        PROCESS-CENTRIC \n             (simpler)       \n                         \n```","ref":"event_sourcing_paradigms.html#decision-framework"},{"type":"extras","title":"Hybrid Approaches - Event Sourcing Paradigms","doc":"Real systems often use multiple paradigms:\n\n```erlang\n%% Master data: Entity-centric\n%% Stream: product-SKU123 (catalog item, rarely changes)\n%% Stream: customer-C456 (profile data)\n\n%% Transactional processes: Process-centric\n%% Stream: order-process-ORD789 (order fulfillment)\n%% Stream: return-request-RET012 (return processing)\n\n%% Analytics: Tag-based queries (without DCB concurrency)\n%% Query by tags for reporting, not for consistency\n```\n\n---","ref":"event_sourcing_paradigms.html#hybrid-approaches"},{"type":"extras","title":"ReckonDB's Position - Event Sourcing Paradigms","doc":"**ReckonDB recommends the process-centric paradigm** for most event-sourced systems because:\n\n1. **Better auditability** - The dossier captures decision context\n2. **Simpler implementation** - No sagas, no tag indexes for concurrency\n3. **Natural fit** - Most business operations are processes, not entity mutations\n\nHowever, ReckonDB supports **tags for query purposes**:\n\n```erlang\n%% Tags for analytics, NOT for concurrency control\nEvent = #{\n    event_type => enrollment_completed,\n    data => #{\n        process_id => <<\"enrollment-req-123\">>,\n        student_id => 456,\n        course_id => <<\"CS101\">>\n    },\n    tags => [<<\"student:456\">>, <<\"course:CS101\">>]  %% For queries only\n},\n\n%% Concurrency remains stream-version-based\n{ok, V} = reckon_db:append(<<\"enrollment-req-123\">>, [Event], ExpectedVersion).\n\n%% But you can query across processes by tags\n{ok, AllStudentEnrollments} = reckon_db:read_by_tags([<<\"student:456\">>], #{}).\n```\n\n---","ref":"event_sourcing_paradigms.html#reckondb-s-position"},{"type":"extras","title":"Further Reading - Event Sourcing Paradigms","doc":"- [Event Sourcing Guide](event_sourcing.md) - Core event sourcing concepts\n- [CQRS Guide](cqrs.md) - Command Query Responsibility Segregation\n- [Subscriptions Guide](subscriptions.md) - Real-time event notifications\n- [Snapshots Guide](snapshots.md) - Optimizing aggregate loading","ref":"event_sourcing_paradigms.html#further-reading"},{"type":"extras","title":"References - Event Sourcing Paradigms","doc":"","ref":"event_sourcing_paradigms.html#references-1"},{"type":"extras","title":"Process-Centric / Dossier Model - Event Sourcing Paradigms","doc":"- ReckonDB documentation and this guide","ref":"event_sourcing_paradigms.html#process-centric-dossier-model"},{"type":"extras","title":"Dynamic Consistency Boundaries (DCB) - Event Sourcing Paradigms","doc":"- [DCB.events](https://dcb.events/) - Official DCB documentation\n- [Sara Pellegrini's Event Thinking](https://sara.event-thinking.io/) - \"Killing the Aggregate\" thesis\n- [Kill Aggregate? Interview](https://docs.eventsourcingdb.io/blog/2025/12/15/kill-aggregate-an-interview-on-dynamic-consistency-boundaries/) - Bastian Waidelich, EventSourcingDB\n- [DCB in Axon Framework 5](https://www.axoniq.io/blog/dcb-in-af-5) - First major framework support\n- [Axon Server 2025.1](https://www.axoniq.io/blog/axon-server-future-proof-event-store) - Event store with DCB","ref":"event_sourcing_paradigms.html#dynamic-consistency-boundaries-dcb"},{"type":"extras","title":"Entity-Centric (Traditional) - Event Sourcing Paradigms","doc":"- Martin Fowler: [Event Sourcing](https://martinfowler.com/eaaDev/EventSourcing.html)\n- Greg Young: [CQRS and Event Sourcing](https://cqrs.files.wordpress.com/2010/11/cqrs_documents.pdf)\n- Vaughn Vernon: \"Implementing Domain-Driven Design\"","ref":"event_sourcing_paradigms.html#entity-centric-traditional"},{"type":"extras","title":"Academic - Event Sourcing Paradigms","doc":"- Abadi: \"Consistency Tradeoffs in Modern Distributed Database System Design\"\n- Helland & Campbell: \"Building on Quicksand\"","ref":"event_sourcing_paradigms.html#academic"},{"type":"extras","title":"Event Sourcing","doc":"# Event Sourcing with reckon-db\n\nEvent Sourcing is an architectural pattern where the state of an application is determined by a sequence of events. Instead of storing just the current state, you store the complete history of state changes as immutable events.\n\n![Event Sourcing vs CRUD](../assets/event_sourcing.svg)","ref":"event_sourcing.html"},{"type":"extras","title":"What is Event Sourcing? - Event Sourcing","doc":"Traditional CRUD-based systems store only the current state:\n\n```\nUser Record: {id: 123, name: \"Alice\", email: \"alice@example.com\", balance: 150}\n```\n\nEvent-sourced systems store the history of changes:\n\n```\nEvent 1: UserCreated {id: 123, name: \"Alice\", email: \"alice@example.com\"}\nEvent 2: BalanceDeposited {user_id: 123, amount: 200}\nEvent 3: BalanceWithdrawn {user_id: 123, amount: 50}\n```\n\nThe current state is derived by replaying these events.","ref":"event_sourcing.html#what-is-event-sourcing"},{"type":"extras","title":"Benefits of Event Sourcing - Event Sourcing","doc":"","ref":"event_sourcing.html#benefits-of-event-sourcing"},{"type":"extras","title":"Complete Audit Trail - Event Sourcing","doc":"Every change is recorded with a timestamp and metadata. This is invaluable for:\n- Regulatory compliance (financial systems, healthcare)\n- Debugging production issues\n- Understanding user behavior","ref":"event_sourcing.html#complete-audit-trail"},{"type":"extras","title":"Temporal Queries - Event Sourcing","doc":"You can reconstruct the state at any point in time:\n\n```erlang\n%% Get account balance as of last month\n{ok, Events} = reckon_db_streams:read(my_store, <<\"account-123\">>, 0, 1000, forward),\nPastEvents = [E || E <- Events, E#event.timestamp < LastMonthTimestamp],\nBalance = lists:foldl(fun apply_event/2, 0, PastEvents).\n```","ref":"event_sourcing.html#temporal-queries"},{"type":"extras","title":"Event Replay - Event Sourcing","doc":"Rebuild read models, fix bugs in projections, or create new views of historical data:\n\n```erlang\n%% Rebuild a projection from scratch\n{ok, Events} = reckon_db_streams:read(my_store, <<\"orders-*\">>, 0, infinity, forward),\nlists:foreach(fun(E) -> update_projection(E) end, Events).\n```","ref":"event_sourcing.html#event-replay"},{"type":"extras","title":"Decoupled Systems - Event Sourcing","doc":"Events can be consumed by multiple subscribers independently:\n\n![Event Fan-Out](../assets/event_fanout.svg)","ref":"event_sourcing.html#decoupled-systems"},{"type":"extras","title":"Event Sourcing with reckon-db - Event Sourcing","doc":"","ref":"event_sourcing.html#event-sourcing-with-reckon-db"},{"type":"extras","title":"Streams - Event Sourcing","doc":"A **stream** is an ordered sequence of events sharing a common identifier (the stream ID). Streams typically represent:\n- An aggregate (e.g., `order-123`, `user-456`)\n- A category (e.g., `orders`, `users`)\n- A partition (e.g., `orders-region-eu`)\n\n```erlang\n%% Append events to a stream\nEvents = [\n    #{\n        event_type => <<\"OrderPlaced\">>,\n        data => #{order_id => <<\"ord-123\">>, items => [...], total => 9999},\n        metadata => #{user_id => <<\"user-456\">>, correlation_id => <<\"req-789\">>}\n    }\n],\n{ok, Version} = reckon_db_streams:append(my_store, <<\"order-ord-123\">>, -1, Events).\n```","ref":"event_sourcing.html#streams"},{"type":"extras","title":"Events - Event Sourcing","doc":"Events are immutable facts that have happened. They should:\n- Be named in past tense (e.g., `OrderPlaced`, not `PlaceOrder`)\n- Contain all information needed to understand what happened\n- Be business-meaningful (e.g., `AccountOverdrawn`, not `BalanceUpdated`)\n\n```erlang\n%% Event structure\n#{\n    event_type => <<\"OrderPlaced\">>,      %% What happened\n    data => #{                             %% The event payload\n        order_id => <<\"ord-123\">>,\n        customer_id => <<\"cust-456\">>,\n        items => [\n            #{product_id => <<\"prod-1\">>, quantity => 2, price => 1999}\n        ],\n        total => 3998\n    },\n    metadata => #{                         %% Cross-cutting concerns\n        correlation_id => <<\"req-abc\">>,   %% Traces related operations\n        causation_id => <<\"evt-xyz\">>,     %% What caused this event\n        user_id => <<\"user-789\">>,         %% Who triggered it\n        timestamp => 1703001234567         %% When it happened\n    }\n}\n```","ref":"event_sourcing.html#events"},{"type":"extras","title":"Optimistic Concurrency - Event Sourcing","doc":"reckon-db uses optimistic concurrency control to prevent conflicting writes:\n\n```erlang\n%% Expected version semantics:\n%% -1 (NO_STREAM): Stream must not exist (first write)\n%% -2 (ANY_VERSION): No version check, always append\n%% N >= 0: Stream version must equal N\n\n%% First write to a new stream\n{ok, 0} = reckon_db_streams:append(Store, <<\"order-123\">>, -1, [Event1]).\n\n%% Subsequent writes must specify expected version\n{ok, 1} = reckon_db_streams:append(Store, <<\"order-123\">>, 0, [Event2]).\n\n%% Concurrent writes will fail with version mismatch\n%% Process A reads version 1\n%% Process B reads version 1\n%% Process A writes with expected version 1 -> succeeds, version is now 2\n%% Process B writes with expected version 1 -> fails! (wrong_expected_version)\n```","ref":"event_sourcing.html#optimistic-concurrency"},{"type":"extras","title":"Designing Events - Event Sourcing","doc":"","ref":"event_sourcing.html#designing-events"},{"type":"extras","title":"Event Naming - Event Sourcing","doc":"Use past tense verbs that describe business facts:\n\n| Good | Bad |\n|------|-----|\n| `OrderPlaced` | `CreateOrder` |\n| `PaymentReceived` | `ProcessPayment` |\n| `ItemShipped` | `ShipItem` |\n| `AccountOverdrawn` | `UpdateBalance` |","ref":"event_sourcing.html#event-naming"},{"type":"extras","title":"Event Granularity - Event Sourcing","doc":"Events should be atomic business facts. Avoid:\n- Generic events like `EntityUpdated` (not meaningful)\n- Overly fine-grained events (one per field change)\n- Composite events (multiple unrelated changes)\n\n```erlang\n%% Good: Specific, meaningful events\n#{event_type => <<\"AddressChanged\">>, data => #{\n    old_address => OldAddr,\n    new_address => NewAddr,\n    reason => <<\"customer_request\">>\n}}\n\n%% Bad: Generic, meaningless event\n#{event_type => <<\"CustomerUpdated\">>, data => #{\n    field => <<\"address\">>,\n    value => NewAddr\n}}\n```","ref":"event_sourcing.html#event-granularity"},{"type":"extras","title":"Event Versioning - Event Sourcing","doc":"Events are immutable, but schemas evolve. Use explicit versions:\n\n```erlang\n%% Version 1\n#{event_type => <<\"OrderPlaced.v1\">>, data => #{\n    order_id => ...,\n    items => [...]\n}}\n\n%% Version 2 (added shipping_address)\n#{event_type => <<\"OrderPlaced.v2\">>, data => #{\n    order_id => ...,\n    items => [...],\n    shipping_address => #{}\n}}\n```\n\nHandle schema evolution in your projections:\n\n```erlang\nhandle_event(#{event_type := <<\"OrderPlaced.v1\">>} = Event) ->\n    %% Default shipping address for v1 events\n    upgrade_to_v2(Event);\nhandle_event(#{event_type := <<\"OrderPlaced.v2\">>} = Event) ->\n    process_order(Event).\n```","ref":"event_sourcing.html#event-versioning"},{"type":"extras","title":"Building Aggregates - Event Sourcing","doc":"An **aggregate** is a domain object that encapsulates state and enforces invariants. In event sourcing, aggregates:\n1. Load their state by replaying events\n2. Validate commands against current state\n3. Emit new events if the command succeeds\n\n```erlang\n-module(order_aggregate).\n-export([new/0, apply_event/2, place_order/2, add_item/3]).\n\n-record(order, {\n    id,\n    status = pending,\n    items = [],\n    total = 0\n}).\n\n%% Create a new aggregate\nnew() -> #order{}.\n\n%% Apply events to rebuild state\napply_event(#{event_type := <<\"OrderPlaced\">>} = E, _Order) ->\n    Data = maps:get(data, E),\n    #order{\n        id = maps:get(order_id, Data),\n        status = placed,\n        items = maps:get(items, Data),\n        total = maps:get(total, Data)\n    };\n\napply_event(#{event_type := <<\"ItemAdded\">>} = E, Order) ->\n    Data = maps:get(data, E),\n    NewItem = #{\n        product_id => maps:get(product_id, Data),\n        quantity => maps:get(quantity, Data),\n        price => maps:get(price, Data)\n    },\n    Order#order{\n        items = [NewItem | Order#order.items],\n        total = Order#order.total + (maps:get(quantity, Data) * maps:get(price, Data))\n    };\n\napply_event(#{event_type := <<\"OrderShipped\">>}, Order) ->\n    Order#order{status = shipped}.\n\n%% Commands that produce events\nplace_order(OrderId, Items) ->\n    Total = lists:sum([Q * P || #{quantity := Q, price := P} <- Items]),\n    {ok, [#{\n        event_type => <<\"OrderPlaced\">>,\n        data => #{order_id => OrderId, items => Items, total => Total}\n    }]}.\n\nadd_item(#order{status = placed} = _Order, ProductId, Quantity) ->\n    Price = get_product_price(ProductId),\n    {ok, [#{\n        event_type => <<\"ItemAdded\">>,\n        data => #{product_id => ProductId, quantity => Quantity, price => Price}\n    }]};\nadd_item(#order{status = shipped}, _ProductId, _Quantity) ->\n    {error, order_already_shipped}.\n```","ref":"event_sourcing.html#building-aggregates"},{"type":"extras","title":"Loading Aggregate State - Event Sourcing","doc":"Use `reckon_db_aggregator` to rebuild aggregate state:\n\n```erlang\n%% Load order aggregate from event stream\nload_order(StoreId, OrderId) ->\n    StreamId = <<\"order-\", OrderId/binary>>,\n\n    %% Read events from stream\n    {ok, Events} = reckon_db_streams:read(StoreId, StreamId, 0, 10000, forward),\n\n    %% Apply events to initial state using custom apply function\n    InitialState = order_aggregate:new(),\n    FinalState = lists:foldl(\n        fun(Event, Acc) -> order_aggregate:apply_event(Event, Acc) end,\n        InitialState,\n        Events\n    ),\n    FinalState.\n\n%% Or use reckon_db_aggregator for tagged value aggregation\n%% (useful when events have {sum, N} or {overwrite, V} tagged values)\nload_order_with_aggregator(StoreId, OrderId) ->\n    StreamId = <<\"order-\", OrderId/binary>>,\n    {ok, Events} = reckon_db_streams:read(StoreId, StreamId, 0, 10000, forward),\n    TaggedState = reckon_db_aggregator:foldl(Events),\n    reckon_db_aggregator:finalize(TaggedState).\n```","ref":"event_sourcing.html#loading-aggregate-state"},{"type":"extras","title":"Further Reading - Event Sourcing","doc":"- [CQRS Guide](cqrs.md) - Command Query Responsibility Segregation\n- [Subscriptions Guide](subscriptions.md) - Real-time event notifications\n- [Snapshots Guide](snapshots.md) - Optimizing aggregate loading","ref":"event_sourcing.html#further-reading"},{"type":"extras","title":"References - Event Sourcing","doc":"- Martin Fowler: [Event Sourcing](https://martinfowler.com/eaaDev/EventSourcing.html)\n- Greg Young: [CQRS and Event Sourcing](https://cqrs.files.wordpress.com/2010/11/cqrs_documents.pdf)\n- Vaughn Vernon: \"Implementing Domain-Driven Design\" (Chapters 8-10)","ref":"event_sourcing.html#references"},{"type":"extras","title":"CQRS","doc":"# CQRS with reckon-db\n\nCommand Query Responsibility Segregation (CQRS) is an architectural pattern that separates read and write operations into distinct models. Combined with event sourcing, CQRS enables highly scalable and maintainable systems.","ref":"cqrs.html"},{"type":"extras","title":"What is CQRS? - CQRS","doc":"In traditional architectures, the same model handles both reads and writes:\n\n![Traditional Single Model](../assets/cqrs_traditional.svg)\n\nCQRS separates these concerns:\n\n![CQRS Separated Architecture](../assets/cqrs_separated.svg)","ref":"cqrs.html#what-is-cqrs"},{"type":"extras","title":"Why CQRS? - CQRS","doc":"","ref":"cqrs.html#why-cqrs"},{"type":"extras","title":"Different Optimization Strategies - CQRS","doc":"Reads and writes have fundamentally different characteristics:\n\n| Writes | Reads |\n|--------|-------|\n| Validate business rules | No validation needed |\n| Must be consistent | Can be eventually consistent |\n| Lower volume | Higher volume (often 10-100x) |\n| Complex domain logic | Simple queries |\n\nWith CQRS, you optimize each path independently:\n- **Write side**: Focus on business logic, invariants, and consistency\n- **Read side**: Focus on query performance, denormalization, and caching","ref":"cqrs.html#different-optimization-strategies"},{"type":"extras","title":"Scalability - CQRS","doc":"Read and write workloads can scale independently:\n\n![CQRS Scaling](../assets/cqrs_scaling.svg)","ref":"cqrs.html#scalability"},{"type":"extras","title":"Multiple Read Models - CQRS","doc":"Different consumers can have different views of the same data:\n\n```erlang\n%% Same events, different read models\n\n%% Order Events Stream\n[\n    #{event_type => <<\"OrderPlaced\">>, data => #{...}},\n    #{event_type => <<\"PaymentReceived\">>, data => #{...}},\n    #{event_type => <<\"OrderShipped\">>, data => #{...}}\n]\n\n%% Read Model 1: Customer Dashboard (optimized for display)\n#{\n    order_id => <<\"ord-123\">>,\n    status => <<\"Shipped\">>,\n    status_history => [...],\n    tracking_url => <<\"https://...\">>\n}\n\n%% Read Model 2: Warehouse System (optimized for picking)\n#{\n    order_id => <<\"ord-123\">>,\n    items => [#{sku => ..., location => ..., quantity => ...}],\n    priority => high,\n    shipping_method => express\n}\n\n%% Read Model 3: Analytics (optimized for aggregation)\n#{\n    date => <<\"2024-01-15\">>,\n    region => <<\"EU\">>,\n    total_orders => 1547,\n    total_revenue => 234567,\n    avg_order_value => 151.63\n}\n```","ref":"cqrs.html#multiple-read-models"},{"type":"extras","title":"CQRS with reckon-db - CQRS","doc":"","ref":"cqrs.html#cqrs-with-reckon-db"},{"type":"extras","title":"The Command Side - CQRS","doc":"Commands represent intentions to change state. They are validated and may produce events:\n\n```erlang\n-module(order_commands).\n-export([handle/2]).\n\n%% Handle PlaceOrder command\nhandle({place_order, OrderId, CustomerId, Items}, State) ->\n    %% Validate business rules\n    case validate_items(Items) of\n        {error, Reason} ->\n            {error, Reason};\n        ok ->\n            %% Check inventory\n            case check_inventory(Items) of\n                {error, out_of_stock} ->\n                    {error, items_out_of_stock};\n                ok ->\n                    %% Generate events\n                    Total = calculate_total(Items),\n                    Event = #{\n                        event_type => <<\"OrderPlaced\">>,\n                        data => #{\n                            order_id => OrderId,\n                            customer_id => CustomerId,\n                            items => Items,\n                            total => Total\n                        },\n                        metadata => #{\n                            command => place_order,\n                            timestamp => erlang:system_time(millisecond)\n                        }\n                    },\n                    {ok, [Event]}\n            end\n    end;\n\n%% Handle CancelOrder command\nhandle({cancel_order, OrderId, Reason}, State) ->\n    %% Load current state\n    Order = load_order(State, OrderId),\n    case Order#order.status of\n        shipped ->\n            {error, cannot_cancel_shipped_order};\n        cancelled ->\n            {error, already_cancelled};\n        _ ->\n            Event = #{\n                event_type => <<\"OrderCancelled\">>,\n                data => #{order_id => OrderId, reason => Reason}\n            },\n            {ok, [Event]}\n    end.\n```","ref":"cqrs.html#the-command-side"},{"type":"extras","title":"The Query Side: Projections - CQRS","doc":"Projections transform events into read models. They run asynchronously and subscribe to event streams:\n\n```erlang\n-module(order_dashboard_projection).\n-behaviour(gen_server).\n\n-export([start_link/1, get_order/1, list_customer_orders/1]).\n-export([init/1, handle_info/2, handle_call/3]).\n\n%% Read model stored in ETS for fast lookups\n-define(TABLE, order_dashboard).\n\nstart_link(StoreId) ->\n    gen_server:start_link({local, ?MODULE}, ?MODULE, StoreId, []).\n\ninit(StoreId) ->\n    %% Create ETS table for read model\n    ets:new(?TABLE, [named_table, public, {read_concurrency, true}]),\n\n    %% Subscribe to order events\n    {ok, SubKey} = reckon_db_subscriptions:subscribe(\n        StoreId,\n        event_pattern,\n        <<\"order-*\">>,\n        <<\"order_dashboard_projection\">>\n    ),\n\n    %% Join the emitter group to receive events\n    reckon_db_emitter_group:join(StoreId, SubKey, self()),\n\n    {ok, #{store_id => StoreId, sub_key => SubKey}}.\n\n%% Handle events from subscription\nhandle_info({event, Event}, State) ->\n    project_event(Event),\n    {noreply, State}.\n\n%% Query interface\nget_order(OrderId) ->\n    case ets:lookup(?TABLE, {order, OrderId}) of\n        [{_, Order}] -> {ok, Order};\n        [] -> {error, not_found}\n    end.\n\nlist_customer_orders(CustomerId) ->\n    Pattern = {{customer_order, CustomerId, '_'}, '_'},\n    Orders = ets:match_object(?TABLE, Pattern),\n    {ok, [Order || {_, Order} <- Orders]}.\n\n%% Project events into read model\nproject_event(#{event_type := <<\"OrderPlaced\">>} = Event) ->\n    Data = maps:get(data, Event),\n    OrderId = maps:get(order_id, Data),\n    CustomerId = maps:get(customer_id, Data),\n\n    %% Denormalized read model optimized for display\n    ReadModel = #{\n        order_id => OrderId,\n        customer_id => CustomerId,\n        items => maps:get(items, Data),\n        total => maps:get(total, Data),\n        status => <<\"Placed\">>,\n        status_history => [#{status => <<\"Placed\">>, at => Event#event.timestamp}],\n        placed_at => Event#event.timestamp\n    },\n\n    %% Store by order ID\n    ets:insert(?TABLE, {{order, OrderId}, ReadModel}),\n\n    %% Index by customer for listing\n    ets:insert(?TABLE, {{customer_order, CustomerId, OrderId}, ReadModel});\n\nproject_event(#{event_type := <<\"OrderShipped\">>} = Event) ->\n    Data = maps:get(data, Event),\n    OrderId = maps:get(order_id, Data),\n\n    %% Update existing read model\n    case ets:lookup(?TABLE, {order, OrderId}) of\n        [{Key, Order}] ->\n            Updated = Order#{\n                status => <<\"Shipped\">>,\n                status_history => [\n                    #{status => <<\"Shipped\">>, at => Event#event.timestamp}\n                    | maps:get(status_history, Order)\n                ],\n                tracking_number => maps:get(tracking_number, Data, undefined),\n                shipped_at => Event#event.timestamp\n            },\n            ets:insert(?TABLE, {Key, Updated}),\n\n            %% Update customer index too\n            CustomerId = maps:get(customer_id, Order),\n            ets:insert(?TABLE, {{customer_order, CustomerId, OrderId}, Updated});\n        [] ->\n            %% Event for unknown order - log warning\n            logger:warning(\"OrderShipped for unknown order: ~p\", [OrderId])\n    end;\n\nproject_event(_Event) ->\n    %% Ignore events we don't care about\n    ok.\n```","ref":"cqrs.html#the-query-side-projections"},{"type":"extras","title":"Multiple Projections - CQRS","doc":"The same events can drive multiple specialized read models:\n\n```erlang\n%% Analytics projection - aggregates for dashboards\n-module(order_analytics_projection).\n\nproject_event(#{event_type := <<\"OrderPlaced\">>} = Event) ->\n    Data = maps:get(data, Event),\n    Date = date_from_timestamp(Event#event.timestamp),\n    Total = maps:get(total, Data),\n    Region = get_customer_region(maps:get(customer_id, Data)),\n\n    %% Increment daily counters\n    ets:update_counter(?ANALYTICS_TABLE, {daily_orders, Date, Region}, 1, {{daily_orders, Date, Region}, 0}),\n    ets:update_counter(?ANALYTICS_TABLE, {daily_revenue, Date, Region}, Total, {{daily_revenue, Date, Region}, 0}).\n\n%% Inventory projection - tracks stock levels\n-module(inventory_projection).\n\nproject_event(#{event_type := <<\"OrderPlaced\">>} = Event) ->\n    Items = maps:get(items, maps:get(data, Event)),\n    lists:foreach(fun(#{product_id := ProductId, quantity := Qty}) ->\n        %% Decrement reserved stock\n        ets:update_counter(?INVENTORY_TABLE, {reserved, ProductId}, Qty, {{reserved, ProductId}, 0})\n    end, Items);\n\nproject_event(#{event_type := <<\"OrderShipped\">>} = Event) ->\n    Items = maps:get(items, maps:get(data, Event)),\n    lists:foreach(fun(#{product_id := ProductId, quantity := Qty}) ->\n        %% Move from reserved to shipped\n        ets:update_counter(?INVENTORY_TABLE, {reserved, ProductId}, -Qty),\n        ets:update_counter(?INVENTORY_TABLE, {shipped, ProductId}, Qty, {{shipped, ProductId}, 0})\n    end, Items).\n```","ref":"cqrs.html#multiple-projections"},{"type":"extras","title":"Eventual Consistency - CQRS","doc":"With CQRS, read models are **eventually consistent** with the write model. This means:\n\n1. A command succeeds and events are written\n2. Projections receive events asynchronously\n3. Read models are updated\n4. Queries return the updated data\n\nThere's a delay between steps 1 and 4. This is usually milliseconds, but can be longer under load.","ref":"cqrs.html#eventual-consistency"},{"type":"extras","title":"Handling Eventual Consistency - CQRS","doc":"**In the UI:**\n```erlang\n%% After successful command, show optimistic update\ncase order_commands:handle(PlaceOrderCmd, State) of\n    {ok, Events} ->\n        %% Write events\n        {ok, Version} = reckon_db_streams:append(Store, StreamId, ExpectedVer, Events),\n\n        %% Return success with the data the client needs\n        %% Don't query the read model yet - it might not be updated\n        {ok, #{\n            order_id => OrderId,\n            status => <<\"Placed\">>,\n            message => <<\"Order placed successfully\">>\n        }};\n    {error, Reason} ->\n        {error, Reason}\nend.\n```\n\n**For critical queries:**\n```erlang\n%% If consistency is critical, query the event store directly\nget_order_status(StoreId, OrderId) ->\n    StreamId = <<\"order-\", OrderId/binary>>,\n    {ok, Events} = reckon_db_streams:read(StoreId, StreamId, 0, 1000, forward),\n\n    %% Derive status from events\n    Status = lists:foldl(fun\n        (#{event_type := <<\"OrderPlaced\">>}, _) -> placed;\n        (#{event_type := <<\"OrderShipped\">>}, _) -> shipped;\n        (#{event_type := <<\"OrderDelivered\">>}, _) -> delivered;\n        (#{event_type := <<\"OrderCancelled\">>}, _) -> cancelled;\n        (_, Acc) -> Acc\n    end, unknown, Events),\n\n    {ok, Status}.\n```","ref":"cqrs.html#handling-eventual-consistency"},{"type":"extras","title":"Best Practices - CQRS","doc":"","ref":"cqrs.html#best-practices"},{"type":"extras","title":"1. Keep Projections Idempotent - CQRS","doc":"Projections may receive the same event multiple times (redelivery, replay). Make them idempotent:\n\n```erlang\n%% Bad: Not idempotent\nproject_event(#{event_type := <<\"ItemAdded\">>} = E) ->\n    OrderId = maps:get(order_id, maps:get(data, E)),\n    ets:update_counter(?TABLE, {item_count, OrderId}, 1).  %% Will double-count on replay\n\n%% Good: Idempotent using event version\nproject_event(#{event_type := <<\"ItemAdded\">>} = E) ->\n    OrderId = maps:get(order_id, maps:get(data, E)),\n    EventVersion = E#event.version,\n\n    case ets:lookup(?TABLE, {last_version, OrderId}) of\n        [{_, LastVersion}] when EventVersion = \n            %% Already processed this event\n            ok;\n        _ ->\n            %% Process and update version\n            ets:update_counter(?TABLE, {item_count, OrderId}, 1),\n            ets:insert(?TABLE, {{last_version, OrderId}, EventVersion})\n    end.\n```","ref":"cqrs.html#1-keep-projections-idempotent"},{"type":"extras","title":"2. Design Read Models for Queries - CQRS","doc":"Don't normalize read models. Denormalize for query performance:\n\n```erlang\n%% Read model for \"show customer's recent orders with item details\"\n%% Everything needed in one lookup\n#{\n    customer_id => <<\"cust-123\">>,\n    recent_orders => [\n        #{\n            order_id => <<\"ord-456\">>,\n            placed_at => 1703001234567,\n            status => <<\"Delivered\">>,\n            items => [\n                #{name => <<\"Widget\">>, quantity => 2, price => 999}\n            ],\n            total => 1998\n        }\n    ]\n}\n```","ref":"cqrs.html#2-design-read-models-for-queries"},{"type":"extras","title":"3. Separate Projection Processes - CQRS","doc":"Run projections in separate processes for isolation:\n\n```erlang\n%% In your supervisor\n{ok, _} = order_dashboard_projection:start_link(StoreId),\n{ok, _} = order_analytics_projection:start_link(StoreId),\n{ok, _} = inventory_projection:start_link(StoreId).\n```\n\nIf one projection fails or falls behind, others continue working.","ref":"cqrs.html#3-separate-projection-processes"},{"type":"extras","title":"Further Reading - CQRS","doc":"- [Event Sourcing Guide](event_sourcing.md) - Foundation for CQRS\n- [Subscriptions Guide](subscriptions.md) - Event delivery for projections\n- [Snapshots Guide](snapshots.md) - Optimizing projection rebuilds","ref":"cqrs.html#further-reading"},{"type":"extras","title":"References - CQRS","doc":"- Martin Fowler: [CQRS](https://martinfowler.com/bliki/CQRS.html)\n- Greg Young: [CQRS Documents](https://cqrs.files.wordpress.com/2010/11/cqrs_documents.pdf)\n- Udi Dahan: [Clarified CQRS](https://udidahan.com/2009/12/09/clarified-cqrs/)","ref":"cqrs.html#references"},{"type":"extras","title":"Subscriptions","doc":"# Subscriptions in reckon-db\n\nSubscriptions enable real-time event delivery to consumers. When events are written to streams, subscribers receive notifications automatically, enabling reactive architectures and event-driven systems.\n\n![Subscription Types](../assets/subscriptions.svg)","ref":"subscriptions.html"},{"type":"extras","title":"How Subscriptions Work - Subscriptions","doc":"reckon-db uses Khepri triggers for guaranteed event delivery:\n\n![Subscription Flow](../assets/subscription_flow.svg)\n\n**Key guarantee**: Triggers only fire AFTER events are committed via Raft consensus. Subscribers never receive events that don't exist.","ref":"subscriptions.html#how-subscriptions-work"},{"type":"extras","title":"Subscription Types - Subscriptions","doc":"reckon-db supports four subscription types for flexible event filtering:","ref":"subscriptions.html#subscription-types"},{"type":"extras","title":"1. Stream Subscription - Subscriptions","doc":"Subscribe to all events in a specific stream:\n\n```erlang\n%% Subscribe to a single order's events\n{ok, SubKey} = reckon_db_subscriptions:subscribe(\n    my_store,\n    stream,                          %% subscription type\n    <<\"order-123\">>,                 %% stream ID\n    <<\"order_123_handler\">>          %% subscription name\n).\n```\n\nUse cases:\n- Aggregate projections (one read model per aggregate)\n- Saga/process managers following a specific entity\n- Real-time UI updates for a specific resource","ref":"subscriptions.html#1-stream-subscription"},{"type":"extras","title":"2. Event Type Subscription - Subscriptions","doc":"Subscribe to events of a specific type across all streams:\n\n```erlang\n%% Subscribe to all PaymentReceived events\n{ok, SubKey} = reckon_db_subscriptions:subscribe(\n    my_store,\n    event_type,\n    <<\"PaymentReceived\">>,\n    <<\"payment_processor\">>\n).\n```\n\nUse cases:\n- Cross-cutting concerns (logging, auditing)\n- Metrics collection\n- Notification services","ref":"subscriptions.html#2-event-type-subscription"},{"type":"extras","title":"3. Event Pattern Subscription - Subscriptions","doc":"Subscribe to events matching a stream ID pattern with wildcards:\n\n```erlang\n%% Subscribe to all order streams\n{ok, SubKey} = reckon_db_subscriptions:subscribe(\n    my_store,\n    event_pattern,\n    <<\"order-*\">>,                   %% wildcard pattern\n    <<\"order_projection\">>\n).\n\n%% Subscribe to all streams in a region\n{ok, SubKey2} = reckon_db_subscriptions:subscribe(\n    my_store,\n    event_pattern,\n    <<\"*-region-eu\">>,\n    <<\"eu_analytics\">>\n).\n```\n\nUse cases:\n- Category projections (all orders, all users)\n- Regional processing\n- Multi-tenant partitioning","ref":"subscriptions.html#3-event-pattern-subscription"},{"type":"extras","title":"4. Payload Subscription - Subscriptions","doc":"Subscribe to events matching specific payload criteria:\n\n```erlang\n%% Subscribe to high-value orders\n{ok, SubKey} = reckon_db_subscriptions:subscribe(\n    my_store,\n    event_payload,\n    #{total => {gt, 10000}},         %% total > 10000\n    <<\"high_value_order_handler\">>\n).\n\n%% Subscribe to orders from VIP customers\n{ok, SubKey2} = reckon_db_subscriptions:subscribe(\n    my_store,\n    event_payload,\n    #{customer_type => <<\"VIP\">>},\n    <<\"vip_handler\">>\n).\n```\n\nUse cases:\n- Conditional processing\n- Fraud detection (high amounts)\n- Priority handling","ref":"subscriptions.html#4-payload-subscription"},{"type":"extras","title":"Creating Subscriptions - Subscriptions","doc":"","ref":"subscriptions.html#creating-subscriptions"},{"type":"extras","title":"Basic Subscription - Subscriptions","doc":"```erlang\n%% Create a subscription\n{ok, SubscriptionKey} = reckon_db_subscriptions:subscribe(\n    StoreId,\n    Type,\n    Selector,\n    SubscriptionName\n).\n\n%% The subscription key uniquely identifies this subscription\n%% Use it for management operations\n```","ref":"subscriptions.html#basic-subscription"},{"type":"extras","title":"Subscription Options - Subscriptions","doc":"```erlang\n%% Advanced options\n{ok, SubKey} = reckon_db_subscriptions:subscribe(\n    my_store,\n    event_pattern,\n    <<\"order-*\">>,\n    <<\"order_projection\">>,\n    #{\n        pool_size => 4,              %% Emitter pool size for parallelism\n        start_from => 0,             %% Start position (0 = beginning)\n        subscriber => self()         %% Direct subscriber PID\n    }\n).\n```\n\nOptions:\n- `pool_size`: Number of emitter workers for parallel delivery (default: 1)\n- `start_from`: Starting event position for catch-up (default: 0)\n- `subscriber`: PID to receive events directly","ref":"subscriptions.html#subscription-options"},{"type":"extras","title":"Receiving Events - Subscriptions","doc":"","ref":"subscriptions.html#receiving-events"},{"type":"extras","title":"Using Process Groups - Subscriptions","doc":"The recommended approach is joining the emitter group:\n\n```erlang\n-module(my_event_handler).\n-behaviour(gen_server).\n\ninit(StoreId) ->\n    %% Create subscription\n    {ok, SubKey} = reckon_db_subscriptions:subscribe(\n        StoreId,\n        event_pattern,\n        <<\"order-*\">>,\n        <<\"my_handler\">>\n    ),\n\n    %% Join the emitter group to receive events\n    reckon_db_emitter_group:join(StoreId, SubKey, self()),\n\n    {ok, #{store_id => StoreId, sub_key => SubKey}}.\n\nhandle_info({event, Event}, State) ->\n    %% Process the event\n    handle_event(Event),\n    {noreply, State}.\n\nhandle_event(#event{event_type = <<\"OrderPlaced\">>} = Event) ->\n    logger:info(\"Order placed: ~p\", [Event#event.data]),\n    update_projection(Event);\nhandle_event(#event{event_type = <<\"OrderShipped\">>} = Event) ->\n    logger:info(\"Order shipped: ~p\", [Event#event.data]),\n    update_projection(Event);\nhandle_event(_Event) ->\n    %% Ignore other events\n    ok.\n```","ref":"subscriptions.html#using-process-groups"},{"type":"extras","title":"Multiple Handlers - Subscriptions","doc":"For high throughput, use multiple handler processes:\n\n```erlang\n-module(order_handler_pool).\n\nstart_pool(StoreId, PoolSize) ->\n    %% Create subscription with pool size\n    {ok, SubKey} = reckon_db_subscriptions:subscribe(\n        StoreId,\n        event_pattern,\n        <<\"order-*\">>,\n        <<\"order_handler_pool\">>,\n        #{pool_size => PoolSize}\n    ),\n\n    %% Start worker processes\n    [begin\n        {ok, Pid} = order_handler_worker:start_link(StoreId, SubKey, N),\n        Pid\n    end || N <- lists:seq(1, PoolSize)].\n\n-module(order_handler_worker).\n\ninit({StoreId, SubKey, WorkerId}) ->\n    %% Join the same emitter group\n    %% Events are distributed round-robin among workers\n    reckon_db_emitter_group:join(StoreId, SubKey, self()),\n    {ok, #{worker_id => WorkerId}}.\n```","ref":"subscriptions.html#multiple-handlers"},{"type":"extras","title":"Managing Subscriptions - Subscriptions","doc":"","ref":"subscriptions.html#managing-subscriptions"},{"type":"extras","title":"List Subscriptions - Subscriptions","doc":"```erlang\n%% List all subscriptions\n{ok, Subscriptions} = reckon_db_subscriptions:list(my_store).\n\n%% Each subscription record contains:\n%% - type: stream | event_type | event_pattern | event_payload\n%% - selector: The filter criteria\n%% - subscription_name: Human-readable name\n%% - created_at: Timestamp\n%% - pool_size: Number of emitters\n```","ref":"subscriptions.html#list-subscriptions"},{"type":"extras","title":"Check Subscription Exists - Subscriptions","doc":"```erlang\n%% Check if subscription exists\ncase reckon_db_subscriptions:exists(my_store, SubscriptionKey) of\n    true -> io:format(\"Subscription is active~n\");\n    false -> io:format(\"Subscription not found~n\")\nend.\n```","ref":"subscriptions.html#check-subscription-exists"},{"type":"extras","title":"Get Subscription Details - Subscriptions","doc":"```erlang\n%% Get subscription by key\ncase reckon_db_subscriptions:get(my_store, SubscriptionKey) of\n    {ok, Subscription} ->\n        io:format(\"Type: ~p~n\", [Subscription#subscription.type]),\n        io:format(\"Selector: ~p~n\", [Subscription#subscription.selector]);\n    {error, not_found} ->\n        io:format(\"Subscription not found~n\")\nend.\n```","ref":"subscriptions.html#get-subscription-details"},{"type":"extras","title":"Unsubscribe - Subscriptions","doc":"```erlang\n%% Unsubscribe by key\nok = reckon_db_subscriptions:unsubscribe(my_store, SubscriptionKey).\n\n%% Unsubscribe by type and name\nok = reckon_db_subscriptions:unsubscribe(my_store, event_pattern, <<\"order_projection\">>).\n```","ref":"subscriptions.html#unsubscribe"},{"type":"extras","title":"Catch-Up Subscriptions - Subscriptions","doc":"Catch-up subscriptions process historical events before receiving live events:\n\n```erlang\n%% Start from the beginning (catch up on all history)\n{ok, SubKey} = reckon_db_subscriptions:subscribe(\n    my_store,\n    event_pattern,\n    <<\"order-*\">>,\n    <<\"new_projection\">>,\n    #{start_from => 0}                %% Start from first event\n).\n\n%% Start from a specific position (e.g., after rebuilding)\n{ok, SubKey2} = reckon_db_subscriptions:subscribe(\n    my_store,\n    event_pattern,\n    <<\"order-*\">>,\n    <<\"resumed_projection\">>,\n    #{start_from => 12345}            %% Resume from position 12345\n).\n```","ref":"subscriptions.html#catch-up-subscriptions"},{"type":"extras","title":"Checkpointing - Subscriptions","doc":"Track your position to resume after restarts:\n\n```erlang\n-module(checkpointed_handler).\n\ninit(StoreId) ->\n    %% Load last processed position\n    LastPosition = load_checkpoint(StoreId),\n\n    {ok, SubKey} = reckon_db_subscriptions:subscribe(\n        StoreId,\n        event_pattern,\n        <<\"order-*\">>,\n        <<\"checkpointed_handler\">>,\n        #{start_from => LastPosition}\n    ),\n\n    reckon_db_emitter_group:join(StoreId, SubKey, self()),\n    {ok, #{store_id => StoreId, sub_key => SubKey}}.\n\nhandle_info({event, Event}, #{store_id := StoreId} = State) ->\n    %% Process the event\n    handle_event(Event),\n\n    %% Save checkpoint\n    save_checkpoint(StoreId, Event#event.version),\n\n    {noreply, State}.\n\n%% Checkpoints can be stored in:\n%% - The event store itself (as a special stream)\n%% - ETS/DETS\n%% - External database\n```","ref":"subscriptions.html#checkpointing"},{"type":"extras","title":"Best Practices - Subscriptions","doc":"","ref":"subscriptions.html#best-practices"},{"type":"extras","title":"1. Idempotent Event Handling - Subscriptions","doc":"Events may be delivered more than once. Make handlers idempotent:\n\n```erlang\nhandle_event(Event) ->\n    EventId = Event#event.event_id,\n\n    %% Check if already processed\n    case ets:lookup(processed_events, EventId) of\n        [{EventId, _}] ->\n            %% Already processed, skip\n            ok;\n        [] ->\n            %% Process and mark as done\n            do_process_event(Event),\n            ets:insert(processed_events, {EventId, erlang:system_time()})\n    end.\n```","ref":"subscriptions.html#1-idempotent-event-handling"},{"type":"extras","title":"2. Handle Ordering Carefully - Subscriptions","doc":"Within a single stream, events are ordered. Across streams, ordering is not guaranteed:\n\n```erlang\n%% Events from stream \"order-123\" arrive in order:\n%% OrderPlaced -> ItemAdded -> OrderShipped\n\n%% But events from different streams may interleave:\n%% order-123:OrderPlaced\n%% order-456:OrderPlaced     %% Different stream, no ordering guarantee\n%% order-123:ItemAdded\n%% order-456:PaymentReceived\n```","ref":"subscriptions.html#2-handle-ordering-carefully"},{"type":"extras","title":"3. Graceful Shutdown - Subscriptions","doc":"Leave emitter groups on shutdown:\n\n```erlang\nterminate(_Reason, #{store_id := StoreId, sub_key := SubKey}) ->\n    %% Leave the emitter group\n    reckon_db_emitter_group:leave(StoreId, SubKey, self()),\n    ok.\n```","ref":"subscriptions.html#3-graceful-shutdown"},{"type":"extras","title":"4. Monitor Subscription Lag - Subscriptions","doc":"Track how far behind your subscription is:\n\n```erlang\n%% Check current stream version vs processed version\nStreamVersion = reckon_db_streams:get_version(StoreId, StreamId),\nProcessedVersion = get_last_processed_version(),\nLag = StreamVersion - ProcessedVersion,\n\ncase Lag > 1000 of\n    true ->\n        logger:warning(\"Subscription lag is high: ~p events behind\", [Lag]);\n    false ->\n        ok\nend.\n```","ref":"subscriptions.html#4-monitor-subscription-lag"},{"type":"extras","title":"Further Reading - Subscriptions","doc":"- [Event Sourcing Guide](event_sourcing.md) - Foundation concepts\n- [CQRS Guide](cqrs.md) - Using subscriptions for projections\n- [Snapshots Guide](snapshots.md) - Optimizing catch-up subscriptions","ref":"subscriptions.html#further-reading"},{"type":"extras","title":"References - Subscriptions","doc":"- Event Store: [Persistent Subscriptions](https://developers.eventstore.com/clients/grpc/persistent-subscriptions/)\n- Axon Framework: [Event Processors](https://docs.axoniq.io/reference-guide/axon-framework/events/event-processors)","ref":"subscriptions.html#references"},{"type":"extras","title":"Snapshots","doc":"# Snapshots in reckon-db\n\nSnapshots are periodic captures of aggregate state that optimize event replay performance. Instead of replaying thousands of events, you load the latest snapshot and replay only subsequent events.","ref":"snapshots.html"},{"type":"extras","title":"Why Snapshots? - Snapshots","doc":"In event sourcing, current state is derived by replaying all events. Snapshots dramatically improve recovery performance:\n\n![Snapshot Recovery Performance](../assets/snapshot_recovery.svg)","ref":"snapshots.html#why-snapshots"},{"type":"extras","title":"When to Use Snapshots - Snapshots","doc":"| Scenario | Use Snapshots? |\n|----------|----------------|\n| Aggregates with < 100 events | Probably not needed |\n| Aggregates with 100-1000 events | Consider it |\n| Aggregates with > 1000 events | Recommended |\n| Frequent aggregate loading | Recommended |\n| Long-lived aggregates | Recommended |\n| Read-heavy workloads | Recommended |","ref":"snapshots.html#when-to-use-snapshots"},{"type":"extras","title":"When NOT to Use Snapshots - Snapshots","doc":"- Small aggregates (few events)\n- Write-heavy, read-light workloads\n- When event replay is fast enough\n- Development/testing (adds complexity)","ref":"snapshots.html#when-not-to-use-snapshots"},{"type":"extras","title":"Snapshot API - Snapshots","doc":"","ref":"snapshots.html#snapshot-api"},{"type":"extras","title":"Saving Snapshots - Snapshots","doc":"```erlang\n%% Save a snapshot of aggregate state\nok = reckon_db_snapshots:save(\n    my_store,                    %% Store ID\n    <<\"account-123\">>,           %% Stream ID\n    150,                         %% Version (event number this snapshot is at)\n    AccountState,                %% The state to snapshot (any term)\n    #{                           %% Metadata\n        aggregate_type => account,\n        created_by => self()\n    }\n).\n```\n\nThe snapshot stores:\n- **Stream ID**: Which stream this snapshot belongs to\n- **Version**: The event version this snapshot represents\n- **Data**: The serialized aggregate state\n- **Metadata**: Additional information (optional)\n- **Timestamp**: When the snapshot was created","ref":"snapshots.html#saving-snapshots"},{"type":"extras","title":"Loading Snapshots - Snapshots","doc":"```erlang\n%% Load the latest snapshot\ncase reckon_db_snapshots:load(my_store, <<\"account-123\">>) of\n    {ok, Snapshot} ->\n        Version = Snapshot#snapshot.version,\n        State = Snapshot#snapshot.data,\n        %% Replay events after the snapshot\n        replay_from(State, Version + 1);\n    {error, not_found} ->\n        %% No snapshot, replay from beginning\n        replay_from(initial_state(), 0)\nend.\n\n%% Load snapshot at a specific version\n{ok, Snapshot} = reckon_db_snapshots:load_at(my_store, <<\"account-123\">>, 100).\n```","ref":"snapshots.html#loading-snapshots"},{"type":"extras","title":"Listing Snapshots - Snapshots","doc":"```erlang\n%% List all snapshots for a stream\n{ok, Snapshots} = reckon_db_snapshots:list(my_store, <<\"account-123\">>).\n\n%% Returns list of snapshots sorted by version (newest first)\n[\n    #snapshot{stream_id = <<\"account-123\">>, version = 150, ...},\n    #snapshot{stream_id = <<\"account-123\">>, version = 100, ...},\n    #snapshot{stream_id = <<\"account-123\">>, version = 50, ...}\n]\n```","ref":"snapshots.html#listing-snapshots"},{"type":"extras","title":"Deleting Snapshots - Snapshots","doc":"```erlang\n%% Delete a specific snapshot\nok = reckon_db_snapshots:delete_at(my_store, <<\"account-123\">>, 50).\n\n%% Delete old snapshots (keep only recent ones)\n{ok, Snapshots} = reckon_db_snapshots:list(my_store, <<\"account-123\">>),\nOldSnapshots = lists:nthtail(3, Snapshots),  %% Keep 3 most recent\n[reckon_db_snapshots:delete_at(my_store, S#snapshot.stream_id, S#snapshot.version)\n || S <- OldSnapshots].\n```","ref":"snapshots.html#deleting-snapshots"},{"type":"extras","title":"Using Snapshots with Aggregates - Snapshots","doc":"","ref":"snapshots.html#using-snapshots-with-aggregates"},{"type":"extras","title":"Complete Aggregate Pattern - Snapshots","doc":"```erlang\n-module(account_aggregate).\n-export([load/2, execute/3, save_snapshot_if_needed/3]).\n\n-record(account, {\n    id,\n    balance = 0,\n    status = active,\n    events_since_snapshot = 0\n}).\n\n-define(SNAPSHOT_THRESHOLD, 100).  %% Snapshot every 100 events\n\n%% Load aggregate from store\nload(StoreId, AccountId) ->\n    StreamId = <<\"account-\", AccountId/binary>>,\n\n    %% Try to load from snapshot first\n    {InitialState, StartVersion} = case reckon_db_snapshots:load(StoreId, StreamId) of\n        {ok, Snapshot} ->\n            {Snapshot#snapshot.data, Snapshot#snapshot.version + 1};\n        {error, not_found} ->\n            {#account{id = AccountId}, 0}\n    end,\n\n    %% Replay events after snapshot\n    case reckon_db_streams:read(StoreId, StreamId, StartVersion, 10000, forward) of\n        {ok, Events} ->\n            FinalState = lists:foldl(fun apply_event/2, InitialState, Events),\n            EventCount = length(Events),\n            {ok, FinalState#account{events_since_snapshot = EventCount}};\n        {error, {stream_not_found, _}} when StartVersion =:= 0 ->\n            %% New aggregate\n            {ok, InitialState};\n        {error, Reason} ->\n            {error, Reason}\n    end.\n\n%% Execute command and persist events\nexecute(StoreId, AccountId, Command) ->\n    StreamId = <<\"account-\", AccountId/binary>>,\n\n    %% Load current state\n    {ok, Account} = load(StoreId, AccountId),\n\n    %% Execute command\n    case handle_command(Command, Account) of\n        {ok, Events} ->\n            %% Get current version\n            CurrentVersion = reckon_db_streams:get_version(StoreId, StreamId),\n\n            %% Append events\n            {ok, NewVersion} = reckon_db_streams:append(\n                StoreId,\n                StreamId,\n                CurrentVersion,\n                Events\n            ),\n\n            %% Apply events to get new state\n            NewState = lists:foldl(fun apply_event/2, Account, Events),\n\n            %% Maybe save snapshot\n            EventsSinceSnapshot = Account#account.events_since_snapshot + length(Events),\n            save_snapshot_if_needed(StoreId, StreamId, NewState, NewVersion, EventsSinceSnapshot),\n\n            {ok, NewVersion, NewState};\n        {error, Reason} ->\n            {error, Reason}\n    end.\n\n%% Save snapshot if threshold reached\nsave_snapshot_if_needed(StoreId, StreamId, State, Version, EventsSinceSnapshot)\n  when EventsSinceSnapshot >= ?SNAPSHOT_THRESHOLD ->\n    %% Save snapshot\n    ok = reckon_db_snapshots:save(StoreId, StreamId, Version, State, #{}),\n    logger:info(\"Saved snapshot for ~s at version ~p\", [StreamId, Version]);\nsave_snapshot_if_needed(_StoreId, _StreamId, _State, _Version, _EventsSinceSnapshot) ->\n    %% Not enough events, skip snapshot\n    ok.\n\n%% Command handlers\nhandle_command({deposit, Amount}, #account{status = active} = Account) when Amount > 0 ->\n    {ok, [#{\n        event_type => <<\"MoneyDeposited\">>,\n        data => #{amount => Amount, balance_after => Account#account.balance + Amount}\n    }]};\nhandle_command({deposit, _Amount}, #account{status = frozen}) ->\n    {error, account_frozen};\n\nhandle_command({withdraw, Amount}, #account{status = active, balance = Balance})\n  when Amount > 0, Amount = \n    {ok, [#{\n        event_type => <<\"MoneyWithdrawn\">>,\n        data => #{amount => Amount, balance_after => Balance - Amount}\n    }]};\nhandle_command({withdraw, Amount}, #account{balance = Balance}) when Amount > Balance ->\n    {error, insufficient_funds}.\n\n%% Event application\napply_event(#{event_type := <<\"MoneyDeposited\">>} = Event, Account) ->\n    Amount = maps:get(amount, maps:get(data, Event)),\n    Account#account{balance = Account#account.balance + Amount};\napply_event(#{event_type := <<\"MoneyWithdrawn\">>} = Event, Account) ->\n    Amount = maps:get(amount, maps:get(data, Event)),\n    Account#account{balance = Account#account.balance - Amount};\napply_event(#{event_type := <<\"AccountFrozen\">>}, Account) ->\n    Account#account{status = frozen}.\n```","ref":"snapshots.html#complete-aggregate-pattern"},{"type":"extras","title":"Using reckon_db_aggregator with Snapshots - Snapshots","doc":"The `reckon_db_aggregator` module provides the `aggregate/3` function for snapshot-based aggregation:\n\n```erlang\n%% Load snapshot and replay events manually\nStreamId = <<\"account-123\">>,\n\n%% Try to load snapshot\n{Snapshot, StartVersion} = case reckon_db_snapshots:load(my_store, StreamId) of\n    {ok, S} -> {S, S#snapshot.version + 1};\n    {error, not_found} -> {undefined, 0}\nend,\n\n%% Read events after snapshot\n{ok, Events} = reckon_db_streams:read(my_store, StreamId, StartVersion, 10000, forward),\n\n%% Aggregate with snapshot support\nFinalState = reckon_db_aggregator:aggregate(Events, Snapshot, #{\n    initial_state => #account{id = <<\"123\">>}\n}).\n```","ref":"snapshots.html#using-reckon_db_aggregator-with-snapshots"},{"type":"extras","title":"Snapshot Strategies - Snapshots","doc":"","ref":"snapshots.html#snapshot-strategies"},{"type":"extras","title":"Time-Based Snapshotting - Snapshots","doc":"Save snapshots at regular time intervals:\n\n```erlang\n-module(snapshot_scheduler).\n-behaviour(gen_server).\n\n-define(SNAPSHOT_INTERVAL_MS, 60000).  %% Every minute\n\ninit(StoreId) ->\n    timer:send_interval(?SNAPSHOT_INTERVAL_MS, snapshot_check),\n    {ok, #{store_id => StoreId}}.\n\nhandle_info(snapshot_check, #{store_id := StoreId} = State) ->\n    %% Get active aggregates that need snapshots\n    ActiveAggregates = get_active_aggregates(),\n\n    lists:foreach(fun({StreamId, CurrentState, Version}) ->\n        %% Check if snapshot is stale\n        case should_snapshot(StoreId, StreamId, Version) of\n            true ->\n                reckon_db_snapshots:save(StoreId, StreamId, Version, CurrentState, #{});\n            false ->\n                ok\n        end\n    end, ActiveAggregates),\n\n    {noreply, State}.\n\nshould_snapshot(StoreId, StreamId, CurrentVersion) ->\n    case reckon_db_snapshots:load(StoreId, StreamId) of\n        {ok, Snapshot} ->\n            %% Snapshot if > 100 events since last snapshot\n            CurrentVersion - Snapshot#snapshot.version > 100;\n        {error, not_found} ->\n            %% No snapshot, create one if > 50 events\n            CurrentVersion > 50\n    end.\n```","ref":"snapshots.html#time-based-snapshotting"},{"type":"extras","title":"Event-Count-Based Snapshotting - Snapshots","doc":"Snapshot after N events (shown in aggregate example above):\n\n```erlang\n-define(SNAPSHOT_EVERY_N_EVENTS, 100).\n\nmaybe_snapshot(StoreId, StreamId, State, Version, EventsSinceSnapshot) ->\n    case EventsSinceSnapshot >= ?SNAPSHOT_EVERY_N_EVENTS of\n        true ->\n            reckon_db_snapshots:save(StoreId, StreamId, Version, State, #{}),\n            0;  %% Reset counter\n        false ->\n            EventsSinceSnapshot\n    end.\n```","ref":"snapshots.html#event-count-based-snapshotting"},{"type":"extras","title":"On-Demand Snapshotting - Snapshots","doc":"Snapshot on specific conditions:\n\n```erlang\n%% Snapshot after significant state changes\nhandle_event(#{event_type := <<\"LargeTransactionProcessed\">>} = Event, State) ->\n    NewState = apply_event(Event, State),\n    %% Force snapshot after large transactions\n    reckon_db_snapshots:save(StoreId, StreamId, Version, NewState, #{\n        reason => large_transaction,\n        amount => maps:get(amount, maps:get(data, Event))\n    }),\n    NewState.\n```","ref":"snapshots.html#on-demand-snapshotting"},{"type":"extras","title":"Snapshot Storage Considerations - Snapshots","doc":"","ref":"snapshots.html#snapshot-storage-considerations"},{"type":"extras","title":"What to Store - Snapshots","doc":"Store only the essential state:\n\n```erlang\n%% Good: Minimal, reconstructable state\nsave_snapshot(State) ->\n    #{\n        balance => State#account.balance,\n        status => State#account.status,\n        last_transaction_id => State#account.last_transaction_id\n    }.\n\n%% Bad: Storing derived/cacheable data\nsave_snapshot(State) ->\n    #{\n        balance => State#account.balance,\n        status => State#account.status,\n        transaction_history => State#account.history,  %% Can be replayed\n        monthly_totals => State#account.monthly_totals  %% Derived data\n    }.\n```","ref":"snapshots.html#what-to-store"},{"type":"extras","title":"Snapshot Versioning - Snapshots","doc":"Handle schema changes in snapshots:\n\n```erlang\n%% Version your snapshot schema\nsave_snapshot(State) ->\n    #{\n        schema_version => 2,\n        data => #{\n            balance => State#account.balance,\n            status => State#account.status,\n            currency => State#account.currency  %% New in v2\n        }\n    }.\n\n%% Handle old snapshot formats\nload_snapshot(#{schema_version := 1, data := Data}) ->\n    %% Migrate v1 to current format\n    #account{\n        balance = maps:get(balance, Data),\n        status = maps:get(status, Data),\n        currency = <<\"USD\">>  %% Default for v1 snapshots\n    };\nload_snapshot(#{schema_version := 2, data := Data}) ->\n    %% Current version\n    #account{\n        balance = maps:get(balance, Data),\n        status = maps:get(status, Data),\n        currency = maps:get(currency, Data)\n    }.\n```","ref":"snapshots.html#snapshot-versioning"},{"type":"extras","title":"Best Practices - Snapshots","doc":"","ref":"snapshots.html#best-practices"},{"type":"extras","title":"1. Keep Snapshots Small - Snapshots","doc":"Snapshots should contain minimal state:\n\n```erlang\n%% Calculate snapshot size\nSnapshotData = State#account{\n    %% Exclude non-essential fields\n    audit_log => [],           %% Clear logs\n    cached_calculations => #{} %% Clear caches\n},\nSize = byte_size(term_to_binary(SnapshotData)),\ncase Size > 1000000 of  %% > 1MB\n    true -> logger:warning(\"Large snapshot: ~p bytes\", [Size]);\n    false -> ok\nend.\n```","ref":"snapshots.html#1-keep-snapshots-small"},{"type":"extras","title":"2. Cleanup Old Snapshots - Snapshots","doc":"Don't keep unlimited snapshots:\n\n```erlang\n%% Keep only the N most recent snapshots\ncleanup_old_snapshots(StoreId, StreamId, KeepCount) ->\n    {ok, Snapshots} = reckon_db_snapshots:list(StoreId, StreamId),\n    ToDelete = lists:nthtail(KeepCount, Snapshots),\n    [reckon_db_snapshots:delete(StoreId, S#snapshot.stream_id, S#snapshot.version)\n     || S <- ToDelete].\n```","ref":"snapshots.html#2-cleanup-old-snapshots"},{"type":"extras","title":"3. Test Snapshot/Replay Consistency - Snapshots","doc":"Verify snapshots produce correct state:\n\n```erlang\n%% Property test: snapshot + replay = full replay\nprop_snapshot_consistency() ->\n    ?FORALL(Events, non_empty(list(event())),\n        begin\n            StreamId = unique_stream_id(),\n\n            %% Write events\n            {ok, FinalVersion} = write_events(StoreId, StreamId, Events),\n\n            %% Full replay\n            {ok, AllEvents} = reckon_db_streams:read(StoreId, StreamId, 0, 10000, forward),\n            FullReplayState = lists:foldl(fun apply_event/2, initial_state(), AllEvents),\n\n            %% Save snapshot midway\n            MidVersion = FinalVersion div 2,\n            {ok, MidEvents} = reckon_db_streams:read(StoreId, StreamId, 0, MidVersion, forward),\n            MidState = lists:foldl(fun apply_event/2, initial_state(), MidEvents),\n            reckon_db_snapshots:save(StoreId, StreamId, MidVersion, MidState, #{}),\n\n            %% Snapshot + replay\n            {ok, Snapshot} = reckon_db_snapshots:load(StoreId, StreamId),\n            {ok, RemainingEvents} = reckon_db_streams:read(StoreId, StreamId, MidVersion + 1, 10000, forward),\n            SnapshotReplayState = lists:foldl(fun apply_event/2, Snapshot#snapshot.data, RemainingEvents),\n\n            %% States must match\n            FullReplayState =:= SnapshotReplayState\n        end).\n```","ref":"snapshots.html#3-test-snapshot-replay-consistency"},{"type":"extras","title":"4. Monitor Snapshot Performance - Snapshots","doc":"Track snapshot metrics:\n\n```erlang\nsave_snapshot_with_metrics(StoreId, StreamId, Version, State) ->\n    StartTime = erlang:monotonic_time(microsecond),\n\n    ok = reckon_db_snapshots:save(StoreId, StreamId, Version, State, #{}),\n\n    Duration = erlang:monotonic_time(microsecond) - StartTime,\n    Size = byte_size(term_to_binary(State)),\n\n    telemetry:execute(\n        [reckon_db, snapshot, created],\n        #{duration => Duration, size_bytes => Size},\n        #{store_id => StoreId, stream_id => StreamId, version => Version}\n    ).\n```","ref":"snapshots.html#4-monitor-snapshot-performance"},{"type":"extras","title":"Further Reading - Snapshots","doc":"- [Event Sourcing Guide](event_sourcing.md) - Foundation concepts\n- [CQRS Guide](cqrs.md) - Read model projections\n- [Subscriptions Guide](subscriptions.md) - Event delivery","ref":"snapshots.html#further-reading"},{"type":"extras","title":"References - Snapshots","doc":"- Vaughn Vernon: \"Implementing Domain-Driven Design\" (Chapter 10: Aggregates)\n- Greg Young: [Snapshot Strategies](https://cqrs.files.wordpress.com/2010/11/cqrs_documents.pdf)\n- Event Store: [Projections and Snapshots](https://developers.eventstore.com/)","ref":"snapshots.html#references"},{"type":"extras","title":"Temporal Queries","doc":"# Temporal Queries\n\nTemporal queries enable point-in-time reconstruction of aggregate state and time-range analytics. This guide covers the server-side implementation, storage mechanics, and cluster behavior.","ref":"temporal_queries.html"},{"type":"extras","title":"Overview - Temporal Queries","doc":"The `reckon_db_temporal` module provides three core operations:\n\n| Function | Purpose |\n|----------|---------|\n| `read_until/3,4` | Read events up to a timestamp |\n| `read_range/4,5` | Read events within a time range |\n| `version_at/3` | Get stream version at a timestamp |","ref":"temporal_queries.html#overview"},{"type":"extras","title":"Architecture - Temporal Queries","doc":"![Temporal Query Flow](../assets/temporal_query_flow.svg)","ref":"temporal_queries.html#architecture"},{"type":"extras","title":"How Temporal Filtering Works - Temporal Queries","doc":"","ref":"temporal_queries.html#how-temporal-filtering-works"},{"type":"extras","title":"Timestamp Storage - Temporal Queries","doc":"Every event stored in reckon-db includes an `epoch_us` field - microseconds since Unix epoch:\n\n```erlang\n#event{\n    event_id = <<\"evt-123\">>,\n    stream_id = <<\"orders-456\">>,\n    version = 5,\n    epoch_us = 1735689600000000,  %% Jan 1, 2025 00:00:00 UTC\n    event_type = <<\"OrderPlaced\">>,\n    data = #{...},\n    metadata = #{...}\n}\n```\n\nThis timestamp is set at append time using `erlang:system_time(microsecond)`.","ref":"temporal_queries.html#timestamp-storage"},{"type":"extras","title":"Query Execution - Temporal Queries","doc":"Temporal queries follow this execution path:\n\n1. **Stream Existence Check**: Verify stream exists in Khepri\n2. **Event Retrieval**: Read all events from the stream\n3. **Timestamp Filtering**: Filter events by `epoch_us` field\n4. **Options Application**: Apply direction and limit options\n\n```erlang\n%% Internal filtering for read_until\nfilter_events_until(Events, Timestamp) ->\n    [E || E <- Events, E#event.epoch_us = \n    [E || E <- Events,\n          E#event.epoch_us >= FromTimestamp,\n          E#event.epoch_us =< ToTimestamp].\n```","ref":"temporal_queries.html#query-execution"},{"type":"extras","title":"Khepri Storage Path - Temporal Queries","doc":"Events are stored at:\n\n```\n[streams, StreamId, PaddedVersion] -> #event{}\n```\n\nThe 12-character zero-padded version enables lexicographic ordering:\n\n```\n[streams, <<\"orders-123\">>, <<\"000000000000\">>] -> Event v0\n[streams, <<\"orders-123\">>, <<\"000000000001\">>] -> Event v1\n[streams, <<\"orders-123\">>, <<\"000000000002\">>] -> Event v2\n```","ref":"temporal_queries.html#khepri-storage-path"},{"type":"extras","title":"Cluster Behavior - Temporal Queries","doc":"","ref":"temporal_queries.html#cluster-behavior"},{"type":"extras","title":"Consistency Guarantees - Temporal Queries","doc":"Temporal queries inherit Khepri/Ra's consistency model:\n\n| Aspect | Behavior |\n|--------|----------|\n| **Read Consistency** | Strongly consistent (reads go through Raft leader) |\n| **Cross-Node** | Same results on any cluster node |\n| **Partition Tolerance** | Queries fail if no quorum available |","ref":"temporal_queries.html#consistency-guarantees"},{"type":"extras","title":"Leader Routing - Temporal Queries","doc":"All reads are routed to the Ra leader:\n\n```\nClient Request\n     \n     \n\n Any Node    \n (Follower)  \n\n        Forward to leader\n       \n\n Ra Leader   \n (Reads)     \n\n        Raft log consistency\n       \n  Query Result\n```","ref":"temporal_queries.html#leader-routing"},{"type":"extras","title":"Performance Considerations - Temporal Queries","doc":"","ref":"temporal_queries.html#performance-considerations"},{"type":"extras","title":"Current Implementation - Temporal Queries","doc":"The current implementation reads all events, then filters in memory:\n\n```erlang\nread_all_events(StoreId, StreamId) ->\n    Version = reckon_db_streams:get_version(StoreId, StreamId),\n    case Version of\n        ?NO_STREAM -> {ok, []};\n        _ -> reckon_db_streams:read(StoreId, StreamId, 0, Version + 1, forward)\n    end.\n```\n\n**Implications:**\n- O(n) memory usage where n = total events in stream\n- Suitable for streams with < 10,000 events\n- For larger streams, consider snapshots + temporal queries","ref":"temporal_queries.html#current-implementation"},{"type":"extras","title":"Optimization Opportunities - Temporal Queries","doc":"Future versions could add:\n\n1. **Khepri Timestamp Index**: Secondary index on `epoch_us`\n2. **Binary Search**: If events are sorted by timestamp (they are by version, usually correlated)\n3. **Streaming API**: Process events in batches to reduce memory","ref":"temporal_queries.html#optimization-opportunities"},{"type":"extras","title":"Use Cases - Temporal Queries","doc":"","ref":"temporal_queries.html#use-cases"},{"type":"extras","title":"1. Point-in-Time State Reconstruction - Temporal Queries","doc":"```erlang\n%% Reconstruct order state as it was on Dec 31, 2024\nTimestamp = 1735603200000000,  %% Dec 31, 2024 00:00:00 UTC\n{ok, Events} = reckon_db_temporal:read_until(my_store, <<\"order-123\">>, Timestamp),\nState = lists:foldl(fun apply_event/2, initial_state(), Events).\n```","ref":"temporal_queries.html#1-point-in-time-state-reconstruction"},{"type":"extras","title":"2. Audit Queries - Temporal Queries","doc":"```erlang\n%% What was the account balance at end of fiscal year?\nFiscalYearEnd = 1735689599999999,  %% Dec 31, 2024 23:59:59.999999 UTC\n{ok, Events} = reckon_db_temporal:read_until(my_store, <<\"account-456\">>, FiscalYearEnd),\nBalance = calculate_balance(Events).\n```","ref":"temporal_queries.html#2-audit-queries"},{"type":"extras","title":"3. Time-Range Analytics - Temporal Queries","doc":"```erlang\n%% Analyze orders from Q4 2024\nQ4Start = 1727740800000000,  %% Oct 1, 2024\nQ4End = 1735689599999999,    %% Dec 31, 2024\n{ok, Events} = reckon_db_temporal:read_range(my_store, <<\"orders-*\">>, Q4Start, Q4End),\nanalyze_quarterly_orders(Events).\n```","ref":"temporal_queries.html#3-time-range-analytics"},{"type":"extras","title":"4. Version Discovery - Temporal Queries","doc":"```erlang\n%% What version should I replay to for a snapshot at timestamp T?\n{ok, Version} = reckon_db_temporal:version_at(my_store, <<\"user-789\">>, Timestamp),\n%% Now load snapshot at Version, or replay events 0..Version\n```","ref":"temporal_queries.html#4-version-discovery"},{"type":"extras","title":"Options - Temporal Queries","doc":"Both `read_until/4` and `read_range/5` accept options:\n\n```erlang\n-type opts() :: #{\n    direction => forward | backward,  %% Event order (default: forward)\n    limit => pos_integer()             %% Max events to return\n}.\n```\n\n**Direction**: Controls the order of returned events\n- `forward`: Oldest first (ascending by version)\n- `backward`: Newest first (descending by version)\n\n**Limit**: Truncates result after applying direction","ref":"temporal_queries.html#options"},{"type":"extras","title":"Telemetry - Temporal Queries","doc":"Temporal queries emit telemetry events:\n\n```erlang\n%% Event: [reckon_db, temporal, read_until]\n%% Measurements: #{duration => integer(), event_count => integer()}\n%% Metadata: #{store_id => atom(), stream_id => binary(), timestamp => integer()}\n\n%% Event: [reckon_db, temporal, read_range]\n%% Measurements: #{duration => integer(), event_count => integer()}\n%% Metadata: #{store_id => atom(), stream_id => binary(), timestamp => {From, To}}\n```","ref":"temporal_queries.html#telemetry"},{"type":"extras","title":"Error Handling - Temporal Queries","doc":"| Error | Cause | Resolution |\n|-------|-------|------------|\n| `{error, {stream_not_found, StreamId}}` | Stream does not exist | Verify stream ID |\n| `{error, timeout}` | Khepri/Ra timeout | Check cluster health |\n| `{error, no_quorum}` | Ra cluster partitioned | Wait for partition heal |","ref":"temporal_queries.html#error-handling"},{"type":"extras","title":"Best Practices - Temporal Queries","doc":"1. **Use Snapshots**: For frequently queried historical points, save snapshots\n2. **Limit Result Size**: Use the `limit` option for large time ranges\n3. **Monitor Duration**: Track telemetry for slow queries\n4. **Consider Stream Size**: Temporal queries load full streams; partition large streams","ref":"temporal_queries.html#best-practices"},{"type":"extras","title":"See Also - Temporal Queries","doc":"- [Snapshots](snapshots.md) - State caching for performance\n- [Scavenging](scavenging.md) - Event lifecycle management\n- [Storage Internals](storage_internals.md) - Khepri path structure","ref":"temporal_queries.html#see-also"},{"type":"extras","title":"Scavenging","doc":"# Scavenging and Event Lifecycle\n\nScavenging is the process of removing old events from streams to reduce storage costs while maintaining stream integrity. This guide covers the server-side implementation, safety guarantees, and archival strategies.","ref":"scavenging.html"},{"type":"extras","title":"Overview - Scavenging","doc":"The `reckon_db_scavenge` module provides:\n\n| Function | Purpose |\n|----------|---------|\n| `scavenge/3` | Remove old events from a single stream |\n| `scavenge_matching/3` | Scavenge streams matching a pattern |\n| `archive_and_scavenge/4` | Archive events before deletion |\n| `dry_run/3` | Preview what would be deleted |","ref":"scavenging.html#overview"},{"type":"extras","title":"Architecture - Scavenging","doc":"![Scavenge Lifecycle](../assets/scavenge_lifecycle.svg)","ref":"scavenging.html#architecture"},{"type":"extras","title":"Event Lifecycle - Scavenging","doc":"Events in reckon-db follow this lifecycle:\n\n```\n               \n  Active     Archived    Scavenged   Deleted  \n (Khepri)       (Backend)      (Marked)       (Gone)   \n               \n                                                     \n       Hot storage     Cold storage    Reference       Permanent\n       Fast reads      Slow reads      only            removal\n```","ref":"scavenging.html#event-lifecycle"},{"type":"extras","title":"Safety Guarantees - Scavenging","doc":"","ref":"scavenging.html#safety-guarantees"},{"type":"extras","title":"Snapshot Requirement - Scavenging","doc":"By default, scavenging requires a snapshot to exist:\n\n```erlang\n%% This will fail if no snapshot exists\n{error, {no_snapshot, <<\"orders-123\">>}} =\n    reckon_db_scavenge:scavenge(my_store, <<\"orders-123\">>, #{\n        before_version => 100\n    }).\n\n%% Override the safety check (use with caution)\n{ok, _} = reckon_db_scavenge:scavenge(my_store, <<\"orders-123\">>, #{\n    before_version => 100,\n    require_snapshot => false\n}).\n```\n\n**Why this matters**: Without a snapshot, replaying to a historical state requires all events from version 0. Scavenging old events without a snapshot breaks replay capability.","ref":"scavenging.html#snapshot-requirement"},{"type":"extras","title":"Keep Versions - Scavenging","doc":"Always keep a minimum number of recent versions:\n\n```erlang\n%% Keep at least the last 10 versions, regardless of timestamp\n{ok, Result} = reckon_db_scavenge:scavenge(my_store, <<\"orders-123\">>, #{\n    before => OneYearAgo,\n    keep_versions => 10\n}).\n```","ref":"scavenging.html#keep-versions"},{"type":"extras","title":"Dry Run - Scavenging","doc":"Preview what would be deleted before actually deleting:\n\n```erlang\n{ok, Preview} = reckon_db_scavenge:dry_run(my_store, <<\"orders-123\">>, #{\n    before => RetentionCutoff\n}),\n%% Preview contains:\n%% #{\n%%     stream_id => <<\"orders-123\">>,\n%%     deleted_count => 500,\n%%     deleted_versions => {0, 499},\n%%     archived => false,\n%%     dry_run => true\n%% }\n```","ref":"scavenging.html#dry-run"},{"type":"extras","title":"Scavenge Options - Scavenging","doc":"```erlang\n-type scavenge_opts() :: #{\n    before => integer(),           %% Delete events before this timestamp (epoch_us)\n    before_version => integer(),   %% Delete events before this version\n    keep_versions => pos_integer(),%% Keep at least N latest versions\n    require_snapshot => boolean(), %% Require snapshot exists (default: true)\n    dry_run => boolean()           %% Preview only (default: false)\n}.\n```","ref":"scavenging.html#scavenge-options"},{"type":"extras","title":"Timestamp-Based Scavenging - Scavenging","doc":"```erlang\n%% Delete events older than 1 year\nOneYearAgo = erlang:system_time(microsecond) - (365 * 24 * 60 * 60 * 1000000),\n{ok, Result} = reckon_db_scavenge:scavenge(my_store, <<\"orders-123\">>, #{\n    before => OneYearAgo\n}).\n```","ref":"scavenging.html#timestamp-based-scavenging"},{"type":"extras","title":"Version-Based Scavenging - Scavenging","doc":"```erlang\n%% Delete events before version 1000\n{ok, Result} = reckon_db_scavenge:scavenge(my_store, <<\"orders-123\">>, #{\n    before_version => 1000\n}).\n```","ref":"scavenging.html#version-based-scavenging"},{"type":"extras","title":"Khepri Storage Operations - Scavenging","doc":"","ref":"scavenging.html#khepri-storage-operations"},{"type":"extras","title":"How Events Are Deleted - Scavenging","doc":"Events are deleted individually from Khepri:\n\n```erlang\ndelete_event_versions(StoreId, StreamId, FromVersion, ToVersion) ->\n    lists:foreach(\n        fun(Version) ->\n            PaddedVersion = pad_version(Version, ?VERSION_PADDING),\n            Path = ?STREAMS_PATH ++ [StreamId, PaddedVersion],\n            khepri:delete(StoreId, Path)\n        end,\n        lists:seq(FromVersion, ToVersion)\n    ).\n```\n\n**Storage path structure:**\n```\n[streams, <<\"orders-123\">>, <<\"000000000000\">>] -> Deleted\n[streams, <<\"orders-123\">>, <<\"000000000001\">>] -> Deleted\n...\n[streams, <<\"orders-123\">>, <<\"000000000499\">>] -> Deleted\n[streams, <<\"orders-123\">>, <<\"000000000500\">>] -> Kept (after cutoff)\n```","ref":"scavenging.html#how-events-are-deleted"},{"type":"extras","title":"Cluster Behavior - Scavenging","doc":"Deletions are replicated through Ra consensus:\n\n1. Delete request received on any node\n2. Request forwarded to Ra leader\n3. Leader appends delete to Raft log\n4. Followers replicate and apply\n5. Quorum achieved, deletion confirmed\n\n**Important**: Deletions are permanent once committed to the Raft log.","ref":"scavenging.html#cluster-behavior"},{"type":"extras","title":"Archival - Scavenging","doc":"","ref":"scavenging.html#archival"},{"type":"extras","title":"Archive Before Delete - Scavenging","doc":"Use `archive_and_scavenge/4` to preserve events before removal:\n\n```erlang\n%% Initialize file backend\n{ok, BackendState} = reckon_db_archive_file:init(#{\n    base_path => \"/bulk0/archives/reckon_db\"\n}),\n\n%% Archive then scavenge\n{ok, Result} = reckon_db_scavenge:archive_and_scavenge(\n    my_store,\n    <<\"orders-123\">>,\n    {reckon_db_archive_file, BackendState},\n    #{before => RetentionCutoff}\n),\n\n%% Result includes archive key\n%% #{\n%%     stream_id => <<\"orders-123\">>,\n%%     deleted_count => 500,\n%%     deleted_versions => {0, 499},\n%%     archived => true,\n%%     archive_key => <<\"my_store/orders-123/0-499.archive\">>\n%% }\n```","ref":"scavenging.html#archive-before-delete"},{"type":"extras","title":"Archive Key Format - Scavenging","doc":"Archive keys follow a standard format:\n\n```\n{StoreId}/{StreamId}/{FromVersion}-{ToVersion}.archive\n\nExamples:\nmy_store/orders-123/0-999.archive\nmy_store/users-456/1000-1999.archive\n```","ref":"scavenging.html#archive-key-format"},{"type":"extras","title":"Custom Archive Backends - Scavenging","doc":"Implement the `reckon_db_archive_backend` behaviour:\n\n```erlang\n-callback init(Opts :: map()) ->\n    {ok, State :: term()} | {error, Reason :: term()}.\n\n-callback archive(State, ArchiveKey, Events) ->\n    {ok, NewState} | {error, Reason}.\n\n-callback read(State, ArchiveKey) ->\n    {ok, Events, NewState} | {error, Reason}.\n\n-callback list(State, StoreId, StreamId) ->\n    {ok, [ArchiveKey], NewState} | {error, Reason}.\n\n-callback delete(State, ArchiveKey) ->\n    {ok, NewState} | {error, Reason}.\n\n-callback exists(State, ArchiveKey) ->\n    {boolean(), NewState}.\n```\n\nBuilt-in backends:\n- `reckon_db_archive_file` - Local file system storage","ref":"scavenging.html#custom-archive-backends"},{"type":"extras","title":"Pattern Matching - Scavenging","doc":"Scavenge multiple streams at once:\n\n```erlang\n%% Scavenge all order streams\n{ok, Results} = reckon_db_scavenge:scavenge_matching(my_store, <<\"orders-*\">>, #{\n    before => RetentionCutoff,\n    keep_versions => 10\n}).\n\n%% Results is a list of scavenge_result() for each matching stream\n```\n\nSupported patterns:\n- `orders-*` - Prefix match\n- `*-completed` - Suffix match\n- `orders-*-v2` - Multiple wildcards","ref":"scavenging.html#pattern-matching"},{"type":"extras","title":"Telemetry - Scavenging","doc":"Scavenge operations emit telemetry:\n\n```erlang\n%% Event: [reckon_db, scavenge, complete]\n%% Measurements:\n%%   #{duration => integer(), deleted_count => integer()}\n%% Metadata:\n%%   #{store_id => atom(), stream_id => binary(), archived => boolean()}\n```","ref":"scavenging.html#telemetry"},{"type":"extras","title":"Best Practices - Scavenging","doc":"","ref":"scavenging.html#best-practices"},{"type":"extras","title":"1. Always Preview First - Scavenging","doc":"```erlang\n%% Preview\n{ok, Preview} = reckon_db_scavenge:dry_run(Store, Stream, Opts),\nio:format(\"Would delete ~p events~n\", [maps:get(deleted_count, Preview)]),\n\n%% Then execute\n{ok, Result} = reckon_db_scavenge:scavenge(Store, Stream, Opts).\n```","ref":"scavenging.html#1-always-preview-first"},{"type":"extras","title":"2. Snapshot Before Scavenging - Scavenging","doc":"```erlang\n%% Save current state as snapshot\n{ok, Events} = reckon_db_streams:read(Store, Stream, 0, Version, forward),\nState = rebuild_state(Events),\nok = reckon_db_snapshots:save(Store, Stream, Version, State, #{}),\n\n%% Now safe to scavenge\n{ok, _} = reckon_db_scavenge:scavenge(Store, Stream, #{\n    before_version => Version\n}).\n```","ref":"scavenging.html#2-snapshot-before-scavenging"},{"type":"extras","title":"3. Archive for Compliance - Scavenging","doc":"For audit requirements, always archive before scavenging:\n\n```erlang\n%% 7-year retention in cold storage\n{ok, _} = reckon_db_scavenge:archive_and_scavenge(\n    Store, Stream,\n    {reckon_db_archive_s3, S3State},  %% Custom S3 backend\n    #{before => SevenYearsAgo}\n).\n```","ref":"scavenging.html#3-archive-for-compliance"},{"type":"extras","title":"4. Schedule Off-Peak - Scavenging","doc":"Run scavenging during low-traffic periods:\n\n```erlang\n%% Example: Run at 3 AM daily\nschedule_scavenge() ->\n    timer:apply_after(\n        time_until_3am(),\n        fun() ->\n            scavenge_old_streams(),\n            schedule_scavenge()  %% Reschedule\n        end\n    ).\n```","ref":"scavenging.html#4-schedule-off-peak"},{"type":"extras","title":"Error Handling - Scavenging","doc":"| Error | Cause | Resolution |\n|-------|-------|------------|\n| `{error, {no_snapshot, StreamId}}` | Snapshot required but missing | Save snapshot first |\n| `{error, {stream_not_found, StreamId}}` | Stream does not exist | Verify stream ID |\n| `{error, archive_failed}` | Archive backend error | Check backend logs |","ref":"scavenging.html#error-handling"},{"type":"extras","title":"See Also - Scavenging","doc":"- [Temporal Queries](temporal_queries.md) - Time-based event retrieval\n- [Snapshots](snapshots.md) - State caching\n- [Storage Internals](storage_internals.md) - Khepri path structure","ref":"scavenging.html#see-also"},{"type":"extras","title":"Causation Tracking","doc":"# Causation and Correlation Tracking\n\nCausation tracking enables tracing event lineage through distributed systems. This guide covers how to track, query, and visualize event relationships.","ref":"causation.html"},{"type":"extras","title":"Overview - Causation Tracking","doc":"The `reckon_db_causation` module provides:\n\n| Function | Purpose |\n|----------|---------|\n| `get_effects/2` | Find events caused by an event |\n| `get_cause/2` | Find the event that caused this one |\n| `get_chain/2` | Trace causation chain to root |\n| `get_correlated/2` | Find all events in a saga |\n| `build_graph/2` | Build visualization graph |\n| `to_dot/1` | Export as Graphviz DOT |","ref":"causation.html#overview"},{"type":"extras","title":"Architecture - Causation Tracking","doc":"![Causation Graph](../assets/causation_graph.svg)","ref":"causation.html#architecture"},{"type":"extras","title":"Metadata Convention - Causation Tracking","doc":"Events track lineage through standard metadata fields:\n\n```erlang\n#event{\n    event_id = <<\"evt-456\">>,\n    event_type = <<\"OrderShipped\">>,\n    data = #{...},\n    metadata = #{\n        causation_id => <<\"evt-123\">>,     %% Direct cause\n        correlation_id => <<\"saga-789\">>,  %% Business process\n        actor_id => <<\"user-001\">>         %% Who triggered this\n    }\n}\n```","ref":"causation.html#metadata-convention"},{"type":"extras","title":"Field Definitions - Causation Tracking","doc":"| Field | Type | Purpose |\n|-------|------|---------|\n| `causation_id` | binary | Event ID that directly caused this event |\n| `correlation_id` | binary | Groups events in a saga/process |\n| `actor_id` | binary | Identity that triggered the event |","ref":"causation.html#field-definitions"},{"type":"extras","title":"Causation Queries - Causation Tracking","doc":"","ref":"causation.html#causation-queries"},{"type":"extras","title":"Finding Effects - Causation Tracking","doc":"Get all events caused by a specific event:\n\n```erlang\n{ok, Effects} = reckon_db_causation:get_effects(my_store, <<\"evt-123\">>),\n%% Returns all events where metadata.causation_id = \"evt-123\"\n```","ref":"causation.html#finding-effects"},{"type":"extras","title":"Finding Cause - Causation Tracking","doc":"Get the event that caused this one:\n\n```erlang\n{ok, CauseEvent} = reckon_db_causation:get_cause(my_store, <<\"evt-456\">>),\n%% Looks up event with ID matching this event's causation_id\n```","ref":"causation.html#finding-cause"},{"type":"extras","title":"Tracing the Chain - Causation Tracking","doc":"Walk backward from an event to its root cause:\n\n```erlang\n{ok, Chain} = reckon_db_causation:get_chain(my_store, <<\"evt-789\">>),\n%% Returns [RootEvent, ..., IntermediateEvents, ..., TargetEvent]\n%% Ordered from root to target\n```","ref":"causation.html#tracing-the-chain"},{"type":"extras","title":"Finding Correlated Events - Causation Tracking","doc":"Get all events in a saga or business process:\n\n```erlang\n{ok, SagaEvents} = reckon_db_causation:get_correlated(my_store, <<\"saga-001\">>),\n%% Returns all events where metadata.correlation_id = \"saga-001\"\n%% Sorted by epoch_us (temporal order)\n```","ref":"causation.html#finding-correlated-events"},{"type":"extras","title":"Graph Building - Causation Tracking","doc":"","ref":"causation.html#graph-building"},{"type":"extras","title":"Build Causation Graph - Causation Tracking","doc":"```erlang\n%% Build from a specific event\n{ok, Graph} = reckon_db_causation:build_graph(my_store, <<\"evt-123\">>),\n\n%% Or build from a correlation ID\n{ok, Graph} = reckon_db_causation:build_graph(my_store, <<\"saga-001\">>),\n\n%% Graph structure:\n%% #{\n%%     nodes => [Event1, Event2, ...],\n%%     edges => [{CauseId, EffectId}, ...],\n%%     root => RootEventId | undefined\n%% }\n```","ref":"causation.html#build-causation-graph"},{"type":"extras","title":"Export to Graphviz - Causation Tracking","doc":"Generate DOT format for visualization:\n\n```erlang\n{ok, Graph} = reckon_db_causation:build_graph(my_store, <<\"saga-001\">>),\nDotBinary = reckon_db_causation:to_dot(Graph),\n\n%% Write to file\nfile:write_file(\"causation.dot\", DotBinary),\n\n%% Render with graphviz:\n%% dot -Tpng -o causation.png causation.dot\n%% dot -Tsvg -o causation.svg causation.dot\n```","ref":"causation.html#export-to-graphviz"},{"type":"extras","title":"How It Works - Causation Tracking","doc":"","ref":"causation.html#how-it-works"},{"type":"extras","title":"Storage (No Index) - Causation Tracking","doc":"Currently, causation queries scan all streams:\n\n```erlang\nscan_for_metadata(StoreId, Field, Value) ->\n    {ok, StreamIds} = reckon_db_streams:list_streams(StoreId),\n    Events = lists:foldl(\n        fun(StreamId, Acc) ->\n            MatchingEvents = scan_stream_for_metadata(StoreId, StreamId, Field, Value),\n            Acc ++ MatchingEvents\n        end,\n        [],\n        StreamIds\n    ),\n    {ok, lists:sort(fun(A, B) -> A#event.epoch_us =< B#event.epoch_us end, Events)}.\n```\n\n**Performance Note**: This is O(n) where n = total events across all streams. For production systems with many events, consider adding secondary indexes.","ref":"causation.html#storage-no-index"},{"type":"extras","title":"Chain Building - Causation Tracking","doc":"Chains are built by walking backward through causation links:\n\n```erlang\nbuild_chain_backward(StoreId, Event, Acc) ->\n    case maps:get(causation_id, Event#event.metadata, undefined) of\n        undefined ->\n            Acc;  %% Reached root\n        CausationId ->\n            case find_event_by_id(StoreId, CausationId) of\n                {ok, CauseEvent} ->\n                    build_chain_backward(StoreId, CauseEvent, [CauseEvent | Acc]);\n                {error, _} ->\n                    Acc  %% Chain broken (event deleted?)\n            end\n    end.\n```","ref":"causation.html#chain-building"},{"type":"extras","title":"Use Cases - Causation Tracking","doc":"","ref":"causation.html#use-cases"},{"type":"extras","title":"1. Debugging Distributed Flows - Causation Tracking","doc":"```erlang\n%% An order failed - trace back to root cause\n{ok, Chain} = reckon_db_causation:get_chain(my_store, <<\"order-failed-evt\">>),\nlists:foreach(\n    fun(Event) ->\n        io:format(\"~s: ~s (~s)~n\", [\n            Event#event.epoch_us,\n            Event#event.event_type,\n            Event#event.stream_id\n        ])\n    end,\n    Chain\n).\n```","ref":"causation.html#1-debugging-distributed-flows"},{"type":"extras","title":"2. Saga Visualization - Causation Tracking","doc":"```erlang\n%% Visualize a checkout saga\n{ok, Graph} = reckon_db_causation:build_graph(my_store, <<\"checkout-saga-123\">>),\nDot = reckon_db_causation:to_dot(Graph),\nfile:write_file(\"/tmp/checkout.dot\", Dot),\nos:cmd(\"dot -Tsvg -o /tmp/checkout.svg /tmp/checkout.dot\").\n```","ref":"causation.html#2-saga-visualization"},{"type":"extras","title":"3. Audit Trail - Causation Tracking","doc":"```erlang\n%% Who/what triggered this sensitive operation?\n{ok, Event} = find_event(my_store, <<\"sensitive-action-evt\">>),\n{ok, Chain} = reckon_db_causation:get_chain(my_store, Event#event.event_id),\nRootEvent = hd(Chain),\nActorId = maps:get(actor_id, RootEvent#event.metadata, <<\"unknown\">>),\nio:format(\"Sensitive action originated from actor: ~s~n\", [ActorId]).\n```","ref":"causation.html#3-audit-trail"},{"type":"extras","title":"4. Impact Analysis - Causation Tracking","doc":"```erlang\n%% What happened because of this event?\n{ok, Effects} = reckon_db_causation:get_effects(my_store, <<\"payment-received-evt\">>),\nio:format(\"Payment triggered ~p downstream events~n\", [length(Effects)]).\n```","ref":"causation.html#4-impact-analysis"},{"type":"extras","title":"Setting Causation IDs - Causation Tracking","doc":"When appending events, include causation metadata:\n\n```erlang\n%% In a command handler\nhandle_command(Command, State, CausationContext) ->\n    Event = #{\n        event_type => <<\"OrderPlaced\">>,\n        data => #{order_id => Command#cmd.order_id, items => Command#cmd.items},\n        metadata => #{\n            causation_id => CausationContext#ctx.event_id,\n            correlation_id => CausationContext#ctx.correlation_id,\n            actor_id => CausationContext#ctx.user_id\n        }\n    },\n    {ok, _} = reckon_db_streams:append(Store, Stream, ExpectedVersion, [Event]).\n```","ref":"causation.html#setting-causation-ids"},{"type":"extras","title":"Telemetry - Causation Tracking","doc":"Causation queries emit telemetry:\n\n```erlang\n%% Event: [reckon_db, causation, query]\n%% Measurements:\n%%   #{duration => integer(), event_count => integer()}\n%% Metadata:\n%%   #{store_id => atom(), id => binary(), query_type => atom()}\n\n%% Query types: causation_effects, causation_cause, causation_chain, causation_correlated\n```","ref":"causation.html#telemetry"},{"type":"extras","title":"DOT Format Example - Causation Tracking","doc":"Generated DOT output:\n\n```dot\ndigraph causation {\n  \"evt-001\" [label=\"evt-001\\nCommandReceived\"];\n  \"evt-002\" [label=\"evt-002\\nOrderCreated\"];\n  \"evt-003\" [label=\"evt-003\\nPaymentRequested\"];\n  \"evt-004\" [label=\"evt-004\\nPaymentReceived\"];\n  \"evt-005\" [label=\"evt-005\\nOrderShipped\"];\n  \"evt-001\" -> \"evt-002\";\n  \"evt-002\" -> \"evt-003\";\n  \"evt-003\" -> \"evt-004\";\n  \"evt-004\" -> \"evt-005\";\n}\n```\n\nRendered as:\n\n```\nCommandReceived  OrderCreated  PaymentRequested  PaymentReceived  OrderShipped\n```","ref":"causation.html#dot-format-example"},{"type":"extras","title":"Error Handling - Causation Tracking","doc":"| Error | Cause | Resolution |\n|-------|-------|------------|\n| `{error, not_found}` | Event ID does not exist | Verify event ID |\n| `{error, no_cause}` | Event has no causation_id | This is a root event |","ref":"causation.html#error-handling"},{"type":"extras","title":"Best Practices - Causation Tracking","doc":"","ref":"causation.html#best-practices"},{"type":"extras","title":"1. Always Set Correlation ID - Causation Tracking","doc":"```erlang\n%% Generate at saga/process start, propagate through all events\nCorrelationId = uuid:uuid_to_string(uuid:get_v4()),\n%% Use this same ID for all events in the business process\n```","ref":"causation.html#1-always-set-correlation-id"},{"type":"extras","title":"2. Preserve Causation Context - Causation Tracking","doc":"```erlang\n%% When handling an event that triggers new events\nhandle_event(SourceEvent, State) ->\n    NewEventMetadata = #{\n        causation_id => SourceEvent#event.event_id,\n        correlation_id => maps:get(correlation_id, SourceEvent#event.metadata)\n    },\n    ...\n```","ref":"causation.html#2-preserve-causation-context"},{"type":"extras","title":"3. Include Actor ID at Entry Points - Causation Tracking","doc":"```erlang\n%% At API/command entry points\nMetadata = #{\n    correlation_id => new_correlation_id(),\n    actor_id => RequestContext#ctx.authenticated_user\n}.\n```","ref":"causation.html#3-include-actor-id-at-entry-points"},{"type":"extras","title":"Future Enhancements - Causation Tracking","doc":"Potential improvements for production scale:\n\n1. **Secondary Index**: Add Khepri index on `causation_id` and `correlation_id`\n2. **Materialized Graph**: Pre-compute causation graph per correlation\n3. **Streaming Query**: Process events in batches to reduce memory\n4. **Time-Bounded Search**: Limit search to recent time window","ref":"causation.html#future-enhancements"},{"type":"extras","title":"See Also - Causation Tracking","doc":"- [Stream Links](stream_links.md) - Derived streams from causation patterns\n- [Temporal Queries](temporal_queries.md) - Time-based event retrieval\n- [Storage Internals](storage_internals.md) - Khepri path structure","ref":"causation.html#see-also"},{"type":"extras","title":"Stream Links","doc":"# Stream Links and Projections\n\nStream links enable derived streams from source streams through filtering and transformation. This guide covers the server-side implementation, lifecycle management, and use cases.","ref":"stream_links.html"},{"type":"extras","title":"Overview - Stream Links","doc":"The `reckon_db_links` module provides:\n\n| Function | Purpose |\n|----------|---------|\n| `create/2` | Create a new link definition |\n| `delete/2` | Remove a link |\n| `get/2` | Get link configuration |\n| `list/1` | List all links |\n| `start/2` | Start link processing |\n| `stop/2` | Stop link processing |\n| `info/2` | Get detailed link statistics |","ref":"stream_links.html#overview"},{"type":"extras","title":"Architecture - Stream Links","doc":"![Stream Links Architecture](../assets/stream_links.svg)","ref":"stream_links.html#architecture"},{"type":"extras","title":"What is a Stream Link? - Stream Links","doc":"A stream link is a derived stream that:\n\n1. **Subscribes** to one or more source streams\n2. **Filters** events based on a predicate function\n3. **Transforms** events (optional) before writing\n4. **Writes** matching events to a link stream\n\nLink streams are named with a `$link:` prefix and behave like regular streams.","ref":"stream_links.html#what-is-a-stream-link"},{"type":"extras","title":"Creating Links - Stream Links","doc":"","ref":"stream_links.html#creating-links"},{"type":"extras","title":"Basic Link - Stream Links","doc":"```erlang\nok = reckon_db_links:create(my_store, #{\n    name => <<\"high-value-orders\">>,\n    source => #{type => stream_pattern, pattern => <<\"orders-*\">>},\n    filter => fun(E) -> maps:get(total, E#event.data, 0) > 1000 end\n}).\n```","ref":"stream_links.html#basic-link"},{"type":"extras","title":"Link with Transform - Stream Links","doc":"```erlang\nok = reckon_db_links:create(my_store, #{\n    name => <<\"flagged-orders\">>,\n    source => #{type => stream_pattern, pattern => <<\"orders-*\">>},\n    filter => fun(E) -> maps:get(total, E#event.data, 0) > 5000 end,\n    transform => fun(E) ->\n        NewData = maps:put(flagged, true, E#event.data),\n        NewData2 = maps:put(flagged_at, erlang:system_time(millisecond), NewData),\n        E#event{data = NewData2}\n    end\n}).\n```","ref":"stream_links.html#link-with-transform"},{"type":"extras","title":"Link with Backfill - Stream Links","doc":"Process existing events when starting:\n\n```erlang\nok = reckon_db_links:create(my_store, #{\n    name => <<\"all-payments\">>,\n    source => #{type => stream_pattern, pattern => <<\"payments-*\">>},\n    backfill => true  %% Process existing events on start\n}).\n```","ref":"stream_links.html#link-with-backfill"},{"type":"extras","title":"Source Specifications - Stream Links","doc":"","ref":"stream_links.html#source-specifications"},{"type":"extras","title":"Single Stream - Stream Links","doc":"```erlang\nsource => #{type => stream, stream_id => <<\"orders-123\">>}\n```","ref":"stream_links.html#single-stream"},{"type":"extras","title":"Stream Pattern (Wildcard) - Stream Links","doc":"```erlang\nsource => #{type => stream_pattern, pattern => <<\"orders-*\">>}\nsource => #{type => stream_pattern, pattern => <<\"*-completed\">>}\n```","ref":"stream_links.html#stream-pattern-wildcard"},{"type":"extras","title":"All Streams - Stream Links","doc":"```erlang\nsource => #{type => all}\n%% Excludes $link: streams to prevent loops\n```","ref":"stream_links.html#all-streams"},{"type":"extras","title":"Link Lifecycle - Stream Links","doc":"","ref":"stream_links.html#link-lifecycle"},{"type":"extras","title":"Starting a Link - Stream Links","doc":"```erlang\nok = reckon_db_links:start(my_store, <<\"high-value-orders\">>).\n```\n\nThis:\n1. Updates link status to `running`\n2. Performs backfill if `backfill => true`\n3. Subscribes to source stream(s) for new events","ref":"stream_links.html#starting-a-link"},{"type":"extras","title":"Stopping a Link - Stream Links","doc":"```erlang\nok = reckon_db_links:stop(my_store, <<\"high-value-orders\">>).\n```\n\nThis:\n1. Unsubscribes from source streams\n2. Updates link status to `stopped`","ref":"stream_links.html#stopping-a-link"},{"type":"extras","title":"Checking Link Status - Stream Links","doc":"```erlang\n{ok, Info} = reckon_db_links:info(my_store, <<\"high-value-orders\">>),\n%% #{\n%%     name => <<\"high-value-orders\">>,\n%%     source => #{type => stream_pattern, pattern => <<\"orders-*\">>},\n%%     status => running,\n%%     processed => 1523,\n%%     link_stream => <<\"$link:high-value-orders\">>,\n%%     link_stream_version => 156,\n%%     created_at => 1735689600000,\n%%     last_event => <<\"evt-789\">>\n%% }\n```","ref":"stream_links.html#checking-link-status"},{"type":"extras","title":"Khepri Storage - Stream Links","doc":"","ref":"stream_links.html#khepri-storage"},{"type":"extras","title":"Link Definitions - Stream Links","doc":"Links are stored at:\n\n```\n[links, StoreId, LinkName] -> #link{} record\n```","ref":"stream_links.html#link-definitions"},{"type":"extras","title":"Link Streams - Stream Links","doc":"Link output is written to:\n\n```\n[streams, <<\"$link:LinkName\">>, PaddedVersion] -> #event{}\n```","ref":"stream_links.html#link-streams"},{"type":"extras","title":"Link Record Structure - Stream Links","doc":"```erlang\n-record(link, {\n    name :: binary(),\n    source :: source_spec(),\n    filter :: fun((event()) -> boolean()) | undefined,\n    transform :: fun((event()) -> event()) | undefined,\n    backfill :: boolean(),\n    created_at :: integer(),\n    status = stopped :: running | stopped | error,\n    processed = 0 :: non_neg_integer(),\n    last_event :: binary() | undefined\n}).\n```","ref":"stream_links.html#link-record-structure"},{"type":"extras","title":"Subscribing to Link Streams - Stream Links","doc":"Link streams are regular streams:\n\n```erlang\n%% Subscribe to the link stream\nreckon_db_subscriptions:subscribe(\n    my_store,\n    stream,\n    <<\"$link:high-value-orders\">>,\n    <<\"my-subscription\">>,\n    #{handler => fun handle_high_value_order/1}\n).\n\n%% Read from the link stream\n{ok, Events} = reckon_db_streams:read(\n    my_store,\n    <<\"$link:high-value-orders\">>,\n    0, 100, forward\n).\n```","ref":"stream_links.html#subscribing-to-link-streams"},{"type":"extras","title":"Event Transformation - Stream Links","doc":"","ref":"stream_links.html#event-transformation"},{"type":"extras","title":"Preserved Metadata - Stream Links","doc":"When events are written to link streams, source information is preserved:\n\n```erlang\nevent_to_map(Event) ->\n    #{\n        event_id => Event#event.event_id,\n        event_type => Event#event.event_type,\n        data => Event#event.data,\n        metadata => maps:merge(Event#event.metadata, #{\n            source_stream => Event#event.stream_id,\n            source_version => Event#event.version\n        })\n    }.\n```","ref":"stream_links.html#preserved-metadata"},{"type":"extras","title":"Custom Transforms - Stream Links","doc":"Transform functions receive the full event record:\n\n```erlang\ntransform => fun(Event) ->\n    %% Add computed field\n    Data = Event#event.data,\n    Total = maps:get(quantity, Data, 0) * maps:get(price, Data, 0),\n    NewData = maps:put(computed_total, Total, Data),\n\n    %% Return modified event\n    Event#event{data = NewData}\nend\n```","ref":"stream_links.html#custom-transforms"},{"type":"extras","title":"Pattern Matching - Stream Links","doc":"Wildcard patterns are converted to regex:\n\n```erlang\nwildcard_to_regex(Pattern) ->\n    Escaped = re:replace(Pattern, <<\"[.^$+?{}\\\\[\\\\]\\\\\\\\|()]\">>,\n                         <<\"\\\\\\\\&\">>, [global, {return, binary}]),\n    Converted = binary:replace(Escaped, <<\"*\">>, <<\".*\">>, [global]),\n    <<\"^\", Converted/binary, \"$\">>.\n```\n\nExamples:\n- `orders-*` matches `orders-123`, `orders-456`\n- `*-completed` matches `order-completed`, `payment-completed`\n- `user-*-events` matches `user-123-events`","ref":"stream_links.html#pattern-matching"},{"type":"extras","title":"Use Cases - Stream Links","doc":"","ref":"stream_links.html#use-cases"},{"type":"extras","title":"1. Event Type Aggregation - Stream Links","doc":"Collect all payment events across customer streams:\n\n```erlang\nok = reckon_db_links:create(my_store, #{\n    name => <<\"all-payments\">>,\n    source => #{type => all},\n    filter => fun(E) ->\n        EventType = E#event.event_type,\n        EventType =:= <<\"PaymentReceived\">> orelse\n        EventType =:= <<\"PaymentFailed\">> orelse\n        EventType =:= <<\"RefundIssued\">>\n    end,\n    backfill => true\n}).\n```","ref":"stream_links.html#1-event-type-aggregation"},{"type":"extras","title":"2. High-Value Transaction Monitoring - Stream Links","doc":"```erlang\nok = reckon_db_links:create(my_store, #{\n    name => <<\"high-value-txns\">>,\n    source => #{type => stream_pattern, pattern => <<\"account-*\">>},\n    filter => fun(E) ->\n        Amount = abs(maps:get(amount, E#event.data, 0)),\n        Amount > 10000\n    end,\n    transform => fun(E) ->\n        E#event{metadata = maps:put(flagged_reason, <<\"high_value\">>,\n                                     E#event.metadata)}\n    end\n}).\n```","ref":"stream_links.html#2-high-value-transaction-monitoring"},{"type":"extras","title":"3. Audit Trail - Stream Links","doc":"```erlang\nok = reckon_db_links:create(my_store, #{\n    name => <<\"security-audit\">>,\n    source => #{type => all},\n    filter => fun(E) ->\n        EventType = E#event.event_type,\n        lists:member(EventType, [\n            <<\"UserLoggedIn\">>,\n            <<\"PasswordChanged\">>,\n            <<\"PermissionGranted\">>,\n            <<\"DataExported\">>\n        ])\n    end\n}).\n```","ref":"stream_links.html#3-audit-trail"},{"type":"extras","title":"4. Real-Time Analytics Feed - Stream Links","doc":"```erlang\nok = reckon_db_links:create(my_store, #{\n    name => <<\"analytics-feed\">>,\n    source => #{type => all},\n    filter => fun(_) -> true end,  %% All events\n    transform => fun(E) ->\n        %% Strip PII, keep only analytics-relevant fields\n        SafeData = maps:with([event_type, timestamp, stream_id], #{\n            event_type => E#event.event_type,\n            timestamp => E#event.epoch_us,\n            stream_id => E#event.stream_id\n        }),\n        E#event{data = SafeData}\n    end\n}).\n```","ref":"stream_links.html#4-real-time-analytics-feed"},{"type":"extras","title":"Telemetry - Stream Links","doc":"Link operations emit telemetry:\n\n```erlang\n%% Event: [reckon_db, link, created | started | stopped | deleted]\n%% Measurements: #{system_time => integer()}\n%% Metadata: #{store_id => atom(), link_name => binary()}\n```","ref":"stream_links.html#telemetry"},{"type":"extras","title":"Error Handling - Stream Links","doc":"| Error | Cause | Resolution |\n|-------|-------|------------|\n| `{error, not_found}` | Link does not exist | Verify link name |\n| `{error, already_exists}` | Link name in use | Choose different name |\n| Filter crash | Filter function threw exception | Returns `false` (event skipped) |\n| Transform crash | Transform function threw exception | Returns original event |","ref":"stream_links.html#error-handling"},{"type":"extras","title":"Best Practices - Stream Links","doc":"","ref":"stream_links.html#best-practices"},{"type":"extras","title":"1. Use Defensive Filters - Stream Links","doc":"```erlang\nfilter => fun(E) ->\n    try\n        maps:get(amount, E#event.data, 0) > 1000\n    catch\n        _:_ -> false\n    end\nend\n```","ref":"stream_links.html#1-use-defensive-filters"},{"type":"extras","title":"2. Avoid Expensive Transforms - Stream Links","doc":"Transforms run for every matching event. Keep them lightweight.","ref":"stream_links.html#2-avoid-expensive-transforms"},{"type":"extras","title":"3. Consider Backfill Costs - Stream Links","doc":"For large streams, backfill can be expensive. Consider:\n- Creating link without backfill first\n- Manually backfilling in batches during off-peak","ref":"stream_links.html#3-consider-backfill-costs"},{"type":"extras","title":"4. Monitor Processed Counts - Stream Links","doc":"```erlang\n%% Check link is keeping up\n{ok, Info} = reckon_db_links:info(my_store, <<\"my-link\">>),\nProcessed = maps:get(processed, Info),\n%% Alert if falling behind\n```","ref":"stream_links.html#4-monitor-processed-counts"},{"type":"extras","title":"See Also - Stream Links","doc":"- [Subscriptions](subscriptions.md) - Event subscription patterns\n- [Causation](causation.md) - Event lineage tracking\n- [Storage Internals](storage_internals.md) - Khepri path structure","ref":"stream_links.html#see-also"},{"type":"extras","title":"Schema Evolution","doc":"# Schema Evolution and Upcasting\n\nSchema evolution enables changing event structures over time without breaking existing consumers. This guide covers schema registration, version management, and automatic upcasting.","ref":"schema_evolution.html"},{"type":"extras","title":"Overview - Schema Evolution","doc":"The `reckon_db_schema` module provides:\n\n| Function | Purpose |\n|----------|---------|\n| `register/3` | Register a schema for an event type |\n| `unregister/2` | Remove a schema |\n| `get/2` | Get schema for an event type |\n| `list/1` | List all registered schemas |\n| `get_version/2` | Get current schema version |\n| `upcast/2` | Upcast a list of events |\n| `upcast_event/2` | Upcast a single event |\n| `validate/2` | Validate event against schema |","ref":"schema_evolution.html#overview"},{"type":"extras","title":"Architecture - Schema Evolution","doc":"![Schema Upcasting Flow](../assets/schema_upcasting.svg)","ref":"schema_evolution.html#architecture"},{"type":"extras","title":"Schema Registration - Schema Evolution","doc":"","ref":"schema_evolution.html#schema-registration"},{"type":"extras","title":"Basic Registration - Schema Evolution","doc":"```erlang\nok = reckon_db_schema:register(my_store, <<\"OrderPlaced\">>, #{\n    version => 1,\n    description => <<\"Initial order event schema\">>\n}).\n```","ref":"schema_evolution.html#basic-registration"},{"type":"extras","title":"Registration with Upcasting - Schema Evolution","doc":"```erlang\nok = reckon_db_schema:register(my_store, <<\"OrderPlaced\">>, #{\n    version => 3,\n    upcast_from => #{\n        1 => fun(Data) ->\n            %% V1 -> V2: Add shipping_address field\n            maps:put(shipping_address, maps:get(address, Data, #{}), Data)\n        end,\n        2 => fun(Data) ->\n            %% V2 -> V3: Rename customer_id to buyer_id\n            BuyerId = maps:get(customer_id, Data),\n            maps:remove(customer_id, maps:put(buyer_id, BuyerId, Data))\n        end\n    }\n}).\n```","ref":"schema_evolution.html#registration-with-upcasting"},{"type":"extras","title":"Registration with Validation - Schema Evolution","doc":"```erlang\nok = reckon_db_schema:register(my_store, <<\"PaymentReceived\">>, #{\n    version => 1,\n    validator => fun(Data) ->\n        case maps:is_key(amount, Data) andalso maps:is_key(currency, Data) of\n            true -> ok;\n            false -> {error, missing_required_fields}\n        end\n    end\n}).\n```","ref":"schema_evolution.html#registration-with-validation"},{"type":"extras","title":"Schema Storage - Schema Evolution","doc":"Schemas are stored in Khepri at:\n\n```\n[schemas, StoreId, EventType] -> schema_map()\n```","ref":"schema_evolution.html#schema-storage"},{"type":"extras","title":"Schema Structure - Schema Evolution","doc":"```erlang\n-type schema() :: #{\n    event_type := binary(),       %% Event type name\n    version := pos_integer(),     %% Current version (1, 2, 3, ...)\n    upcast_from => #{             %% Version -> Transform function\n        pos_integer() => fun((map()) -> map())\n    },\n    validator => fun((map()) -> ok | {error, term()}),\n    description => binary(),\n    registered_at := integer()    %% Timestamp\n}.\n```","ref":"schema_evolution.html#schema-structure"},{"type":"extras","title":"Upcasting - Schema Evolution","doc":"","ref":"schema_evolution.html#upcasting"},{"type":"extras","title":"How Upcasting Works - Schema Evolution","doc":"When reading events, upcasting transforms old versions to the current schema:\n\n```erlang\n%% Read events (may contain multiple versions)\n{ok, Events} = reckon_db_streams:read(my_store, <<\"orders-123\">>, 0, 100, forward),\n\n%% Upcast all events to current schema versions\nUpcastedEvents = reckon_db_schema:upcast(my_store, Events).\n```","ref":"schema_evolution.html#how-upcasting-works"},{"type":"extras","title":"Upcasting Flow - Schema Evolution","doc":"```\nEvent (v1)  upcast_from[1]  Event (v2)  upcast_from[2]  Event (v3)\n```\n\nEvents are upcasted through each version step sequentially.","ref":"schema_evolution.html#upcasting-flow"},{"type":"extras","title":"Version Detection - Schema Evolution","doc":"Event versions are stored in metadata:\n\n```erlang\n#event{\n    metadata = #{\n        schema_version => 2  %% Default: 1 if not present\n    }\n}\n```\n\nAfter upcasting, the `schema_version` is updated:\n\n```erlang\n%% Before: schema_version => 1\n%% After upcast to v3: schema_version => 3\n```","ref":"schema_evolution.html#version-detection"},{"type":"extras","title":"Cluster Behavior - Schema Evolution","doc":"","ref":"schema_evolution.html#cluster-behavior"},{"type":"extras","title":"Consistency - Schema Evolution","doc":"Schema registration is replicated through Khepri/Ra:\n\n1. Schema registered on any node\n2. Replicated through Raft consensus\n3. Available on all cluster nodes","ref":"schema_evolution.html#consistency"},{"type":"extras","title":"Version Conflicts - Schema Evolution","doc":"If different nodes have different schema versions:\n\n- Upcasting uses the local node's schema\n- Ensure schema updates are coordinated\n- Use rolling updates for schema changes","ref":"schema_evolution.html#version-conflicts"},{"type":"extras","title":"Evolution Strategies - Schema Evolution","doc":"","ref":"schema_evolution.html#evolution-strategies"},{"type":"extras","title":"1. Additive Changes (Safe) - Schema Evolution","doc":"Add new optional fields:\n\n```erlang\n%% V1 -> V2: Add optional field with default\n1 => fun(Data) ->\n    maps:put(priority, <<\"normal\">>, Data)\nend\n```","ref":"schema_evolution.html#1-additive-changes-safe"},{"type":"extras","title":"2. Renaming Fields - Schema Evolution","doc":"Rename while preserving data:\n\n```erlang\n%% V1 -> V2: Rename customerId to customer_id\n1 => fun(Data) ->\n    CustomerId = maps:get(customerId, Data),\n    maps:remove(customerId, maps:put(customer_id, CustomerId, Data))\nend\n```","ref":"schema_evolution.html#2-renaming-fields"},{"type":"extras","title":"3. Splitting Fields - Schema Evolution","doc":"Split one field into multiple:\n\n```erlang\n%% V1 -> V2: Split name into first_name and last_name\n1 => fun(Data) ->\n    Name = maps:get(name, Data, <<\"\">>),\n    [First | Rest] = binary:split(Name, <<\" \">>),\n    Last = iolist_to_binary(lists:join(<<\" \">>, Rest)),\n    Data2 = maps:remove(name, Data),\n    maps:merge(Data2, #{first_name => First, last_name => Last})\nend\n```","ref":"schema_evolution.html#3-splitting-fields"},{"type":"extras","title":"4. Merging Fields - Schema Evolution","doc":"Combine multiple fields:\n\n```erlang\n%% V1 -> V2: Merge address fields into address map\n1 => fun(Data) ->\n    Address = #{\n        street => maps:get(street, Data, <<\"\">>),\n        city => maps:get(city, Data, <<\"\">>),\n        zip => maps:get(zip, Data, <<\"\">>)\n    },\n    Data2 = maps:without([street, city, zip], Data),\n    maps:put(address, Address, Data2)\nend\n```","ref":"schema_evolution.html#4-merging-fields"},{"type":"extras","title":"5. Type Changes - Schema Evolution","doc":"Convert field types:\n\n```erlang\n%% V1 -> V2: Convert amount from cents (integer) to dollars (float)\n1 => fun(Data) ->\n    Cents = maps:get(amount, Data, 0),\n    Dollars = Cents / 100.0,\n    maps:put(amount, Dollars, Data)\nend\n```","ref":"schema_evolution.html#5-type-changes"},{"type":"extras","title":"Validation - Schema Evolution","doc":"","ref":"schema_evolution.html#validation"},{"type":"extras","title":"On Write - Schema Evolution","doc":"Validate events before appending:\n\n```erlang\nEvent = #event{event_type = <<\"OrderPlaced\">>, data = #{...}},\ncase reckon_db_schema:validate(my_store, Event) of\n    ok ->\n        reckon_db_streams:append(my_store, StreamId, ExpectedVersion, [Event]);\n    {error, Reason} ->\n        {error, {validation_failed, Reason}}\nend\n```","ref":"schema_evolution.html#on-write"},{"type":"extras","title":"Custom Validators - Schema Evolution","doc":"```erlang\nvalidator => fun(Data) ->\n    Amount = maps:get(amount, Data, 0),\n    Currency = maps:get(currency, Data, undefined),\n\n    case {Amount > 0, Currency =/= undefined} of\n        {true, true} -> ok;\n        {false, _} -> {error, {invalid_amount, Amount}};\n        {_, false} -> {error, missing_currency}\n    end\nend\n```","ref":"schema_evolution.html#custom-validators"},{"type":"extras","title":"Querying Schemas - Schema Evolution","doc":"","ref":"schema_evolution.html#querying-schemas"},{"type":"extras","title":"List All Schemas - Schema Evolution","doc":"```erlang\n{ok, Schemas} = reckon_db_schema:list(my_store),\n%% [#{event_type => <<\"OrderPlaced\">>, version => 3, registered_at => ...}, ...]\n```","ref":"schema_evolution.html#list-all-schemas"},{"type":"extras","title":"Get Specific Schema - Schema Evolution","doc":"```erlang\n{ok, Schema} = reckon_db_schema:get(my_store, <<\"OrderPlaced\">>),\n%% #{event_type => <<\"OrderPlaced\">>, version => 3, upcast_from => #{...}, ...}\n```","ref":"schema_evolution.html#get-specific-schema"},{"type":"extras","title":"Get Current Version - Schema Evolution","doc":"```erlang\n{ok, Version} = reckon_db_schema:get_version(my_store, <<\"OrderPlaced\">>),\n%% 3\n```","ref":"schema_evolution.html#get-current-version"},{"type":"extras","title":"Telemetry - Schema Evolution","doc":"Schema operations emit telemetry:\n\n```erlang\n%% Registration/unregistration\n%% Event: [reckon_db, schema, registered | unregistered]\n%% Measurements: #{version => integer()}\n%% Metadata: #{store_id => atom(), event_type => binary()}\n\n%% Upcasting\n%% Event: [reckon_db, schema, upcasted]\n%% Measurements: #{duration => integer()}\n%% Metadata: #{store_id => atom(), event_type => binary(),\n%%             from_version => integer(), to_version => integer()}\n```","ref":"schema_evolution.html#telemetry"},{"type":"extras","title":"Best Practices - Schema Evolution","doc":"","ref":"schema_evolution.html#best-practices"},{"type":"extras","title":"1. Never Remove Required Fields - Schema Evolution","doc":"```erlang\n%% BAD: Field removed without migration\n%% V1: #{order_id, customer_id, items}\n%% V2: #{order_id, items}  -- customer_id removed!\n\n%% GOOD: Deprecate, then remove in later version\n%% V1 -> V2: Mark deprecated\n%% V2 -> V3: Actually remove (after all consumers updated)\n```","ref":"schema_evolution.html#1-never-remove-required-fields"},{"type":"extras","title":"2. Make New Fields Optional - Schema Evolution","doc":"```erlang\n%% GOOD: New field has default value\n1 => fun(Data) ->\n    maps:put_new(priority, <<\"normal\">>, Data)\nend\n```","ref":"schema_evolution.html#2-make-new-fields-optional"},{"type":"extras","title":"3. Test Upcasting Thoroughly - Schema Evolution","doc":"```erlang\n%% Test each version transition\ntest_v1_to_v2() ->\n    V1Data = #{order_id => <<\"123\">>, customerId => <<\"cust-1\">>},\n    V2Data = upcast_v1_to_v2(V1Data),\n    ?assertEqual(<<\"cust-1\">>, maps:get(customer_id, V2Data)),\n    ?assertNot(maps:is_key(customerId, V2Data)).\n```","ref":"schema_evolution.html#3-test-upcasting-thoroughly"},{"type":"extras","title":"4. Version Incrementally - Schema Evolution","doc":"```erlang\n%% GOOD: One version per change\n%% V1 -> V2: Add field\n%% V2 -> V3: Rename field\n%% V3 -> V4: Change type\n\n%% BAD: Multiple changes per version\n%% V1 -> V2: Add field AND rename field AND change type\n```","ref":"schema_evolution.html#4-version-incrementally"},{"type":"extras","title":"5. Document Schema Changes - Schema Evolution","doc":"```erlang\nok = reckon_db_schema:register(my_store, <<\"OrderPlaced\">>, #{\n    version => 3,\n    description => <<\"V3: Renamed customer_id to buyer_id for consistency\">>,\n    upcast_from => #{...}\n}).\n```","ref":"schema_evolution.html#5-document-schema-changes"},{"type":"extras","title":"Error Handling - Schema Evolution","doc":"| Error | Cause | Resolution |\n|-------|-------|------------|\n| `{error, {invalid_version, V}}` | Version not positive integer | Use version >= 1 |\n| `{error, not_found}` | Schema not registered | Register schema first |\n| Upcast crash | Upcast function threw | Event returned unchanged, logged |","ref":"schema_evolution.html#error-handling"},{"type":"extras","title":"See Also - Schema Evolution","doc":"- [Event Sourcing](event_sourcing.md) - Event design principles\n- [Subscriptions](subscriptions.md) - Event consumption\n- [Storage Internals](storage_internals.md) - Khepri path structure","ref":"schema_evolution.html#see-also"},{"type":"extras","title":"Configuration","doc":"# Configuration Guide\n\nThis guide covers all configuration options for reckon-db, with examples for both Erlang (sys.config) and Elixir (config.exs).","ref":"configuration.html"},{"type":"extras","title":"Quick Start - Configuration","doc":"","ref":"configuration.html#quick-start"},{"type":"extras","title":"Erlang (sys.config) - Configuration","doc":"```erlang\n[\n  {reckon_db, [\n    {stores, [\n      {my_store, [\n        {data_dir, \"/var/lib/reckon_db/my_store\"},\n        {mode, single},\n        {timeout, 5000},\n        {writer_pool_size, 10},\n        {reader_pool_size, 10}\n      ]}\n    ]},\n    {telemetry_handlers, [logger]}\n  ]}\n].\n```","ref":"configuration.html#erlang-sys-config"},{"type":"extras","title":"Elixir (config.exs) - Configuration","doc":"```elixir\nconfig :reckon_db,\n  stores: [\n    my_store: [\n      data_dir: \"/var/lib/reckon_db/my_store\",\n      mode: :single,\n      timeout: 5_000,\n      writer_pool_size: 10,\n      reader_pool_size: 10\n    ]\n  ],\n  telemetry_handlers: [:logger]\n```","ref":"configuration.html#elixir-config-exs"},{"type":"extras","title":"Configuration Reference - Configuration","doc":"","ref":"configuration.html#configuration-reference"},{"type":"extras","title":"Store Configuration - Configuration","doc":"Stores are the primary configuration. Each store is an independent event store instance backed by Khepri/Ra.\n\n| Option | Type | Default | Description |\n|--------|------|---------|-------------|\n| `stores` | proplist | `[]` | List of store configurations |\n\nEach store in the list has:\n\n| Option | Type | Default | Description |\n|--------|------|---------|-------------|\n| `data_dir` | string | `/var/lib/reckon_db/{store_id}` | Data directory for Khepri/Ra |\n| `mode` | atom | `single` | Operation mode: `single` or `cluster` |\n| `timeout` | integer | 5000 | Default timeout in milliseconds |\n| `writer_pool_size` | integer | 10 | Number of writer workers |\n| `reader_pool_size` | integer | 10 | Number of reader workers |\n| `gateway_pool_size` | integer | 1 | Number of gateway workers |\n\n#### Erlang Example\n\n```erlang\n{stores, [\n  {orders_store, [\n    {data_dir, \"/bulk0/reckon_db/orders\"},\n    {mode, cluster},\n    {timeout, 10000},\n    {writer_pool_size, 20},\n    {reader_pool_size, 50}\n  ]},\n  {users_store, [\n    {data_dir, \"/bulk0/reckon_db/users\"},\n    {mode, single},\n    {writer_pool_size, 5},\n    {reader_pool_size, 10}\n  ]}\n]}\n```\n\n#### Elixir Example\n\n```elixir\nconfig :reckon_db,\n  stores: [\n    orders_store: [\n      data_dir: \"/bulk0/reckon_db/orders\",\n      mode: :cluster,\n      timeout: 10_000,\n      writer_pool_size: 20,\n      reader_pool_size: 50\n    ],\n    users_store: [\n      data_dir: \"/bulk0/reckon_db/users\",\n      mode: :single,\n      writer_pool_size: 5,\n      reader_pool_size: 10\n    ]\n  ]\n```","ref":"configuration.html#store-configuration"},{"type":"extras","title":"Pool Sizes (Global Defaults) - Configuration","doc":"These set defaults for all stores that don't specify their own values.\n\n| Option | Type | Default | Description |\n|--------|------|---------|-------------|\n| `writer_pool_size` | integer | 10 | Default writer pool size |\n| `reader_pool_size` | integer | 10 | Default reader pool size |\n| `gateway_pool_size` | integer | 1 | Default gateway pool size |\n\n#### Erlang Example\n\n```erlang\n{writer_pool_size, 20},\n{reader_pool_size, 50},\n{gateway_pool_size, 2}\n```\n\n#### Elixir Example\n\n```elixir\nconfig :reckon_db,\n  writer_pool_size: 20,\n  reader_pool_size: 50,\n  gateway_pool_size: 2\n```","ref":"configuration.html#pool-sizes-global-defaults"},{"type":"extras","title":"Worker Timeouts - Configuration","doc":"Idle timeout for reader and writer workers.\n\n| Option | Type | Default | Description |\n|--------|------|---------|-------------|\n| `reader_idle_timeout_ms` | integer | 60000 | Reader idle timeout (ms) |\n| `writer_idle_timeout_ms` | integer | 60000 | Writer idle timeout (ms) |\n\n#### Erlang Example\n\n```erlang\n{reader_idle_timeout_ms, 120000},  %% 2 minutes\n{writer_idle_timeout_ms, 120000}\n```\n\n#### Elixir Example\n\n```elixir\nconfig :reckon_db,\n  reader_idle_timeout_ms: 120_000,\n  writer_idle_timeout_ms: 120_000\n```","ref":"configuration.html#worker-timeouts"},{"type":"extras","title":"Health Probing - Configuration","doc":"Health probing monitors store nodes and triggers failover in cluster mode.\n\n| Option | Type | Default | Description |\n|--------|------|---------|-------------|\n| `health_probe_interval` | integer | 5000 | Probe interval in milliseconds |\n| `health_probe_timeout` | integer | 2000 | Probe timeout in milliseconds |\n| `health_failure_threshold` | integer | 3 | Failures before marking unhealthy |\n| `health_probe_type` | atom | `ping` | Probe type: `ping` or `deep` |\n\n**Probe Types:**\n\n- `ping` - Simple node reachability check (fast, low overhead)\n- `deep` - Checks Khepri/Ra cluster health (thorough, higher overhead)\n\n#### Erlang Example\n\n```erlang\n{health_probe_interval, 10000},\n{health_probe_timeout, 5000},\n{health_failure_threshold, 5},\n{health_probe_type, deep}\n```\n\n#### Elixir Example\n\n```elixir\nconfig :reckon_db,\n  health_probe_interval: 10_000,\n  health_probe_timeout: 5_000,\n  health_failure_threshold: 5,\n  health_probe_type: :deep\n```","ref":"configuration.html#health-probing"},{"type":"extras","title":"Consistency Checking - Configuration","doc":"Periodic consistency verification for cluster mode.\n\n| Option | Type | Default | Description |\n|--------|------|---------|-------------|\n| `consistency_check_interval` | integer | 60000 | Check interval in milliseconds |\n\n#### Erlang Example\n\n```erlang\n{consistency_check_interval, 30000}  %% 30 seconds\n```\n\n#### Elixir Example\n\n```elixir\nconfig :reckon_db,\n  consistency_check_interval: 30_000\n```","ref":"configuration.html#consistency-checking"},{"type":"extras","title":"Persistence - Configuration","doc":"Controls periodic snapshot persistence to disk.\n\n| Option | Type | Default | Description |\n|--------|------|---------|-------------|\n| `persistence_interval` | integer | 60000 | Persistence interval in milliseconds |\n\n#### Erlang Example\n\n```erlang\n{persistence_interval, 30000}  %% 30 seconds\n```\n\n#### Elixir Example\n\n```elixir\nconfig :reckon_db,\n  persistence_interval: 30_000\n```","ref":"configuration.html#persistence"},{"type":"extras","title":"Telemetry - Configuration","doc":"Configure telemetry handlers for metrics and logging.\n\n| Option | Type | Default | Description |\n|--------|------|---------|-------------|\n| `telemetry_handlers` | list | `[logger]` | List of telemetry handlers |\n\n**Available Handlers:**\n\n- `logger` - Logs events via OTP logger\n\n#### Erlang Example\n\n```erlang\n{telemetry_handlers, [logger]}\n```\n\n#### Elixir Example\n\n```elixir\nconfig :reckon_db,\n  telemetry_handlers: [:logger]\n```","ref":"configuration.html#telemetry"},{"type":"extras","title":"Complete Configuration Examples - Configuration","doc":"","ref":"configuration.html#complete-configuration-examples"},{"type":"extras","title":"Single Node Development - Configuration","doc":"```erlang\n%% Erlang sys.config\n[\n  {reckon_db, [\n    {stores, [\n      {dev_store, [\n        {data_dir, \"/tmp/reckon_db/dev\"},\n        {mode, single}\n      ]}\n    ]},\n    %% Small pools for development\n    {writer_pool_size, 2},\n    {reader_pool_size, 5},\n    %% Fast feedback on issues\n    {health_probe_interval, 2000},\n    {health_failure_threshold, 1},\n    %% Frequent persistence for testing\n    {persistence_interval, 5000}\n  ]}\n].\n```\n\n```elixir\n# Elixir config/dev.exs\nconfig :reckon_db,\n  stores: [\n    dev_store: [\n      data_dir: \"/tmp/reckon_db/dev\",\n      mode: :single\n    ]\n  ],\n  writer_pool_size: 2,\n  reader_pool_size: 5,\n  health_probe_interval: 2_000,\n  health_failure_threshold: 1,\n  persistence_interval: 5_000\n```","ref":"configuration.html#single-node-development"},{"type":"extras","title":"Production Cluster - Configuration","doc":"```erlang\n%% Erlang sys.config\n[\n  {reckon_db, [\n    {stores, [\n      {main_store, [\n        {data_dir, \"/bulk0/reckon_db/main\"},\n        {mode, cluster},\n        {timeout, 10000},\n        {writer_pool_size, 50},\n        {reader_pool_size, 100},\n        {gateway_pool_size, 5}\n      ]}\n    ]},\n    %% Production health monitoring\n    {health_probe_interval, 5000},\n    {health_probe_timeout, 3000},\n    {health_failure_threshold, 3},\n    {health_probe_type, deep},\n    %% Consistency and persistence\n    {consistency_check_interval, 60000},\n    {persistence_interval, 30000},\n    %% Longer idle timeouts\n    {reader_idle_timeout_ms, 300000},\n    {writer_idle_timeout_ms, 300000}\n  ]}\n].\n```\n\n```elixir\n# Elixir config/runtime.exs\nconfig :reckon_db,\n  stores: [\n    main_store: [\n      data_dir: \"/bulk0/reckon_db/main\",\n      mode: :cluster,\n      timeout: 10_000,\n      writer_pool_size: 50,\n      reader_pool_size: 100,\n      gateway_pool_size: 5\n    ]\n  ],\n  # Production health monitoring\n  health_probe_interval: 5_000,\n  health_probe_timeout: 3_000,\n  health_failure_threshold: 3,\n  health_probe_type: :deep,\n  # Consistency and persistence\n  consistency_check_interval: 60_000,\n  persistence_interval: 30_000,\n  # Longer idle timeouts\n  reader_idle_timeout_ms: 300_000,\n  writer_idle_timeout_ms: 300_000\n```","ref":"configuration.html#production-cluster"},{"type":"extras","title":"Multi-Store Setup - Configuration","doc":"```elixir\n# Elixir config.exs - Multiple stores for different domains\nconfig :reckon_db,\n  stores: [\n    # High-write orders store\n    orders_store: [\n      data_dir: \"/bulk0/reckon_db/orders\",\n      mode: :cluster,\n      writer_pool_size: 100,\n      reader_pool_size: 50\n    ],\n    # Read-heavy analytics store\n    analytics_store: [\n      data_dir: \"/bulk1/reckon_db/analytics\",\n      mode: :single,\n      writer_pool_size: 5,\n      reader_pool_size: 200\n    ],\n    # Low-volume user store\n    users_store: [\n      data_dir: \"/bulk0/reckon_db/users\",\n      mode: :single,\n      writer_pool_size: 10,\n      reader_pool_size: 20\n    ]\n  ]\n```","ref":"configuration.html#multi-store-setup"},{"type":"extras","title":"Cluster Mode Configuration - Configuration","doc":"When running in cluster mode, additional Erlang VM configuration is needed.","ref":"configuration.html#cluster-mode-configuration"},{"type":"extras","title":"vm.args - Configuration","doc":"```","ref":"configuration.html#vm-args"},{"type":"extras","title":"Node name (required for clustering) - Configuration","doc":"-name store1@192.168.1.10","ref":"configuration.html#node-name-required-for-clustering"},{"type":"extras","title":"Cookie for cluster authentication - Configuration","doc":"-setcookie my_cluster_cookie","ref":"configuration.html#cookie-for-cluster-authentication"},{"type":"extras","title":"Enable distribution - Configuration","doc":"-proto_dist inet_tcp","ref":"configuration.html#enable-distribution"},{"type":"extras","title":"Increase distribution buffer - Configuration","doc":"+zdbbl 32768\n```","ref":"configuration.html#increase-distribution-buffer"},{"type":"extras","title":"Connecting Nodes - Configuration","doc":"Cluster formation happens via Khepri/Ra. Use the API to join nodes:\n\n```erlang\n%% On the joining node\nreckon_db:join_cluster(my_store, 'store1@192.168.1.10').\n```\n\n```elixir\n# Elixir\n:reckon_db.join_cluster(:my_store, :\"store1@192.168.1.10\")\n```","ref":"configuration.html#connecting-nodes"},{"type":"extras","title":"Data Directory Guidelines - Configuration","doc":"","ref":"configuration.html#data-directory-guidelines"},{"type":"extras","title":"Linux/Production - Configuration","doc":"Store data on separate disk partitions for performance:\n\n```erlang\n{data_dir, \"/bulk0/reckon_db/my_store\"}\n```","ref":"configuration.html#linux-production"},{"type":"extras","title":"Development - Configuration","doc":"Use temp directory for ephemeral data:\n\n```erlang\n{data_dir, \"/tmp/reckon_db/dev_store\"}\n```","ref":"configuration.html#development"},{"type":"extras","title":"Docker/Container - Configuration","doc":"Mount a volume for persistence:\n\n```yaml\nvolumes:\n  - esdb_data:/var/lib/reckon_db\n```\n\n```erlang\n{data_dir, \"/var/lib/reckon_db/my_store\"}\n```","ref":"configuration.html#docker-container"},{"type":"extras","title":"Performance Tuning - Configuration","doc":"","ref":"configuration.html#performance-tuning"},{"type":"extras","title":"High-Write Workloads - Configuration","doc":"```elixir\nconfig :reckon_db,\n  stores: [\n    high_write: [\n      writer_pool_size: 100,      # Many concurrent writers\n      reader_pool_size: 20,       # Fewer readers needed\n      mode: :cluster              # Distribute writes\n    ]\n  ],\n  persistence_interval: 60_000    # Less frequent persistence\n```","ref":"configuration.html#high-write-workloads"},{"type":"extras","title":"High-Read Workloads - Configuration","doc":"```elixir\nconfig :reckon_db,\n  stores: [\n    high_read: [\n      writer_pool_size: 10,       # Fewer writers needed\n      reader_pool_size: 200,      # Many concurrent readers\n      mode: :cluster              # Read from any replica\n    ]\n  ],\n  reader_idle_timeout_ms: 600_000 # Keep readers alive longer\n```","ref":"configuration.html#high-read-workloads"},{"type":"extras","title":"Low-Latency Requirements - Configuration","doc":"```elixir\nconfig :reckon_db,\n  stores: [\n    low_latency: [\n      timeout: 2_000,             # Fast timeouts\n      gateway_pool_size: 10       # More gateway workers\n    ]\n  ],\n  health_probe_interval: 1_000,   # Fast failure detection\n  health_failure_threshold: 1     # Immediate failover\n```","ref":"configuration.html#low-latency-requirements"},{"type":"extras","title":"Telemetry Events - Configuration","doc":"reckon-db emits telemetry events for monitoring:\n\n| Event | Measurements | Metadata |\n|-------|--------------|----------|\n| `[reckon_db, append, start]` | - | store_id, stream |\n| `[reckon_db, append, stop]` | duration | store_id, stream, count |\n| `[reckon_db, append, exception]` | duration | store_id, stream, reason |\n| `[reckon_db, read, start]` | - | store_id, stream |\n| `[reckon_db, read, stop]` | duration | store_id, stream, count |\n| `[reckon_db, read, exception]` | duration | store_id, stream, reason |\n| `[reckon_db, health, probe]` | latency | store_id, node, status |\n| `[reckon_db, health, change]` | - | store_id, node, old, new |","ref":"configuration.html#telemetry-events"},{"type":"extras","title":"Attaching Handlers - Configuration","doc":"```elixir\n# Elixir\n:telemetry.attach_many(\n  \"reckon-db-metrics\",\n  [\n    [:reckon_db, :append, :stop],\n    [:reckon_db, :read, :stop],\n    [:reckon_db, :health, :change]\n  ],\n  &MyApp.Metrics.handle_event/4,\n  nil\n)\n```\n\n```erlang\n%% Erlang\ntelemetry:attach_many(\n  <<\"reckon-db-metrics\">>,\n  [\n    [reckon_db, append, stop],\n    [reckon_db, read, stop],\n    [reckon_db, health, change]\n  ],\n  fun my_metrics:handle_event/4,\n  undefined\n).\n```","ref":"configuration.html#attaching-handlers"},{"type":"extras","title":"See Also - Configuration","doc":"- [Storage Internals](storage_internals.md) - How data is stored\n- [Cluster Consistency](cluster_consistency.md) - Consistency guarantees\n- [Memory Pressure](memory_pressure.md) - Memory management\n- [Snapshots](snapshots.md) - Snapshot configuration","ref":"configuration.html#see-also"},{"type":"extras","title":"Memory Pressure","doc":"# Memory Pressure Monitoring\n\nMemory pressure monitoring enables adaptive behavior when system memory becomes constrained. This guide covers the monitoring architecture, pressure levels, and integration patterns.","ref":"memory_pressure.html"},{"type":"extras","title":"Overview - Memory Pressure","doc":"The `reckon_db_memory` module provides:\n\n| Function | Purpose |\n|----------|---------|\n| `start_link/0,1` | Start the memory monitor |\n| `level/0` | Get current pressure level |\n| `configure/1` | Update thresholds |\n| `on_pressure_change/1` | Register callback |\n| `remove_callback/1` | Remove callback |\n| `get_stats/0` | Get memory statistics |\n| `check_now/0` | Force immediate check |","ref":"memory_pressure.html#overview"},{"type":"extras","title":"Architecture - Memory Pressure","doc":"![Memory Pressure Levels](../assets/memory_levels.svg)","ref":"memory_pressure.html#architecture"},{"type":"extras","title":"Pressure Levels - Memory Pressure","doc":"| Level | Default Threshold | Description |\n|-------|-------------------|-------------|\n| `normal` | < 70% | Full caching, all features enabled |\n| `elevated` | 70-85% | Reduce cache sizes, flush more often |\n| `critical` | > 85% | Pause non-essential operations, aggressive cleanup |","ref":"memory_pressure.html#pressure-levels"},{"type":"extras","title":"Starting the Monitor - Memory Pressure","doc":"","ref":"memory_pressure.html#starting-the-monitor"},{"type":"extras","title":"Default Configuration - Memory Pressure","doc":"```erlang\n{ok, _Pid} = reckon_db_memory:start_link().\n```","ref":"memory_pressure.html#default-configuration"},{"type":"extras","title":"Custom Configuration - Memory Pressure","doc":"```erlang\n{ok, _Pid} = reckon_db_memory:start_link(#{\n    elevated_threshold => 0.60,    %% Trigger elevated at 60%\n    critical_threshold => 0.80,    %% Trigger critical at 80%\n    check_interval => 5000         %% Check every 5 seconds\n}).\n```","ref":"memory_pressure.html#custom-configuration"},{"type":"extras","title":"Checking Pressure Level - Memory Pressure","doc":"","ref":"memory_pressure.html#checking-pressure-level"},{"type":"extras","title":"Current Level - Memory Pressure","doc":"```erlang\nLevel = reckon_db_memory:level(),\n%% normal | elevated | critical\n```","ref":"memory_pressure.html#current-level"},{"type":"extras","title":"With Statistics - Memory Pressure","doc":"```erlang\nStats = reckon_db_memory:get_stats(),\n%% #{\n%%     level => normal,\n%%     memory_used => 2147483648,      %% bytes\n%%     memory_total => 4294967296,     %% bytes\n%%     last_check => 1735689600000,    %% timestamp\n%%     callback_count => 3\n%% }\n```","ref":"memory_pressure.html#with-statistics"},{"type":"extras","title":"Registering Callbacks - Memory Pressure","doc":"","ref":"memory_pressure.html#registering-callbacks"},{"type":"extras","title":"On Pressure Change - Memory Pressure","doc":"```erlang\nCallbackRef = reckon_db_memory:on_pressure_change(fun(Level) ->\n    case Level of\n        normal ->\n            logger:info(\"Memory pressure normalized\"),\n            restore_full_functionality();\n        elevated ->\n            logger:warning(\"Memory pressure elevated\"),\n            reduce_cache_sizes();\n        critical ->\n            logger:error(\"Memory pressure critical!\"),\n            pause_non_essential_operations()\n    end\nend).\n```","ref":"memory_pressure.html#on-pressure-change"},{"type":"extras","title":"Removing Callback - Memory Pressure","doc":"```erlang\nok = reckon_db_memory:remove_callback(CallbackRef).\n```","ref":"memory_pressure.html#removing-callback"},{"type":"extras","title":"Memory Detection - Memory Pressure","doc":"","ref":"memory_pressure.html#memory-detection"},{"type":"extras","title":"How Memory is Measured - Memory Pressure","doc":"The monitor uses `memsup` (SASL memory supervisor):\n\n```erlang\nget_memory_info() ->\n    MemData = memsup:get_system_memory_data(),\n    case MemData of\n        [] ->\n            %% Fallback to erlang:memory()\n            ErlangMem = erlang:memory(),\n            Used = proplists:get_value(total, ErlangMem, 0),\n            {Used, Used * 2};\n        _ ->\n            Total = proplists:get_value(total_memory, MemData, 0),\n            Free = proplists:get_value(free_memory, MemData, 0),\n            Cached = proplists:get_value(cached_memory, MemData, 0),\n            Buffered = proplists:get_value(buffered_memory, MemData, 0),\n            Available = Free + Cached + Buffered,\n            Used = Total - Available,\n            {Used, Total}\n    end.\n```","ref":"memory_pressure.html#how-memory-is-measured"},{"type":"extras","title":"Available Memory Calculation - Memory Pressure","doc":"Available memory includes:\n- Free memory\n- Cached memory (can be reclaimed)\n- Buffered memory (can be reclaimed)\n\nThis provides a more accurate picture than just free memory.","ref":"memory_pressure.html#available-memory-calculation"},{"type":"extras","title":"Integration Patterns - Memory Pressure","doc":"","ref":"memory_pressure.html#integration-patterns"},{"type":"extras","title":"1. Subscription Backpressure - Memory Pressure","doc":"```erlang\n%% In reckon_db_emitter or subscription handler\ninit(State) ->\n    CallbackRef = reckon_db_memory:on_pressure_change(fun(Level) ->\n        gen_server:cast(self(), {memory_pressure, Level})\n    end),\n    {ok, State#{memory_callback => CallbackRef}}.\n\nhandle_cast({memory_pressure, critical}, State) ->\n    %% Pause subscription delivery\n    {noreply, State#{paused => true}};\nhandle_cast({memory_pressure, _}, State) ->\n    %% Resume\n    {noreply, State#{paused => false}}.\n```","ref":"memory_pressure.html#1-subscription-backpressure"},{"type":"extras","title":"2. Cache Management - Memory Pressure","doc":"```erlang\nhandle_info(check_cache, State) ->\n    case reckon_db_memory:level() of\n        critical ->\n            ets:delete_all_objects(my_cache),\n            logger:warning(\"Cache cleared due to memory pressure\");\n        elevated ->\n            evict_oldest_entries(my_cache, 0.5);  %% Evict 50%\n        normal ->\n            ok\n    end,\n    {noreply, State}.\n```","ref":"memory_pressure.html#2-cache-management"},{"type":"extras","title":"3. Write Throttling - Memory Pressure","doc":"```erlang\nappend_with_throttle(Store, Stream, Version, Events) ->\n    case reckon_db_memory:level() of\n        critical ->\n            timer:sleep(100),  %% Slow down writes\n            append_with_throttle(Store, Stream, Version, Events);\n        elevated ->\n            timer:sleep(10),   %% Minor throttle\n            reckon_db_streams:append(Store, Stream, Version, Events);\n        normal ->\n            reckon_db_streams:append(Store, Stream, Version, Events)\n    end.\n```","ref":"memory_pressure.html#3-write-throttling"},{"type":"extras","title":"Configuration - Memory Pressure","doc":"","ref":"memory_pressure.html#configuration"},{"type":"extras","title":"Dynamic Reconfiguration - Memory Pressure","doc":"```erlang\nok = reckon_db_memory:configure(#{\n    elevated_threshold => 0.75,\n    critical_threshold => 0.90,\n    check_interval => 15000\n}).\n```","ref":"memory_pressure.html#dynamic-reconfiguration"},{"type":"extras","title":"Getting Current Config - Memory Pressure","doc":"```erlang\nConfig = reckon_db_memory:get_config(),\n%% #{\n%%     elevated_threshold => 0.70,\n%%     critical_threshold => 0.85,\n%%     check_interval => 10000\n%% }\n```","ref":"memory_pressure.html#getting-current-config"},{"type":"extras","title":"Forcing Checks - Memory Pressure","doc":"```erlang\n%% Force immediate memory check\nCurrentLevel = reckon_db_memory:check_now().\n```\n\nUseful for:\n- Testing pressure response\n- After known memory-heavy operations\n- Before starting large batch jobs","ref":"memory_pressure.html#forcing-checks"},{"type":"extras","title":"Telemetry - Memory Pressure","doc":"Pressure changes emit telemetry:\n\n```erlang\n%% Event: [reckon_db, memory, pressure_changed]\n%% Measurements:\n%%   #{usage_ratio => float()}  %% 0.0 - 1.0\n%% Metadata:\n%%   #{old_level => atom(), new_level => atom()}\n```","ref":"memory_pressure.html#telemetry"},{"type":"extras","title":"Example Handler - Memory Pressure","doc":"```erlang\ntelemetry:attach(\n    <<\"memory-pressure-handler\">>,\n    [reckon_db, memory, pressure_changed],\n    fun(_Event, #{usage_ratio := Ratio}, #{old_level := Old, new_level := New}, _Config) ->\n        logger:notice(\"Memory pressure: ~p -> ~p (~.1f%)\",\n                      [Old, New, Ratio * 100])\n    end,\n    #{}\n).\n```","ref":"memory_pressure.html#example-handler"},{"type":"extras","title":"Cluster Considerations - Memory Pressure","doc":"","ref":"memory_pressure.html#cluster-considerations"},{"type":"extras","title":"Local Monitoring - Memory Pressure","doc":"Memory pressure is monitored locally on each node:\n\n```\n        \n     Node 1               Node 2               Node 3      \n              \n  Memory             Memory             Memory       \n  Monitor            Monitor            Monitor      \n  (normal)           (elevated)         (normal)     \n              \n        \n```\n\nEach node manages its own memory pressure independently.","ref":"memory_pressure.html#local-monitoring"},{"type":"extras","title":"Cross-Node Awareness - Memory Pressure","doc":"For cluster-wide memory awareness, consider:\n\n```erlang\n%% Broadcast pressure to other nodes\non_pressure_change(fun(Level) ->\n    rpc:multicall(nodes(), my_module, handle_remote_pressure, [node(), Level])\nend).\n```","ref":"memory_pressure.html#cross-node-awareness"},{"type":"extras","title":"Best Practices - Memory Pressure","doc":"","ref":"memory_pressure.html#best-practices"},{"type":"extras","title":"1. Register Early - Memory Pressure","doc":"```erlang\n%% In application start\nstart(_Type, _Args) ->\n    {ok, _} = reckon_db_memory:start_link(),\n    register_pressure_handlers(),\n    ...\n```","ref":"memory_pressure.html#1-register-early"},{"type":"extras","title":"2. Graceful Degradation - Memory Pressure","doc":"```erlang\n%% Implement graceful degradation, not hard failures\nhandle_pressure(critical) ->\n    %% Don't crash, just slow down\n    reduce_batch_sizes(),\n    increase_flush_intervals(),\n    pause_optional_projections();\nhandle_pressure(elevated) ->\n    %% Warning mode\n    reduce_cache_ttls();\nhandle_pressure(normal) ->\n    restore_defaults().\n```","ref":"memory_pressure.html#2-graceful-degradation"},{"type":"extras","title":"3. Monitor Recovery - Memory Pressure","doc":"```erlang\n%% Track recovery from pressure\nhandle_pressure_change(OldLevel, NewLevel) ->\n    case {OldLevel, NewLevel} of\n        {critical, elevated} ->\n            logger:info(\"Memory recovering - still elevated\");\n        {critical, normal} ->\n            logger:info(\"Memory fully recovered\");\n        {elevated, critical} ->\n            logger:error(\"Memory degrading to critical\");\n        _ ->\n            ok\n    end.\n```","ref":"memory_pressure.html#3-monitor-recovery"},{"type":"extras","title":"4. Test Under Pressure - Memory Pressure","doc":"```erlang\n%% In tests, simulate pressure\ntest_critical_behavior() ->\n    %% Force critical level\n    meck:new(reckon_db_memory, [passthrough]),\n    meck:expect(reckon_db_memory, level, fun() -> critical end),\n\n    %% Test behavior\n    ?assertEqual(expected_behavior, my_module:do_something()),\n\n    meck:unload(reckon_db_memory).\n```","ref":"memory_pressure.html#4-test-under-pressure"},{"type":"extras","title":"SASL Configuration - Memory Pressure","doc":"For `memsup` to work, configure SASL:\n\n```erlang\n%% In sys.config\n{sasl, [\n    {sasl_error_logger, {file, \"log/sasl.log\"}},\n    {errlog_type, error}\n]},\n{os_mon, [\n    {start_memsup, true},\n    {memsup_system_only, false},\n    {memory_check_interval, 60}  %% OS check interval (seconds)\n]}\n```","ref":"memory_pressure.html#sasl-configuration"},{"type":"extras","title":"See Also - Memory Pressure","doc":"- [Subscriptions](subscriptions.md) - Event subscription with backpressure\n- [Storage Internals](storage_internals.md) - Khepri memory usage\n- [Scavenging](scavenging.md) - Free memory by removing old events","ref":"memory_pressure.html#see-also"},{"type":"extras","title":"Storage Internals","doc":"# Storage Internals\n\nThis guide covers the internal storage architecture of reckon-db, including Khepri path structures, data organization, and cluster replication behavior.","ref":"storage_internals.html"},{"type":"extras","title":"Overview - Storage Internals","doc":"reckon-db uses **Khepri** as its storage layer. Khepri is a tree-like key-value store built on Ra (Raft consensus), providing:\n\n- Strong consistency across cluster nodes\n- Automatic replication\n- Tree-structured data organization\n- Efficient prefix queries","ref":"storage_internals.html#overview"},{"type":"extras","title":"Architecture - Storage Internals","doc":"![Khepri Storage Paths](../assets/khepri_paths.svg)","ref":"storage_internals.html#architecture"},{"type":"extras","title":"Khepri Path Structure - Storage Internals","doc":"All data is organized under hierarchical paths:\n\n```\n[root]\n [streams]\n    [StreamId]\n        [PaddedVersion] -> #event{}\n [snapshots]\n    [StreamId]\n        [PaddedVersion] -> #snapshot{}\n [subscriptions]\n    [SubscriptionName] -> #subscription{}\n [schemas]\n    [StoreId]\n        [EventType] -> schema_map()\n [links]\n    [StoreId]\n        [LinkName] -> #link{}\n [metadata]\n     [Key] -> Value\n```","ref":"storage_internals.html#khepri-path-structure"},{"type":"extras","title":"Streams Storage - Storage Internals","doc":"","ref":"storage_internals.html#streams-storage"},{"type":"extras","title":"Path Format - Storage Internals","doc":"```erlang\n?STREAMS_PATH ++ [StreamId, PaddedVersion]\n%% Example: [streams, <<\"orders-123\">>, <<\"000000000042\">>]\n```","ref":"storage_internals.html#path-format"},{"type":"extras","title":"Version Padding - Storage Internals","doc":"Versions are zero-padded to 12 characters for lexicographic ordering:\n\n```erlang\n-define(VERSION_PADDING, 12).\n\npad_version(Version, Length) ->\n    VersionStr = integer_to_list(Version),\n    Padding = Length - length(VersionStr),\n    PaddedStr = lists:duplicate(Padding, $0) ++ VersionStr,\n    list_to_binary(PaddedStr).\n\n%% Examples:\n%% 0 -> <<\"000000000000\">>\n%% 42 -> <<\"000000000042\">>\n%% 999999999999 -> <<\"999999999999\">>\n```\n\nThis supports up to 999,999,999,999 events per stream (~317 years at 100 events/sec).","ref":"storage_internals.html#version-padding"},{"type":"extras","title":"Event Record - Storage Internals","doc":"```erlang\n-record(event, {\n    event_id :: binary(),           %% UUID\n    stream_id :: binary(),          %% Stream identifier\n    version :: non_neg_integer(),   %% 0-indexed position\n    event_type :: binary(),         %% Event type name\n    data :: map(),                  %% Event payload\n    metadata :: map(),              %% Event metadata\n    epoch_us :: integer()           %% Timestamp (microseconds since epoch)\n}).\n```","ref":"storage_internals.html#event-record"},{"type":"extras","title":"Reading Events - Storage Internals","doc":"Events are read using Khepri path queries:\n\n```erlang\n%% Read specific version\nkhepri:get(StoreId, [streams, StreamId, PaddedVersion]).\n\n%% Read range of versions\nPattern = [streams, StreamId, ?KHEPRI_WILDCARD_STAR],\nkhepri:get_many(StoreId, Pattern).\n```","ref":"storage_internals.html#reading-events"},{"type":"extras","title":"Snapshots Storage - Storage Internals","doc":"","ref":"storage_internals.html#snapshots-storage"},{"type":"extras","title":"Path Format - Storage Internals","doc":"```erlang\n?SNAPSHOTS_PATH ++ [StreamId, PaddedVersion]\n%% Example: [snapshots, <<\"orders-123\">>, <<\"000000000100\">>]\n```","ref":"storage_internals.html#path-format-1"},{"type":"extras","title":"Snapshot Record - Storage Internals","doc":"```erlang\n-record(snapshot, {\n    stream_id :: binary(),\n    version :: non_neg_integer(),   %% Event version this snapshot represents\n    state :: term(),                %% Serialized aggregate state\n    metadata :: map(),\n    created_at :: integer()         %% Timestamp\n}).\n```","ref":"storage_internals.html#snapshot-record"},{"type":"extras","title":"Latest Snapshot Query - Storage Internals","doc":"```erlang\n%% Get all snapshots for a stream, sorted by version descending\nPattern = [snapshots, StreamId, ?KHEPRI_WILDCARD_STAR],\n{ok, Snapshots} = khepri:get_many(StoreId, Pattern),\n%% Sort by version descending to get latest first\n```","ref":"storage_internals.html#latest-snapshot-query"},{"type":"extras","title":"Subscriptions Storage - Storage Internals","doc":"","ref":"storage_internals.html#subscriptions-storage"},{"type":"extras","title":"Path Format - Storage Internals","doc":"```erlang\n?SUBSCRIPTIONS_PATH ++ [SubscriptionName]\n%% Example: [subscriptions, <<\"my-projection\">>]\n```","ref":"storage_internals.html#path-format-2"},{"type":"extras","title":"Subscription Record - Storage Internals","doc":"```erlang\n-record(subscription, {\n    name :: binary(),\n    type :: stream | event_type | event_pattern | event_payload,\n    selector :: binary() | map(),\n    handler :: pid() | function(),\n    options :: map(),\n    created_at :: integer()\n}).\n```","ref":"storage_internals.html#subscription-record"},{"type":"extras","title":"Schema Registry Storage - Storage Internals","doc":"","ref":"storage_internals.html#schema-registry-storage"},{"type":"extras","title":"Path Format - Storage Internals","doc":"```erlang\n?SCHEMAS_PATH ++ [StoreId, EventType]\n%% Example: [schemas, my_store, <<\"OrderPlaced\">>]\n```","ref":"storage_internals.html#path-format-3"},{"type":"extras","title":"Schema Structure - Storage Internals","doc":"```erlang\n#{\n    event_type => <<\"OrderPlaced\">>,\n    version => 3,\n    upcast_from => #{\n        1 => fun(Data) -> ... end,\n        2 => fun(Data) -> ... end\n    },\n    validator => fun(Data) -> ok | {error, Reason} end,\n    description => <<\"Current order event schema\">>,\n    registered_at => 1735689600000\n}\n```","ref":"storage_internals.html#schema-structure"},{"type":"extras","title":"Links Storage - Storage Internals","doc":"","ref":"storage_internals.html#links-storage"},{"type":"extras","title":"Path Format - Storage Internals","doc":"```erlang\n?LINKS_PATH ++ [StoreId, LinkName]\n%% Example: [links, my_store, <<\"high-value-orders\">>]\n```","ref":"storage_internals.html#path-format-4"},{"type":"extras","title":"Link Record - Storage Internals","doc":"```erlang\n-record(link, {\n    name :: binary(),\n    source :: source_spec(),\n    filter :: fun((event()) -> boolean()) | undefined,\n    transform :: fun((event()) -> event()) | undefined,\n    backfill :: boolean(),\n    created_at :: integer(),\n    status :: running | stopped | error,\n    processed :: non_neg_integer(),\n    last_event :: binary() | undefined\n}).\n```","ref":"storage_internals.html#link-record"},{"type":"extras","title":"Cluster Replication - Storage Internals","doc":"","ref":"storage_internals.html#cluster-replication"},{"type":"extras","title":"Ra Consensus - Storage Internals","doc":"All writes go through Raft consensus:\n\n```\n   Write Request\n        \n        \n   \n     Leader   All writes go here\n   \n        \n   \n                    \n                    \n  \nFollow Follow Follow\n  1      2      3   \n  \n```","ref":"storage_internals.html#ra-consensus"},{"type":"extras","title":"Consistency Guarantees - Storage Internals","doc":"| Operation | Guarantee |\n|-----------|-----------|\n| Write (append) | Strongly consistent (quorum) |\n| Read | Strongly consistent (from leader) |\n| Cross-stream | No transaction (best effort) |","ref":"storage_internals.html#consistency-guarantees"},{"type":"extras","title":"Failover Behavior - Storage Internals","doc":"1. Leader fails\n2. Ra elects new leader (typically < 1 second)\n3. Writes resume on new leader\n4. In-flight writes may need retry","ref":"storage_internals.html#failover-behavior"},{"type":"extras","title":"Storage Operations - Storage Internals","doc":"","ref":"storage_internals.html#storage-operations"},{"type":"extras","title":"Writing Events - Storage Internals","doc":"```erlang\n%% Append event\nPaddedVersion = pad_version(Version, ?VERSION_PADDING),\nPath = [streams, StreamId, PaddedVersion],\nkhepri:put(StoreId, Path, Event).\n```","ref":"storage_internals.html#writing-events"},{"type":"extras","title":"Reading Events - Storage Internals","doc":"```erlang\n%% Read specific version\n{ok, Event} = khepri:get(StoreId, [streams, StreamId, PaddedVersion]).\n\n%% Read all events in stream\nPattern = [streams, StreamId, ?KHEPRI_WILDCARD_STAR],\n{ok, EventsMap} = khepri:get_many(StoreId, Pattern).\n```","ref":"storage_internals.html#reading-events-1"},{"type":"extras","title":"Deleting Events (Scavenging) - Storage Internals","doc":"```erlang\n%% Delete individual event\nkhepri:delete(StoreId, [streams, StreamId, PaddedVersion]).\n```","ref":"storage_internals.html#deleting-events-scavenging"},{"type":"extras","title":"Listing Streams - Storage Internals","doc":"```erlang\n%% Get all stream IDs\nPattern = [streams, ?KHEPRI_WILDCARD_STAR],\n{ok, StreamNodes} = khepri:get_many(StoreId, Pattern),\nStreamIds = maps:keys(StreamNodes).\n```","ref":"storage_internals.html#listing-streams"},{"type":"extras","title":"Memory Considerations - Storage Internals","doc":"","ref":"storage_internals.html#memory-considerations"},{"type":"extras","title":"Khepri In-Memory - Storage Internals","doc":"Khepri keeps data in memory for fast access:\n\n- All paths and values are in memory\n- Raft log is also in memory (up to snapshot interval)\n- Consider total event size when planning capacity","ref":"storage_internals.html#khepri-in-memory"},{"type":"extras","title":"Memory Estimation - Storage Internals","doc":"```erlang\n%% Rough estimate per event\nEventMemory = byte_size(term_to_binary(Event)),\nTotalEvents = 1000000,\nApproxMemory = EventMemory * TotalEvents * 1.5,  %% 1.5x for overhead\n```","ref":"storage_internals.html#memory-estimation"},{"type":"extras","title":"Reducing Memory - Storage Internals","doc":"1. **Scavenging**: Remove old events\n2. **Snapshots**: Enable state recovery without all events\n3. **Archival**: Move to cold storage before scavenging","ref":"storage_internals.html#reducing-memory"},{"type":"extras","title":"Disk Persistence - Storage Internals","doc":"","ref":"storage_internals.html#disk-persistence"},{"type":"extras","title":"Ra Snapshots - Storage Internals","doc":"Ra periodically snapshots the Raft log to disk:\n\n```erlang\n%% Default location\nDataDir = application:get_env(ra, data_dir, \"ra_data\"),\n%% Store-specific subdirectory\nStoreDir = filename:join(DataDir, atom_to_list(StoreId)).\n```","ref":"storage_internals.html#ra-snapshots"},{"type":"extras","title":"Snapshot Interval - Storage Internals","doc":"Configure via Ra settings:\n\n```erlang\n%% In sys.config\n{ra, [\n    {segment_max_entries, 65536},    %% Entries per segment\n    {wal_max_size_bytes, 134217728}  %% 128MB WAL size\n]}\n```","ref":"storage_internals.html#snapshot-interval"},{"type":"extras","title":"Querying Patterns - Storage Internals","doc":"","ref":"storage_internals.html#querying-patterns"},{"type":"extras","title":"Prefix Queries - Storage Internals","doc":"```erlang\n%% All events for a stream\n[streams, <<\"orders-123\">>, ?KHEPRI_WILDCARD_STAR]\n\n%% All snapshots for a stream\n[snapshots, <<\"orders-123\">>, ?KHEPRI_WILDCARD_STAR]\n```","ref":"storage_internals.html#prefix-queries"},{"type":"extras","title":"Existence Checks - Storage Internals","doc":"```erlang\n%% Check if stream exists\ncase khepri:exists(StoreId, [streams, StreamId]) of\n    true -> stream_exists;\n    false -> no_stream\nend.\n```","ref":"storage_internals.html#existence-checks"},{"type":"extras","title":"Conditional Updates - Storage Internals","doc":"```erlang\n%% Optimistic concurrency via expected version\ncase khepri:get(StoreId, [streams, StreamId, PaddedExpectedVersion]) of\n    {ok, _} ->\n        %% Version exists, write next\n        khepri:put(StoreId, [streams, StreamId, PaddedNextVersion], NewEvent);\n    {error, {khepri, node_not_found, _}} ->\n        {error, wrong_expected_version}\nend.\n```","ref":"storage_internals.html#conditional-updates"},{"type":"extras","title":"Performance Tips - Storage Internals","doc":"","ref":"storage_internals.html#performance-tips"},{"type":"extras","title":"1. Batch Writes - Storage Internals","doc":"```erlang\n%% Write multiple events in single Ra command\nEvents = [Event1, Event2, Event3],\nkhepri:transaction(StoreId, fun() ->\n    lists:foreach(fun(E) ->\n        khepri:put([streams, StreamId, pad_version(E#event.version)], E)\n    end, Events)\nend).\n```","ref":"storage_internals.html#1-batch-writes"},{"type":"extras","title":"2. Use Snapshots - Storage Internals","doc":"```erlang\n%% Avoid replaying thousands of events\n{ok, Snapshot} = load_latest_snapshot(StoreId, StreamId),\n{ok, NewEvents} = read_events_since(StoreId, StreamId, Snapshot#snapshot.version),\nState = apply_events(Snapshot#snapshot.state, NewEvents).\n```","ref":"storage_internals.html#2-use-snapshots"},{"type":"extras","title":"3. Monitor Memory - Storage Internals","doc":"```erlang\n%% Check Khepri memory usage\nKhepriInfo = khepri:info(StoreId),\nMemoryUsed = proplists:get_value(memory, KhepriInfo).\n```","ref":"storage_internals.html#3-monitor-memory"},{"type":"extras","title":"Troubleshooting - Storage Internals","doc":"","ref":"storage_internals.html#troubleshooting"},{"type":"extras","title":"Common Issues - Storage Internals","doc":"| Issue | Cause | Resolution |\n|-------|-------|------------|\n| Slow writes | No quorum | Check cluster health |\n| High memory | Too many events | Enable scavenging |\n| Stale reads | Reading during partition | Wait for partition heal |","ref":"storage_internals.html#common-issues"},{"type":"extras","title":"Diagnostic Commands - Storage Internals","doc":"```erlang\n%% Cluster status\nra:members(StoreId).\n\n%% Leader info\nkhepri:get_leader(StoreId).\n\n%% Store statistics\nkhepri:info(StoreId).\n```","ref":"storage_internals.html#diagnostic-commands"},{"type":"extras","title":"See Also - Storage Internals","doc":"- [Temporal Queries](temporal_queries.md) - Time-based event retrieval\n- [Scavenging](scavenging.md) - Event lifecycle management\n- [Memory Pressure](memory_pressure.md) - Memory monitoring","ref":"storage_internals.html#see-also"},{"type":"extras","title":"Cluster Consistency","doc":"# Cluster Consistency & Split-Brain Prevention\n\nThis guide covers reckon-db's cluster consistency mechanisms, split-brain detection, active health probing, and quorum management. These systems work together to ensure data integrity in distributed deployments.","ref":"cluster_consistency.html"},{"type":"extras","title":"Overview - Cluster Consistency","doc":"Distributed event stores face fundamental challenges from the CAP theorem. reckon-db prioritizes **Consistency** and **Partition tolerance**, using Raft consensus via Khepri/Ra. However, network partitions can still cause split-brain scenarios that require detection and mitigation.","ref":"cluster_consistency.html#overview"},{"type":"extras","title":"Architecture - Cluster Consistency","doc":"![Consistency Checker Architecture](../assets/consistency_checker.svg)","ref":"cluster_consistency.html#architecture"},{"type":"extras","title":"The Split-Brain Problem - Cluster Consistency","doc":"","ref":"cluster_consistency.html#the-split-brain-problem"},{"type":"extras","title":"What is Split-Brain? - Cluster Consistency","doc":"Split-brain occurs when network partitions cause nodes to form independent clusters, each believing it is the authoritative source. This can lead to:\n\n- **Divergent event streams** - Different events written to the same stream on different partitions\n- **Lost events** - Events written to minority partition may be discarded on merge\n- **Inconsistent state** - Projections built from divergent streams\n\n![Split-Brain Detection](../assets/split_brain_detection.svg)","ref":"cluster_consistency.html#what-is-split-brain"},{"type":"extras","title":"How reckon-db Prevents Split-Brain - Cluster Consistency","doc":"1. **Raft Consensus** - Khepri/Ra requires quorum for writes\n2. **Deterministic Coordinator** - Lowest node name becomes cluster coordinator\n3. **Active Detection** - Consistency checker identifies partition scenarios\n4. **Health Probing** - Fast detection of node failures","ref":"cluster_consistency.html#how-reckon-db-prevents-split-brain"},{"type":"extras","title":"Consistency Checker - Cluster Consistency","doc":"The `reckon_db_consistency_checker` module provides continuous cluster health verification.","ref":"cluster_consistency.html#consistency-checker"},{"type":"extras","title":"Starting the Checker - Cluster Consistency","doc":"```erlang\n%% Started automatically with store in cluster mode\n%% Or manually configure check interval (default: 5000ms)\napplication:set_env(reckon_db, consistency_check_interval, 3000).\n```","ref":"cluster_consistency.html#starting-the-checker"},{"type":"extras","title":"Consistency Status Levels - Cluster Consistency","doc":"| Status | Description | Action Required |\n|--------|-------------|-----------------|\n| `healthy` | All checks passing, full consensus | None |\n| `degraded` | Warnings present, but operational | Investigate |\n| `split_brain` | Nodes disagree on membership/leader | Critical - resolve partition |\n| `no_quorum` | Insufficient nodes for operations | Critical - restore nodes |","ref":"cluster_consistency.html#consistency-status-levels"},{"type":"extras","title":"Forcing Immediate Check - Cluster Consistency","doc":"```erlang\n%% Force check and get result\nResult = reckon_db_consistency_checker:check_now(my_store),\n%% #{status => healthy,\n%%   checks => #{membership => ..., leader => ..., raft => ..., quorum => ...},\n%%   timestamp => 1703000000000,\n%%   duration_us => 1234}\n```","ref":"cluster_consistency.html#forcing-immediate-check"},{"type":"extras","title":"Registering Status Callbacks - Cluster Consistency","doc":"```erlang\n%% Get notified when status changes\nCallbackRef = reckon_db_consistency_checker:on_status_change(my_store, fun(Status) ->\n    case Status of\n        healthy ->\n            logger:info(\"Cluster health restored\");\n        degraded ->\n            logger:warning(\"Cluster degraded - investigate\"),\n            alert_ops_team(degraded);\n        split_brain ->\n            logger:error(\"SPLIT-BRAIN DETECTED!\"),\n            emergency_alert(split_brain),\n            pause_writes();\n        no_quorum ->\n            logger:error(\"Quorum lost - operations unavailable\"),\n            emergency_alert(no_quorum)\n    end\nend).\n\n%% Remove callback when done\nreckon_db_consistency_checker:remove_callback(my_store, CallbackRef).\n```","ref":"cluster_consistency.html#registering-status-callbacks"},{"type":"extras","title":"Verification Checks - Cluster Consistency","doc":"","ref":"cluster_consistency.html#verification-checks"},{"type":"extras","title":"1. Membership Consensus - Cluster Consistency","doc":"Verifies all nodes agree on cluster membership.\n\n```erlang\n{ok, Result} = reckon_db_consistency_checker:verify_membership_consensus(my_store).\n%% #{status => consensus,\n%%   nodes_checked => 3,\n%%   nodes_responded => 3,\n%%   failed_nodes => [],\n%%   consistent_view => [{my_store, 'node1@host'}, ...]}\n\n%% Or if split-brain detected:\n%% #{status => split_brain,\n%%   conflicting_views => 2,\n%%   views => #{'node1@host' => [...], 'node2@host' => [...]}}\n```","ref":"cluster_consistency.html#1-membership-consensus"},{"type":"extras","title":"2. Leader Consensus - Cluster Consistency","doc":"Verifies all nodes agree on the current Raft leader.\n\n```erlang\n{ok, Result} = reckon_db_consistency_checker:verify_leader_consensus(my_store).\n%% #{status => consensus,\n%%   leader => 'node1@host',\n%%   nodes_checked => 3,\n%%   nodes_responded => 3}\n\n%% Or if disagreement:\n%% #{status => no_consensus,\n%%   leaders_reported => ['node1@host', 'node2@host']}\n```","ref":"cluster_consistency.html#2-leader-consensus"},{"type":"extras","title":"3. Raft Log Consistency - Cluster Consistency","doc":"Verifies follower nodes have consistent Raft log state.\n\n```erlang\n{ok, Result} = reckon_db_consistency_checker:verify_raft_consistency(my_store).\n%% #{status => consensus,\n%%   leader => 'node1@host',\n%%   terms => [5],\n%%   terms_consistent => true,\n%%   commit_index_range => {100, 102},\n%%   max_commit_lag => 2}\n```","ref":"cluster_consistency.html#3-raft-log-consistency"},{"type":"extras","title":"4. Quorum Status - Cluster Consistency","doc":"Checks if sufficient nodes are available for operations.\n\n```erlang\n{ok, Result} = reckon_db_consistency_checker:get_quorum_status(my_store).\n%% #{has_quorum => true,\n%%   total_nodes => 3,\n%%   available_nodes => 3,\n%%   required_quorum => 2,\n%%   quorum_margin => 1,\n%%   can_lose => 1}\n```","ref":"cluster_consistency.html#4-quorum-status"},{"type":"extras","title":"Health Probing - Cluster Consistency","doc":"The `reckon_db_health_prober` module implements active health checks for faster failure detection.\n\n![Health Probing Flow](../assets/health_probing.svg)","ref":"cluster_consistency.html#health-probing"},{"type":"extras","title":"Why Active Probing? - Cluster Consistency","doc":"Passive monitoring via `net_kernel:monitor_nodes/1` can take 60+ seconds to detect failures (depending on `net_ticktime`). Active probing provides:\n\n- **Faster detection** - Configurable intervals (default: 2 seconds)\n- **Failure threshold** - Avoid false positives from transient issues\n- **Recovery detection** - Know when failed nodes come back","ref":"cluster_consistency.html#why-active-probing"},{"type":"extras","title":"Probe Types - Cluster Consistency","doc":"| Type | Speed | Depth | Use Case |\n|------|-------|-------|----------|\n| `ping` | Fastest | Shallow | Network connectivity only |\n| `rpc` | Medium | Medium | Process responsiveness |\n| `khepri` | Slowest | Deepest | Store health verification |","ref":"cluster_consistency.html#probe-types"},{"type":"extras","title":"Configuring the Prober - Cluster Consistency","doc":"```erlang\n%% In sys.config\n{reckon_db, [\n    {health_probe_interval, 2000},     %% 2 seconds between probes\n    {health_probe_timeout, 1000},      %% 1 second timeout per probe\n    {health_failure_threshold, 3},     %% 3 failures before declaring failed\n    {health_probe_type, rpc}           %% rpc probe type\n]}\n\n%% Or dynamically\nreckon_db_health_prober:configure(my_store, #{\n    probe_interval => 1000,\n    failure_threshold => 2\n}).\n```","ref":"cluster_consistency.html#configuring-the-prober"},{"type":"extras","title":"Node Status - Cluster Consistency","doc":"```erlang\n%% Check specific node\n{ok, Status} = reckon_db_health_prober:get_node_status(my_store, 'node2@host').\n%% healthy | suspect | failed | unknown\n\n%% Check all nodes\nAllStatus = reckon_db_health_prober:get_all_status(my_store).\n%% #{'node2@host' => healthy, 'node3@host' => suspect}\n```","ref":"cluster_consistency.html#node-status"},{"type":"extras","title":"Failure and Recovery Callbacks - Cluster Consistency","doc":"```erlang\n%% Get notified when nodes fail\nFailedRef = reckon_db_health_prober:on_node_failed(my_store, fun(Node) ->\n    logger:error(\"Node ~p failed health checks\", [Node]),\n    remove_from_load_balancer(Node)\nend).\n\n%% Get notified when nodes recover\nRecoveredRef = reckon_db_health_prober:on_node_recovered(my_store, fun(Node) ->\n    logger:info(\"Node ~p recovered\", [Node]),\n    add_to_load_balancer(Node)\nend).\n```","ref":"cluster_consistency.html#failure-and-recovery-callbacks"},{"type":"extras","title":"Quorum Management - Cluster Consistency","doc":"","ref":"cluster_consistency.html#quorum-management"},{"type":"extras","title":"Understanding Quorum - Cluster Consistency","doc":"Raft consensus requires a **majority** (quorum) of nodes to agree on operations:\n\n| Cluster Size | Quorum Required | Nodes Can Fail |\n|--------------|-----------------|----------------|\n| 1 | 1 | 0 |\n| 2 | 2 | 0 |\n| 3 | 2 | 1 |\n| 4 | 3 | 1 |\n| 5 | 3 | 2 |\n| 7 | 4 | 3 |\n\n**Recommendation**: Use odd-numbered clusters (3, 5, 7) for optimal fault tolerance.","ref":"cluster_consistency.html#understanding-quorum"},{"type":"extras","title":"Quorum Loss Behavior - Cluster Consistency","doc":"When quorum is lost:\n\n1. **Writes blocked** - Cannot append events\n2. **Reads may work** - If local data available (stale)\n3. **Subscriptions pause** - No new events delivered\n\n```erlang\n%% Check before critical operations\ncase reckon_db_consistency_checker:get_quorum_status(my_store) of\n    {ok, #{has_quorum := true, can_lose := N}} ->\n        logger:info(\"Quorum healthy, can lose ~p more nodes\", [N]),\n        proceed_with_operation();\n    {ok, #{has_quorum := false}} ->\n        logger:error(\"No quorum - operation blocked\"),\n        {error, no_quorum}\nend.\n```","ref":"cluster_consistency.html#quorum-loss-behavior"},{"type":"extras","title":"Integration Patterns - Cluster Consistency","doc":"","ref":"cluster_consistency.html#integration-patterns"},{"type":"extras","title":"1. Startup Verification - Cluster Consistency","doc":"```erlang\n%% In application startup\nstart_link() ->\n    {ok, Pid} = reckon_db:start_store(my_store, #{mode => cluster}),\n\n    %% Wait for cluster health before accepting traffic\n    case wait_for_healthy(my_store, 30000) of\n        ok ->\n            logger:info(\"Store healthy, accepting traffic\"),\n            {ok, Pid};\n        {error, Reason} ->\n            logger:error(\"Store unhealthy: ~p\", [Reason]),\n            {error, cluster_unhealthy}\n    end.\n\nwait_for_healthy(StoreId, Timeout) ->\n    Deadline = erlang:monotonic_time(millisecond) + Timeout,\n    wait_for_healthy_loop(StoreId, Deadline).\n\nwait_for_healthy_loop(StoreId, Deadline) ->\n    case reckon_db_consistency_checker:get_status(StoreId) of\n        {ok, healthy} ->\n            ok;\n        {ok, Status} ->\n            Now = erlang:monotonic_time(millisecond),\n            case Now  \n                    timer:sleep(1000),\n                    wait_for_healthy_loop(StoreId, Deadline);\n                false ->\n                    {error, {timeout, Status}}\n            end;\n        {error, not_running} ->\n            timer:sleep(500),\n            wait_for_healthy_loop(StoreId, Deadline)\n    end.\n```","ref":"cluster_consistency.html#1-startup-verification"},{"type":"extras","title":"2. Load Balancer Integration - Cluster Consistency","doc":"```erlang\n%% Remove unhealthy nodes from load balancer\ninit([]) ->\n    reckon_db_health_prober:on_node_failed(my_store, fun(Node) ->\n        haproxy_api:disable_server(Node)\n    end),\n    reckon_db_health_prober:on_node_recovered(my_store, fun(Node) ->\n        haproxy_api:enable_server(Node)\n    end),\n    {ok, #state{}}.\n```","ref":"cluster_consistency.html#2-load-balancer-integration"},{"type":"extras","title":"3. Circuit Breaker Pattern - Cluster Consistency","doc":"```erlang\n-record(state, {\n    circuit :: closed | open | half_open,\n    failures :: non_neg_integer(),\n    last_attempt :: integer()\n}).\n\nhandle_call({append, Stream, Events}, From, #state{circuit = open} = State) ->\n    %% Check if should try again\n    case should_retry(State) of\n        true ->\n            try_append(Stream, Events, From, State#state{circuit = half_open});\n        false ->\n            {reply, {error, circuit_open}, State}\n    end;\n\nhandle_call({append, Stream, Events}, From, State) ->\n    try_append(Stream, Events, From, State).\n\ntry_append(Stream, Events, _From, State) ->\n    case reckon_db_consistency_checker:get_status(my_store) of\n        {ok, healthy} ->\n            Result = reckon_db_streams:append(my_store, Stream, any, Events),\n            {reply, Result, State#state{circuit = closed, failures = 0}};\n        {ok, Status} when Status =:= split_brain; Status =:= no_quorum ->\n            NewState = State#state{\n                circuit = open,\n                failures = State#state.failures + 1,\n                last_attempt = erlang:monotonic_time(millisecond)\n            },\n            {reply, {error, {cluster_unhealthy, Status}}, NewState};\n        _ ->\n            {reply, {error, status_unknown}, State}\n    end.\n```","ref":"cluster_consistency.html#3-circuit-breaker-pattern"},{"type":"extras","title":"Telemetry Events - Cluster Consistency","doc":"","ref":"cluster_consistency.html#telemetry-events"},{"type":"extras","title":"Consistency Checker Events - Cluster Consistency","doc":"```erlang\n%% Check completed\n[reckon_db, consistency, check, complete]\n%% Measurements: #{duration_us => integer()}\n%% Metadata: #{store_id => atom(), status => atom(), checks => map()}\n\n%% Status changed\n[reckon_db, consistency, status, changed]\n%% Measurements: #{system_time => integer()}\n%% Metadata: #{store_id => atom(), old_status => atom(), new_status => atom()}\n\n%% Split-brain detected\n[reckon_db, consistency, split_brain, detected]\n%% Measurements: #{system_time => integer()}\n%% Metadata: #{store_id => atom(), result => map()}\n```","ref":"cluster_consistency.html#consistency-checker-events"},{"type":"extras","title":"Health Prober Events - Cluster Consistency","doc":"```erlang\n%% Probe cycle completed\n[reckon_db, health, probe, complete]\n%% Measurements: #{duration_us => integer(), success_count => integer(), failure_count => integer()}\n%% Metadata: #{store_id => atom()}\n\n%% Node declared failed\n[reckon_db, health, node, failed]\n%% Measurements: #{system_time => integer(), consecutive_failures => integer()}\n%% Metadata: #{store_id => atom(), node => node()}\n\n%% Node recovered\n[reckon_db, health, node, recovered]\n%% Measurements: #{system_time => integer()}\n%% Metadata: #{store_id => atom(), node => node()}\n```","ref":"cluster_consistency.html#health-prober-events"},{"type":"extras","title":"Example Telemetry Handler - Cluster Consistency","doc":"```erlang\nsetup_telemetry() ->\n    telemetry:attach_many(\n        <<\"cluster-health-handler\">>,\n        [\n            [reckon_db, consistency, split_brain, detected],\n            [reckon_db, health, node, failed]\n        ],\n        fun handle_cluster_event/4,\n        #{}\n    ).\n\nhandle_cluster_event([reckon_db, consistency, split_brain, detected],\n                     _Measurements, #{store_id := StoreId}, _Config) ->\n    pagerduty:trigger(#{\n        severity => critical,\n        summary => io_lib:format(\"Split-brain detected in ~p\", [StoreId])\n    });\n\nhandle_cluster_event([reckon_db, health, node, failed],\n                     #{consecutive_failures := Failures},\n                     #{store_id := StoreId, node := Node}, _Config) ->\n    prometheus_counter:inc(node_failures_total, [StoreId, Node]),\n    slack:post(ops_channel,\n               io_lib:format(\"Node ~p failed after ~p probes\", [Node, Failures])).\n```","ref":"cluster_consistency.html#example-telemetry-handler"},{"type":"extras","title":"Troubleshooting - Cluster Consistency","doc":"","ref":"cluster_consistency.html#troubleshooting"},{"type":"extras","title":"Common Issues - Cluster Consistency","doc":"| Symptom | Likely Cause | Resolution |\n|---------|--------------|------------|\n| Frequent `degraded` status | Network latency | Increase probe timeout |\n| `no_quorum` after restart | Nodes not discovered | Check UDP multicast |\n| `split_brain` detected | Network partition | Identify partition, restore connectivity |\n| Slow recovery detection | High failure threshold | Reduce threshold (with caution) |","ref":"cluster_consistency.html#common-issues"},{"type":"extras","title":"Diagnostic Commands - Cluster Consistency","doc":"```erlang\n%% Full cluster status\n{ok, Result} = reckon_db_consistency_checker:check_now(my_store).\nio:format(\"Status: ~p~n\", [maps:get(status, Result)]).\nio:format(\"Checks: ~p~n\", [maps:get(checks, Result)]).\n\n%% Node health details\nAllStatus = reckon_db_health_prober:get_all_status(my_store).\nmaps:foreach(fun(Node, Status) ->\n    io:format(\"  ~p: ~p~n\", [Node, Status])\nend, AllStatus).\n\n%% Quorum margin\n{ok, Quorum} = reckon_db_consistency_checker:get_quorum_status(my_store).\nio:format(\"Can lose ~p more nodes~n\", [maps:get(can_lose, Quorum)]).\n```","ref":"cluster_consistency.html#diagnostic-commands"},{"type":"extras","title":"Recovery Procedures - Cluster Consistency","doc":"#### Split-Brain Recovery\n\n1. **Identify partitioned nodes** - Check which nodes are in each partition\n2. **Stop minority partition** - Gracefully stop nodes in smaller partition\n3. **Restore connectivity** - Fix network issues\n4. **Restart stopped nodes** - They will rejoin and sync from majority\n5. **Verify consistency** - Check events weren't lost\n\n```erlang\n%% After recovery, force verification\nResult = reckon_db_consistency_checker:check_now(my_store),\ncase maps:get(status, Result) of\n    healthy -> logger:info(\"Recovery successful\");\n    Other -> logger:error(\"Still unhealthy: ~p\", [Other])\nend.\n```","ref":"cluster_consistency.html#recovery-procedures"},{"type":"extras","title":"Configuration Reference - Cluster Consistency","doc":"","ref":"cluster_consistency.html#configuration-reference"},{"type":"extras","title":"Consistency Checker - Cluster Consistency","doc":"| Setting | Default | Description |\n|---------|---------|-------------|\n| `consistency_check_interval` | 5000 | Milliseconds between checks |\n| (minimum enforced) | 1000 | Minimum allowed interval |","ref":"cluster_consistency.html#consistency-checker-1"},{"type":"extras","title":"Health Prober - Cluster Consistency","doc":"| Setting | Default | Description |\n|---------|---------|-------------|\n| `health_probe_interval` | 2000 | Milliseconds between probe cycles |\n| `health_probe_timeout` | 1000 | Timeout for each probe (ms) |\n| `health_failure_threshold` | 3 | Consecutive failures before `failed` |\n| `health_probe_type` | `rpc` | Probe type: `ping`, `rpc`, or `khepri` |","ref":"cluster_consistency.html#health-prober"},{"type":"extras","title":"Academic References - Cluster Consistency","doc":"- Ongaro, D. and Ousterhout, J. (2014). *In Search of an Understandable Consensus Algorithm (Raft)*. USENIX ATC 2014.\n- Brewer, E. (2012). *CAP Twelve Years Later: How the \"Rules\" Have Changed*. IEEE Computer, 45(2), 23-29.\n- Kleppmann, M. (2017). *Designing Data-Intensive Applications*. O'Reilly Media. Chapter 9: Consistency and Consensus.","ref":"cluster_consistency.html#academic-references"},{"type":"extras","title":"See Also - Cluster Consistency","doc":"- [Storage Internals](storage_internals.md) - Khepri/Ra replication details\n- [Memory Pressure](memory_pressure.md) - Resource management\n- [Subscriptions](subscriptions.md) - Event delivery in clusters","ref":"cluster_consistency.html#see-also"}],"proglang":"erlang","content_type":"text/markdown","producer":{"name":"ex_doc","version":"0.38.2"}}